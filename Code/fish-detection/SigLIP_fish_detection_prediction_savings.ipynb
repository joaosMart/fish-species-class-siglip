{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxCZa4UQOzXLCBmJ9y0WTu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joaosMart/fish-species-class-siglip/blob/main/Code/fish-detection/SigLIP_fish_detection_prediction_savings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to Use\n",
        "\n",
        "## Prerequisites\n",
        "Install required packages if running this in colab:\n",
        "\n",
        "```bash\n",
        "!pip install transformers open_clip_torch opencv-python pillow torch\n",
        "```\n",
        "\n",
        "## Setup your video list\n",
        "Create a list called `fish_path_list` containing the full file paths to your video files:\n",
        "\n",
        "```\n",
        "fish_path_list = [\n",
        "    \"/path/to/video1.mp4\",\n",
        "    \"/path/to/video2.mp4\",\n",
        "    \"/path/to/video3.mp4\"\n",
        "]\n",
        "```\n",
        "\n",
        "## Run the code\n",
        "Execute the script - it will automatically process each video frame-by-frame, generate fish detection probability scores, and save results to a pickle file with checkpointing every 10 videos.\n",
        "\n",
        "## Output\n",
        "Results are saved to `Scores-ViT-SO400M-14-SigLIP.pkl` containing probability scores for each frame: `[no_fish_probability, fish_probability]`\n",
        "\n",
        "## Resume capability\n",
        "If processing is interrupted, simply re-run the script - it will automatically resume from the last checkpoint."
      ],
      "metadata": {
        "id": "EZVot-F1WbdD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bricCZr4RHsr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import open_clip\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import pickle\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Setup device and model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "model_name = 'ViT-SO400M-14-SigLIP'\n",
        "model, _, preprocess_val = open_clip.create_model_and_transforms(model_name, pretrained='webli')\n",
        "model = model.to(device)\n",
        "tokenizer = open_clip.get_tokenizer(model_name)\n",
        "\n",
        "positive_prompts = tokenizer([\n",
        "    \"Salmon-like fish swimming\",\n",
        "    \"An underwater photo of a salmon-like fish seen clearly swimming.\",\n",
        "    \"Image of salmon-like fish in a contained environment.\",\n",
        "    \"A photo of a salmon-like fish in a controlled river environment.\"\n",
        "], context_length=model.context_length).to(device)\n",
        "\n",
        "negative_prompts = tokenizer([\n",
        "    \"An image of an empty white water container.\",\n",
        "    \"A contained environment with nothing in it.\",\n",
        "    \"An image of a empty container with nothing in it.\"\n",
        "], context_length=model.context_length).to(device)\n",
        "\n",
        "# Encode text features\n",
        "with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "    pos_text_features = model.encode_text(positive_prompts)\n",
        "    neg_text_features = model.encode_text(negative_prompts)\n",
        "    text_features = torch.stack((neg_text_features.mean(axis=0), pos_text_features.mean(axis=0)))\n",
        "    text_features = F.normalize(text_features, dim=-1)\n",
        "\n",
        "def process_video_batch(video_path, text_features, batch_size=128):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    results = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    batch = []\n",
        "    for _ in tqdm(range(frame_count), desc=f\"Processing {os.path.basename(video_path)}\", leave=False):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        image_tensor = preprocess_val(image).unsqueeze(0)\n",
        "        batch.append(image_tensor)\n",
        "\n",
        "        if len(batch) == batch_size:\n",
        "            batch_tensor = torch.cat(batch).to(device)\n",
        "            with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "                image_features = model.encode_image(batch_tensor)\n",
        "                image_features = F.normalize(image_features, dim=-1)\n",
        "                text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "                results.extend(text_probs.cpu().numpy())\n",
        "            batch = []\n",
        "\n",
        "    # Process any remaining frames\n",
        "    if batch:\n",
        "        batch_tensor = torch.cat(batch).to(device)\n",
        "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "            image_features = model.encode_image(batch_tensor)\n",
        "            image_features = F.normalize(image_features, dim=-1)\n",
        "            text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "            results.extend(text_probs.cpu().numpy())\n",
        "\n",
        "    cap.release()\n",
        "    return results\n",
        "\n",
        "# Set the path for saving checkpoints\n",
        "CHECKPOINT_PATH = '/path/to/Scores-ViT-SO400M-14-SigLIP.pkl'\n",
        "\n",
        "def save_checkpoint(results, processed_videos, filename=CHECKPOINT_PATH):\n",
        "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "    checkpoint_data = {\n",
        "        'results': results,\n",
        "        'processed_videos': processed_videos\n",
        "    }\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(checkpoint_data, f)\n",
        "    print(f\"Checkpoint saved to {filename}\")\n",
        "\n",
        "def load_checkpoint(filename=CHECKPOINT_PATH):\n",
        "    if os.path.isfile(filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "            checkpoint = pickle.load(f)\n",
        "        print(f\"Loaded checkpoint from {filename}\")\n",
        "        return checkpoint['results'], checkpoint['processed_videos']\n",
        "    else:\n",
        "        print(f\"No checkpoint found at {filename}\")\n",
        "        return {}, set()  # Return empty results and empty set of processed videos\n",
        "\n",
        "def process_videos(video_files, text_features, checkpoint_interval=10):\n",
        "    overall_results, processed_videos = load_checkpoint()\n",
        "\n",
        "    for i, video_path in enumerate(tqdm(video_files)):\n",
        "        if video_path not in processed_videos:\n",
        "            results = process_video_batch(video_path, text_features)\n",
        "            overall_results[video_path] = results\n",
        "            processed_videos.add(video_path)\n",
        "\n",
        "            if (len(processed_videos) % checkpoint_interval == 0):\n",
        "                save_checkpoint(overall_results, processed_videos)\n",
        "\n",
        "    # Save final results\n",
        "    save_checkpoint(overall_results, processed_videos)\n",
        "\n",
        "    return overall_results\n",
        "\n",
        "# Your list of video paths\n",
        "results = process_videos(fish_path_list, text_features)"
      ]
    }
  ]
}