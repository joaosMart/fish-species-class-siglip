{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+5AJVbVDk3jcyUu4n418I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joaosMart/fish-species-class-siglip/blob/main/Code/fish-detection/SigLIP_fish_detection_prediction_savings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to Use the Complete Fish Detection Pipeline\n",
        "\n",
        "## Prerequisites\n",
        "Install required packages in Google Colab:\n",
        "```bash\n",
        "!pip install transformers open_clip_torch\n",
        "```\n",
        "\n",
        "## Setup your video data\n",
        "\n",
        "\n",
        "\n",
        "### Manual video list\n",
        "Define your video list as follows:\n",
        "\n",
        "```python\n",
        "fish_path_list = [\n",
        "    \"/path/to/video1.mp4\",\n",
        "    \"/path/to/video2.mp4\",\n",
        "    \"/path/to/video3.mp4\"\n",
        "]\n",
        "```\n",
        "\n",
        "## Run the complete pipeline\n",
        "Execute the entire notebook - it will automatically:\n",
        "\n",
        "### Part 1: Generate Fish Detection Scores\n",
        "- Load SigLIP Vision Transformer model\n",
        "- Process each video frame-by-frame\n",
        "- Generate probability scores: `[no_fish_probability, fish_probability]`\n",
        "- Save with checkpointing every 10 videos\n",
        "\n",
        "### Part 2: Filter and Analyze Results\n",
        "- Apply probability threshold (default: 0.977989)\n",
        "- Filter frames containing fish above threshold\n",
        "- Generate comprehensive statistical analysis\n",
        "\n",
        "## Configuration\n",
        "Adjust the detection threshold in Part 2:\n",
        "\n",
        "```python\n",
        "DETECTION_THRESHOLD = 0.977989  # Modify as needed\n",
        "```\n",
        "\n",
        "## Outputs\n",
        "The pipeline generates two files:\n",
        "1. **Raw scores**: `Scores-ViT-SO400M-14-SigLIP.pkl`\n",
        "2. **Filtered results**: `fish_detection_results.json`\n",
        "\n",
        "## Resume capability\n",
        "If processing is interrupted, simply re-run the notebook - it automatically resumes from the last checkpoint.\n",
        "\n",
        "## Analysis results\n",
        "The final section provides detailed statistics including:\n",
        "- Total fish frames detected\n",
        "- Video distribution (with/without fish)\n",
        "- Probability distributions and percentiles\n",
        "- Frame-level statistics and percentages"
      ],
      "metadata": {
        "id": "EZVot-F1WbdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages (run in Colab)\n",
        "# !pip install transformers open_clip_torch\n",
        "\n",
        "\n",
        "# Import libraries for Part 1 (Score Generation)\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import open_clip\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import pickle\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Import libraries for Part 2 (Analysis)\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from statistics import mean, median"
      ],
      "metadata": {
        "id": "ybIObe_AY1MW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fish Detection Score Generation"
      ],
      "metadata": {
        "id": "a3-qZhGqY3Sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"PART 1: GENERATING FISH DETECTION SCORES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Setup device and model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load SigLIP model\n",
        "model_name = 'ViT-SO400M-14-SigLIP'\n",
        "model, _, preprocess_val = open_clip.create_model_and_transforms(model_name, pretrained='webli')\n",
        "model = model.to(device)\n",
        "tokenizer = open_clip.get_tokenizer(model_name)\n",
        "\n",
        "# Define text prompts for fish detection\n",
        "positive_prompts = tokenizer([\n",
        "    \"Salmon-like fish swimming\",\n",
        "    \"An underwater photo of a salmon-like fish seen clearly swimming.\",\n",
        "    \"Image of salmon-like fish in a contained environment.\",\n",
        "    \"A photo of a salmon-like fish in a controlled river environment.\"\n",
        "], context_length=model.context_length).to(device)\n",
        "\n",
        "negative_prompts = tokenizer([\n",
        "    \"An image of an empty white water container.\",\n",
        "    \"A contained environment with nothing in it.\",\n",
        "    \"An image of a empty container with nothing in it.\"\n",
        "], context_length=model.context_length).to(device)\n",
        "\n",
        "# Encode text features once\n",
        "print(\"Encoding text prompts...\")\n",
        "with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "    pos_text_features = model.encode_text(positive_prompts)\n",
        "    neg_text_features = model.encode_text(negative_prompts)\n",
        "    text_features = torch.stack((neg_text_features.mean(axis=0), pos_text_features.mean(axis=0)))\n",
        "    text_features = F.normalize(text_features, dim=-1)\n",
        "\n",
        "print(\"Text features encoded successfully!\")\n",
        "\n",
        "# Load video data!\n",
        "print(\"CSV file not found. Please define fish_path_list manually:\")\n",
        "print(\"fish_path_list = ['/path/to/video1.mp4', '/path/to/video2.mp4', ...]\")\n",
        "fish_path_list = []\n",
        "\n",
        "# Video processing function\n",
        "def process_video_batch(video_path, text_features, batch_size=128):\n",
        "    \"\"\"Process a single video and return probability scores for each frame\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    results = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    batch = []\n",
        "    for _ in tqdm(range(frame_count), desc=f\"Processing {os.path.basename(video_path)}\", leave=False):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        image_tensor = preprocess_val(image).unsqueeze(0)\n",
        "        batch.append(image_tensor)\n",
        "\n",
        "        if len(batch) == batch_size:\n",
        "            batch_tensor = torch.cat(batch).to(device)\n",
        "            with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "                image_features = model.encode_image(batch_tensor)\n",
        "                image_features = F.normalize(image_features, dim=-1)\n",
        "                text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "                results.extend(text_probs.cpu().numpy())\n",
        "            batch = []\n",
        "\n",
        "    # Process any remaining frames\n",
        "    if batch:\n",
        "        batch_tensor = torch.cat(batch).to(device)\n",
        "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "            image_features = model.encode_image(batch_tensor)\n",
        "            image_features = F.normalize(image_features, dim=-1)\n",
        "            text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "            results.extend(text_probs.cpu().numpy())\n",
        "\n",
        "    cap.release()\n",
        "    processing_time = time.time() - start_time\n",
        "    print(f\"Processed {len(results)} frames in {processing_time:.2f} seconds\")\n",
        "    return results\n",
        "\n",
        "# Checkpoint management\n",
        "CHECKPOINT_PATH = '/path/to/Scores-ViT-SO400M-14-SigLIP.pkl'\n",
        "\n",
        "def save_checkpoint(results, processed_videos, filename=CHECKPOINT_PATH):\n",
        "    \"\"\"Save progress to pickle file\"\"\"\n",
        "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "    checkpoint_data = {\n",
        "        'results': results,\n",
        "        'processed_videos': processed_videos\n",
        "    }\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(checkpoint_data, f)\n",
        "    print(f\"Checkpoint saved to {filename}\")\n",
        "\n",
        "def load_checkpoint(filename=CHECKPOINT_PATH):\n",
        "    \"\"\"Load existing progress from pickle file\"\"\"\n",
        "    if os.path.isfile(filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "            checkpoint = pickle.load(f)\n",
        "        print(f\"Loaded checkpoint from {filename}\")\n",
        "        return checkpoint['results'], checkpoint['processed_videos']\n",
        "    else:\n",
        "        print(f\"No checkpoint found at {filename}\")\n",
        "        return {}, set()\n",
        "\n",
        "def process_videos(video_files, text_features, checkpoint_interval=10):\n",
        "    \"\"\"Process all videos with checkpointing\"\"\"\n",
        "    overall_results, processed_videos = load_checkpoint()\n",
        "\n",
        "    print(f\"Starting video processing...\")\n",
        "    print(f\"Total videos to process: {len(video_files)}\")\n",
        "    print(f\"Already processed: {len(processed_videos)}\")\n",
        "    print(f\"Remaining: {len(video_files) - len(processed_videos)}\")\n",
        "\n",
        "    for i, video_path in enumerate(tqdm(video_files, desc=\"Processing videos\")):\n",
        "        if video_path not in processed_videos:\n",
        "            print(f\"\\nProcessing video {len(processed_videos) + 1}/{len(video_files)}: {os.path.basename(video_path)}\")\n",
        "            results = process_video_batch(video_path, text_features)\n",
        "            overall_results[video_path] = results\n",
        "            processed_videos.add(video_path)\n",
        "\n",
        "            if (len(processed_videos) % checkpoint_interval == 0):\n",
        "                save_checkpoint(overall_results, processed_videos)\n",
        "\n",
        "    # Save final results\n",
        "    save_checkpoint(overall_results, processed_videos)\n",
        "    print(f\"\\nScore generation complete! Processed {len(overall_results)} videos.\")\n",
        "    return overall_results\n",
        "\n",
        "# Run video processing (only if fish_path_list is defined)\n",
        "if fish_path_list:\n",
        "    results = process_videos(fish_path_list, text_features)\n",
        "else:\n",
        "    print(\"Skipping video processing - no video list provided\")"
      ],
      "metadata": {
        "id": "3BuYJJDoZChQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fish Detection Analysis and Filtering"
      ],
      "metadata": {
        "id": "BVnHQDNGZDle"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bricCZr4RHsr"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PART 2: ANALYZING FISH DETECTION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load the generated scores\n",
        "file_name = \"/path/to/Scores-ViT-SO400M-14-SigLIP.pkl\"\n",
        "\n",
        "try:\n",
        "    with open(file_name, 'rb') as file:\n",
        "        data = pickle.load(file)\n",
        "\n",
        "    print(f\"Loaded data successfully!\")\n",
        "    print(f\"Number of videos processed: {len(data['results'])}\")\n",
        "    print(f\"Sample video frames: {len(list(data['results'].values())[0])}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Score file not found at {file_name}\")\n",
        "    print(\"Please run Part 1 first to generate scores\")\n",
        "    data = None\n",
        "\n",
        "if data is not None:\n",
        "    # Processing functions\n",
        "    def process_video_cpu(video_path, frame_data, threshold=0.9898):\n",
        "        \"\"\"Filter frames above threshold - CPU version\"\"\"\n",
        "        probabilities = [float(prob[1]) for prob in frame_data]\n",
        "        fish_frames = [i for i, prob in enumerate(probabilities) if prob >= threshold]\n",
        "\n",
        "        return {\n",
        "            \"video_name\": video_path,\n",
        "            \"total_frames\": len(frame_data),\n",
        "            \"fish_frames\": [\n",
        "                {\"frame\": frame, \"probability\": probabilities[frame]}\n",
        "                for frame in fish_frames\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def process_all_videos_gpu(data, output_file, batch_size=512, threshold=0.5):\n",
        "        \"\"\"Process all videos and apply threshold filtering\"\"\"\n",
        "        video_paths = list(data['results'].keys())\n",
        "        all_results = {}\n",
        "\n",
        "        print(f\"Applying threshold filtering (threshold = {threshold})...\")\n",
        "        for i in tqdm(range(0, len(video_paths), batch_size), desc=\"Processing batches\"):\n",
        "            batch_paths = video_paths[i:i+batch_size]\n",
        "            batch_results = [\n",
        "                process_video_cpu(path, data['results'][path], threshold)\n",
        "                for path in batch_paths\n",
        "            ]\n",
        "\n",
        "            for result in batch_results:\n",
        "                all_results[result['video_name']] = {\n",
        "                    \"total_frames\": result['total_frames'],\n",
        "                    \"fish_frames\": result['fish_frames']\n",
        "                }\n",
        "\n",
        "        # Save results to JSON file\n",
        "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "        with open(output_file, 'w') as f:\n",
        "            json.dump(all_results, f, indent=2)\n",
        "\n",
        "        print(f\"Filtered results saved to {output_file}\")\n",
        "        return all_results\n",
        "\n",
        "    # Apply threshold filtering\n",
        "    output_file = '/path/to/fish_detection_results.json'\n",
        "    DETECTION_THRESHOLD = 0.977989  # Adjust this threshold as needed\n",
        "\n",
        "    filtered_results = process_all_videos_gpu(data, output_file, threshold=DETECTION_THRESHOLD)\n",
        "\n",
        "    # =============================================================================\n",
        "    # PART 3: STATISTICAL ANALYSIS\n",
        "    # =============================================================================\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"PART 3: STATISTICAL ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    def load_json_file(file_path):\n",
        "        \"\"\"Load JSON file with error handling\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: File {file_path} not found\")\n",
        "            return None\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Error: File {file_path} is not valid JSON\")\n",
        "            return None\n",
        "\n",
        "    def calculate_probability_stats(data):\n",
        "        \"\"\"Calculate statistics about probabilities across all fish detections\"\"\"\n",
        "        all_probabilities = []\n",
        "        for video_data in data.values():\n",
        "            probabilities = [frame['probability'] for frame in video_data['fish_frames']]\n",
        "            all_probabilities.extend(probabilities)\n",
        "\n",
        "        if not all_probabilities:\n",
        "            return None\n",
        "\n",
        "        probabilities_array = np.array(all_probabilities)\n",
        "        quantiles = np.percentile(probabilities_array, [25, 50, 75])\n",
        "\n",
        "        return {\n",
        "            'min_probability': min(all_probabilities),\n",
        "            'max_probability': max(all_probabilities),\n",
        "            'mean_probability': np.mean(all_probabilities),\n",
        "            'q25': quantiles[0],\n",
        "            'q50': quantiles[1],\n",
        "            'q75': quantiles[2],\n",
        "        }\n",
        "\n",
        "    def calculate_fish_frame_stats(data):\n",
        "        \"\"\"Calculate statistics about frames with fish across all videos\"\"\"\n",
        "        frames_per_video = [len(video['fish_frames']) for video in data.values()]\n",
        "        total_frames_per_video = [video['total_frames'] for video in data.values()]\n",
        "\n",
        "        # Separate videos with and without fish\n",
        "        videos_with_fish = [(total, fish) for total, fish in zip(total_frames_per_video, frames_per_video) if fish > 0]\n",
        "        videos_without_fish = [(total, fish) for total, fish in zip(total_frames_per_video, frames_per_video) if fish == 0]\n",
        "\n",
        "        stats = {\n",
        "            'total_videos': len(data),\n",
        "            'total_fish_frames': sum(frames_per_video),\n",
        "            'average_fish_frames_per_video': mean(frames_per_video),\n",
        "            'median_fish_frames_per_video': median(frames_per_video),\n",
        "            'max_fish_frames': max(frames_per_video),\n",
        "            'min_fish_frames': min(frames_per_video),\n",
        "            'videos_with_fish': len(videos_with_fish),\n",
        "            'videos_without_fish': len(videos_without_fish),\n",
        "            'average_total_frames_all_videos': mean(total_frames_per_video),\n",
        "            'average_total_frames_videos_with_fish': mean([total for total, _ in videos_with_fish]) if videos_with_fish else 0,\n",
        "            'average_total_frames_videos_without_fish': mean([total for total, _ in videos_without_fish]) if videos_without_fish else 0,\n",
        "            'average_fish_frames_in_positive_videos': mean([fish for _, fish in videos_with_fish]) if videos_with_fish else 0\n",
        "        }\n",
        "\n",
        "        # Calculate percentage of frames with fish for each video\n",
        "        percentages = []\n",
        "        for video_name, video_data in data.items():\n",
        "            total_frames = video_data['total_frames']\n",
        "            fish_frames = len(video_data['fish_frames'])\n",
        "            percentage = (fish_frames / total_frames) * 100 if total_frames > 0 else 0\n",
        "            percentages.append(percentage)\n",
        "\n",
        "        stats['average_percentage_frames_with_fish'] = mean(percentages)\n",
        "        return stats\n",
        "\n",
        "    def print_fish_frame_analysis(file_path):\n",
        "        \"\"\"Print comprehensive analysis of fish frames in the dataset\"\"\"\n",
        "        data = load_json_file(file_path)\n",
        "        if not data:\n",
        "            return\n",
        "\n",
        "        stats = calculate_fish_frame_stats(data)\n",
        "        prob_stats = calculate_probability_stats(data)\n",
        "\n",
        "        print(\"\\n=== FISH FRAME ANALYSIS ===\")\n",
        "        print(f\"\\nThreshold used: {DETECTION_THRESHOLD}\")\n",
        "        print(\"\\nOverall Statistics:\")\n",
        "        print(f\"Total number of videos analyzed: {stats['total_videos']}\")\n",
        "        print(f\"Total frames containing fish: {stats['total_fish_frames']}\")\n",
        "\n",
        "        print(\"\\nFrame Distribution:\")\n",
        "        print(f\"Average total frames per video (all videos): {stats['average_total_frames_all_videos']:.2f}\")\n",
        "        print(f\"Average total frames in videos WITH fish: {stats['average_total_frames_videos_with_fish']:.2f}\")\n",
        "        print(f\"Average total frames in videos WITHOUT fish: {stats['average_total_frames_videos_without_fish']:.2f}\")\n",
        "\n",
        "        print(\"\\nFish Frame Statistics:\")\n",
        "        print(f\"Average frames with fish per video (all videos): {stats['average_fish_frames_per_video']:.2f}\")\n",
        "        print(f\"Average frames with fish in videos containing fish: {stats['average_fish_frames_in_positive_videos']:.2f}\")\n",
        "        print(f\"Median frames with fish per video: {stats['median_fish_frames_per_video']}\")\n",
        "        print(f\"Maximum frames with fish in a video: {stats['max_fish_frames']}\")\n",
        "        print(f\"Minimum frames with fish in a video: {stats['min_fish_frames']}\")\n",
        "\n",
        "        print(\"\\nVideo Distribution:\")\n",
        "        print(f\"Videos containing fish: {stats['videos_with_fish']}\")\n",
        "        print(f\"Videos without fish: {stats['videos_without_fish']}\")\n",
        "        print(f\"Percentage of videos with fish: {(stats['videos_with_fish']/stats['total_videos']*100):.1f}%\")\n",
        "\n",
        "        print(\"\\nPercentage Analysis:\")\n",
        "        print(f\"Average percentage of frames with fish per video: {stats['average_percentage_frames_with_fish']:.2f}%\")\n",
        "\n",
        "        if prob_stats:\n",
        "            print(\"\\nProbability Statistics (for detected fish frames):\")\n",
        "            print(f\"Minimum probability: {prob_stats['min_probability']:.6f}\")\n",
        "            print(f\"Maximum probability: {prob_stats['max_probability']:.6f}\")\n",
        "            print(f\"Mean probability: {prob_stats['mean_probability']:.6f}\")\n",
        "            print(\"\\nProbability Quantiles:\")\n",
        "            print(f\"25th percentile (Q1): {prob_stats['q25']:.6f}\")\n",
        "            print(f\"50th percentile (Median): {prob_stats['q50']:.6f}\")\n",
        "            print(f\"75th percentile (Q3): {prob_stats['q75']:.6f}\")\n",
        "\n",
        "        return stats, prob_stats\n",
        "\n",
        "    # Run the complete analysis\n",
        "    analysis_results = print_fish_frame_analysis(output_file)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PIPELINE COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Files generated:\")\n",
        "print(f\"1. Raw scores: {CHECKPOINT_PATH}\")\n",
        "print(f\"2. Filtered results: {output_file}\")\n",
        "print(\"\\nYou can now use these files for further analysis or visualization.\")"
      ]
    }
  ]
}