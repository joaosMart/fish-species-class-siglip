{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joaosMart/fish-species-class-siglip/blob/main/Code/species-classification/resnet_transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNET-50 as a Feature extractor\n",
        "\n",
        "This code was used to implement the training and validation of the Resnet-50 as feature extractor as a baseline."
      ],
      "metadata": {
        "id": "l7HJFyboIzka"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBqWiJb8-lI0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a48a48b-6cbd-4d69-9c16-9c52e1fca67c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying data to: /content/local_features\n",
            "Data copied to: /content/local_features\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "def create_local_copy(drive_path: str) -> str:\n",
        "    # Create directory in /content/\n",
        "    local_path = '/content/local_features'\n",
        "\n",
        "    # Remove if already exists\n",
        "    if os.path.exists(local_path):\n",
        "        shutil.rmtree(local_path)\n",
        "\n",
        "    # Copy data from Drive to local\n",
        "    print(f\"Copying data to: {local_path}\")\n",
        "    shutil.copytree(drive_path, local_path)\n",
        "\n",
        "    return local_path\n",
        "\n",
        "# Example usage\n",
        "drive_path = '/path/to/Feature Extraction/ResNet-50'\n",
        "local_path = create_local_copy(drive_path)\n",
        "print(f\"Data copied to: {local_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import hashlib\n",
        "from collections import defaultdict\n",
        "\n",
        "def get_file_hash(filepath):\n",
        "    \"\"\"Calculate MD5 hash of a file.\"\"\"\n",
        "    hasher = hashlib.md5()\n",
        "    with open(filepath, 'rb') as file:\n",
        "        # Read file in chunks to handle large files efficiently\n",
        "        chunk = file.read(8192)\n",
        "        while chunk:\n",
        "            hasher.update(chunk)\n",
        "            chunk = file.read(8192)\n",
        "    return hasher.hexdigest()\n",
        "\n",
        "def remove_duplicates(folder_path, keep_first=True):\n",
        "    \"\"\"\n",
        "    Remove duplicate files from the specified folder.\n",
        "\n",
        "    Args:\n",
        "        folder_path (str): Path to the folder to check for duplicates\n",
        "        keep_first (bool): If True, keeps the first occurrence of a file\n",
        "\n",
        "    Returns:\n",
        "        list: List of deleted file paths\n",
        "    \"\"\"\n",
        "    # Dictionary to store file hashes and their paths\n",
        "    hash_dict = defaultdict(list)\n",
        "    deleted_files = []\n",
        "\n",
        "    # Walk through the directory\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "        for filename in files:\n",
        "            filepath = os.path.join(root, filename)\n",
        "            try:\n",
        "                file_hash = get_file_hash(filepath)\n",
        "                hash_dict[file_hash].append(filepath)\n",
        "            except (IOError, OSError) as e:\n",
        "                print(f\"Error processing {filepath}: {e}\")\n",
        "\n",
        "    # Remove duplicate files\n",
        "    for file_hash, file_list in hash_dict.items():\n",
        "        if len(file_list) > 1:  # If we found duplicates\n",
        "            # Sort files by creation time if you want to keep the oldest file\n",
        "            file_list.sort(key=lambda x: os.path.getctime(x))\n",
        "\n",
        "            # Keep the first file (or last if keep_first is False)\n",
        "            files_to_delete = file_list[1:] if keep_first else file_list[:-1]\n",
        "\n",
        "            for file_path in files_to_delete:\n",
        "                try:\n",
        "                    os.remove(file_path)\n",
        "                    deleted_files.append(file_path)\n",
        "                    print(f\"Deleted duplicate file: {file_path}\")\n",
        "                except OSError as e:\n",
        "                    print(f\"Error deleting {file_path}: {e}\")\n",
        "\n",
        "    return deleted_files\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    folder_path = \"/content/local_features\"  # Replace with your folder path\n",
        "    deleted = remove_duplicates(folder_path)\n",
        "    print(f\"\\nTotal files deleted: {len(deleted)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vp-amSYyx3D",
        "outputId": "3af8781f-2af2-4a86-f055-c1f44b4aef08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total files deleted: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqXxEQcs15mp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, learning_curve\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import balanced_accuracy_score, f1_score, precision_recall_fscore_support\n",
        "from sklearn.metrics import confusion_matrix, make_scorer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy.stats import loguniform\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from typing import Dict, Tuple, List\n",
        "import json\n",
        "import os\n",
        "import logging\n",
        "import glob\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import balanced_accuracy_score, f1_score, precision_recall_fscore_support\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import seaborn as sns\n",
        "from typing import Dict, Tuple, List\n",
        "import logging\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Custom scorer for weighted F1\n",
        "weighted_f1_scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "class DataLoader:\n",
        "    \"\"\"Handle loading and processing of NPZ files\"\"\"\n",
        "    def __init__(self, data_dir: str):\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "    def load_npz_files(self) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Load all NPZ files from directory and extract averaged_mean features and labels\n",
        "\n",
        "        Returns:\n",
        "            features: numpy array of averaged_mean features\n",
        "            labels: numpy array of fish species labels\n",
        "        \"\"\"\n",
        "        features_list = []\n",
        "        labels_list = []\n",
        "\n",
        "        # Get all NPZ files in directory\n",
        "        npz_files = glob.glob(os.path.join(self.data_dir, \"*.npz\"))\n",
        "\n",
        "        logging.info(f\"Found {len(npz_files)} NPZ files\")\n",
        "\n",
        "        for npz_file in npz_files:\n",
        "            try:\n",
        "                # Load NPZ file\n",
        "                data = np.load(npz_file, allow_pickle=True)\n",
        "\n",
        "                # Extract averaged_mean feature and label\n",
        "                features = data['features']  # Convert from np.ndarray to dict\n",
        "                fish_species = str(data['fish_species'].item())  # Convert to string\n",
        "\n",
        "                if features is not None:\n",
        "                    features_list.append(features)\n",
        "                    labels_list.append(fish_species)\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error processing file {npz_file}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Convert lists to numpy arrays\n",
        "        features_array = np.array(features_list)\n",
        "        labels_array = np.array(labels_list)\n",
        "\n",
        "        # Log data distribution\n",
        "        unique_labels, counts = np.unique(labels_array, return_counts=True)\n",
        "        for label, count in zip(unique_labels, counts):\n",
        "            percentage = (count / len(labels_array)) * 100\n",
        "            logging.info(f\"Class {label}: {count} samples ({percentage:.2f}%)\")\n",
        "\n",
        "        return features_array, labels_array\n",
        "\n",
        "\n",
        "class NumpyEncoder(json.JSONEncoder):\n",
        "    \"\"\"Custom encoder for numpy data types\"\"\"\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        elif isinstance(obj, np.bool_):\n",
        "            return bool(obj)\n",
        "        return super(NumpyEncoder, self).default(obj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv29RAFN2-45"
      },
      "source": [
        "# Fish Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkAubOk42pHA"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class FishClassifier:\n",
        "    def __init__(self, random_state: int = 42):\n",
        "        self.random_state = random_state\n",
        "        self.le = LabelEncoder()\n",
        "        self.setup_logging()\n",
        "\n",
        "    def setup_logging(self):\n",
        "        \"\"\"Setup logging configuration\"\"\"\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            filename='fish_classifier.log'\n",
        "        )\n",
        "\n",
        "    def prepare_data(self, features: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Prepare data by splitting into train and test sets with stratification\n",
        "        \"\"\"\n",
        "        # Encode labels\n",
        "        y = self.le.fit_transform(labels)\n",
        "\n",
        "        # Create stratified train-test split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            features,\n",
        "            y,\n",
        "            test_size=0.2,\n",
        "            random_state=self.random_state,\n",
        "            stratify=y\n",
        "        )\n",
        "\n",
        "        logging.info(f\"Training set size: {X_train.shape[0]}\")\n",
        "        logging.info(f\"Test set size: {X_test.shape[0]}\")\n",
        "\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "    def create_baseline_models(self) -> Dict:\n",
        "        \"\"\"Create baseline models with default parameters\"\"\"\n",
        "        models = {\n",
        "            'svm': LinearSVC(\n",
        "                random_state=self.random_state,\n",
        "                class_weight='balanced',\n",
        "                max_iter=2000  # Increased to ensure convergence\n",
        "            ),\n",
        "            'logistic': LogisticRegression(\n",
        "                random_state=self.random_state,\n",
        "                class_weight='balanced',\n",
        "                max_iter=2000\n",
        "            )\n",
        "        }\n",
        "        return models\n",
        "\n",
        "    def evaluate_model(self, model, X: np.ndarray, y: np.ndarray, model_name: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Evaluate model performance with multiple metrics\n",
        "        \"\"\"\n",
        "        # Get predictions\n",
        "        y_pred = model.predict(X)\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = {\n",
        "            'balanced_accuracy': balanced_accuracy_score(y, y_pred),\n",
        "            'macro_f1': f1_score(y, y_pred, average='macro'),\n",
        "            'confusion_matrix': confusion_matrix(y, y_pred),\n",
        "        }\n",
        "\n",
        "        # Calculate per-class metrics\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y, y_pred)\n",
        "\n",
        "        # Add per-class metrics\n",
        "        for i, class_name in enumerate(self.le.classes_):\n",
        "            metrics[f'{class_name}_precision'] = precision[i]\n",
        "            metrics[f'{class_name}_recall'] = recall[i]\n",
        "            metrics[f'{class_name}_f1'] = f1[i]\n",
        "\n",
        "        # Log results\n",
        "        logging.info(f\"\\nResults for {model_name}:\")\n",
        "        logging.info(f\"Balanced Accuracy: {metrics['balanced_accuracy']:.4f}\")\n",
        "        logging.info(f\"Macro F1: {metrics['macro_f1']:.4f}\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def plot_confusion_matrix(self, confusion_mat: np.ndarray, model_name: str):\n",
        "        \"\"\"\n",
        "        Plot confusion matrix heatmap\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(\n",
        "            confusion_mat,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='Blues',\n",
        "            xticklabels=self.le.classes_,\n",
        "            yticklabels=self.le.classes_\n",
        "        )\n",
        "        plt.title(f'Confusion Matrix - {model_name}')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.savefig(f'confusion_matrix_{model_name}.png')\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO_bGf6a3C-h"
      },
      "source": [
        "# Random Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wH0NBN812sz4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, learning_curve\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import balanced_accuracy_score, f1_score, precision_recall_fscore_support\n",
        "from sklearn.metrics import confusion_matrix, make_scorer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy.stats import loguniform\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from typing import Dict, Tuple, List\n",
        "import json\n",
        "import os\n",
        "import logging\n",
        "import glob\n",
        "from copy import deepcopy\n",
        "\n",
        "\n",
        "class ModelOptimizer:\n",
        "    \"\"\"\n",
        "    A class to handle model optimization for both SVM and Logistic Regression models.\n",
        "    Uses random search with balanced class weights.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, random_state: int = 42, n_iter: int = 100, class_names: List[str] = None):\n",
        "        self.random_state = random_state\n",
        "        self.n_iter = n_iter\n",
        "        self.class_names = class_names or ['Bleikja', 'Lax', 'Urridi']\n",
        "        self.setup_output_dir()\n",
        "        self.setup_logging()\n",
        "\n",
        "    def setup_logging(self):\n",
        "        \"\"\"Configure logging settings\"\"\"\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            handlers=[\n",
        "                logging.FileHandler(os.path.join(self.output_dir, 'optimization.log')),\n",
        "                logging.StreamHandler()\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def setup_output_dir(self):\n",
        "        \"\"\"Setup directory for saving results with timestamp\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        self.output_dir = f'model_optimization_{timestamp}'\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "        logging.info(f\"Created output directory: {self.output_dir}\")\n",
        "\n",
        "    def create_param_distributions(self) -> Dict:\n",
        "        \"\"\"\n",
        "        Create parameter distributions for random search.\n",
        "        Only varies C parameter, using balanced class weights.\n",
        "        \"\"\"\n",
        "        param_distributions = {\n",
        "            'C': loguniform(1e-1, 3e2),  # Wide range for C\n",
        "            'class_weight': ['balanced', None]  # Fixed to balanced weights\n",
        "        }\n",
        "\n",
        "        logging.info(\"Created parameter distributions for random search\")\n",
        "        return param_distributions\n",
        "\n",
        "    def run_random_search(self, model_class, X_train: np.ndarray, y_train: np.ndarray,\n",
        "                         X_test: np.ndarray, y_test: np.ndarray, model_name: str) -> Tuple[RandomizedSearchCV, pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Run random search with cross-validation for model optimization.\n",
        "        \"\"\"\n",
        "        logging.info(f\"Starting random search for {model_name}\")\n",
        "\n",
        "        # Create base model with appropriate parameters\n",
        "        if model_class == LinearSVC:\n",
        "            base_model = model_class(random_state=self.random_state, max_iter=2000)\n",
        "        else:  # LogisticRegression\n",
        "            base_model = model_class(random_state=self.random_state, max_iter=2000,\n",
        "                                   solver='lbfgs', penalty='l2')\n",
        "\n",
        "        # Create parameter distributions\n",
        "        param_distributions = self.create_param_distributions()\n",
        "\n",
        "        # Setup cross-validation\n",
        "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)\n",
        "\n",
        "        # Initialize random search\n",
        "        random_search = RandomizedSearchCV(\n",
        "            estimator=base_model,\n",
        "            param_distributions=param_distributions,\n",
        "            n_iter=self.n_iter,\n",
        "            cv=cv,\n",
        "            scoring=macro_f1_scorer,\n",
        "            n_jobs=-1,\n",
        "            random_state=self.random_state,\n",
        "            verbose=2,\n",
        "            return_train_score=True\n",
        "        )\n",
        "\n",
        "        # Fit random search\n",
        "        random_search.fit(X_train, y_train)\n",
        "\n",
        "        # Create results DataFrame\n",
        "        results_df = pd.DataFrame(random_search.cv_results_)\n",
        "\n",
        "        # Save results\n",
        "        self.save_results(random_search, results_df, model_name)\n",
        "\n",
        "        logging.info(f\"Completed random search for {model_name}\")\n",
        "        return random_search, results_df\n",
        "\n",
        "    def save_results(self, random_search: RandomizedSearchCV, results_df: pd.DataFrame, model_name: str):\n",
        "        \"\"\"Save random search results to files with consistent metrics.\"\"\"\n",
        "        model_dir = os.path.join(self.output_dir, model_name)\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "        # Get best parameters and scores\n",
        "        best_params = random_search.best_params_\n",
        "        best_cv_score = random_search.best_score_\n",
        "\n",
        "        # Find the row with best CV score for consistency check\n",
        "        best_idx = results_df['mean_test_score'].idxmax()\n",
        "        best_row = results_df.loc[best_idx]\n",
        "\n",
        "        # Create comprehensive summary\n",
        "        summary = {\n",
        "            'best_parameters': best_params,\n",
        "            'cross_validation_performance': {\n",
        "                'best_score': best_cv_score,\n",
        "                'std_score': best_row['std_test_score'],\n",
        "                'train_score': best_row['mean_train_score'],\n",
        "                'train_std': best_row['std_train_score']\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Save summary as JSON\n",
        "        with open(os.path.join(model_dir, 'best_params.json'), 'w') as f:\n",
        "            json.dump(summary, f, indent=4)\n",
        "\n",
        "        # Save full results DataFrame\n",
        "        results_df.to_csv(os.path.join(model_dir, 'random_search_results.csv'))\n",
        "\n",
        "        # Save readable summary\n",
        "        with open(os.path.join(model_dir, 'performance_summary.txt'), 'w') as f:\n",
        "            f.write(\"Best Model Configuration\\n\")\n",
        "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "            f.write(\"Parameters:\\n\")\n",
        "            for param, value in best_params.items():\n",
        "                f.write(f\"{param}: {value}\\n\")\n",
        "            f.write(\"\\nCross-validation Performance:\\n\")\n",
        "            f.write(f\"Best CV Score (Macro F1): {best_cv_score:.4f} ± {best_row['std_test_score']:.4f}\\n\")\n",
        "            f.write(f\"CV Training Score: {best_row['mean_train_score']:.4f} ± {best_row['std_train_score']:.4f}\\n\")\n",
        "\n",
        "        logging.info(f\"Saved optimization results for {model_name} to {model_dir}\")\n",
        "\n",
        "\n",
        "    def evaluate_model(self, model, X: np.ndarray, y: np.ndarray, model_name: str) -> Dict:\n",
        "        \"\"\"Evaluate model performance with multiple metrics.\"\"\"\n",
        "        # Get predictions\n",
        "        y_pred = model.predict(X)\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = {\n",
        "            'weighted_f1': f1_score(y, y_pred, average='weighted'),\n",
        "            'macro_f1': f1_score(y, y_pred, average='macro'),\n",
        "            'balanced_accuracy': balanced_accuracy_score(y, y_pred),\n",
        "            'confusion_matrix': confusion_matrix(y, y_pred)\n",
        "        }\n",
        "\n",
        "        # Calculate per-class metrics\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y, y_pred)\n",
        "\n",
        "        # Add per-class metrics\n",
        "        for i, class_name in enumerate(self.class_names):\n",
        "            metrics[f'{class_name}_precision'] = precision[i]\n",
        "            metrics[f'{class_name}_recall'] = recall[i]\n",
        "            metrics[f'{class_name}_f1'] = f1[i]\n",
        "\n",
        "        # Log results\n",
        "        logging.info(f\"\\nResults for {model_name}:\")\n",
        "        logging.info(f\"Weighted F1: {metrics['weighted_f1']:.4f}\")\n",
        "        logging.info(f\"Macro F1: {metrics['macro_f1']:.4f}\")\n",
        "        logging.info(f\"Balanced Accuracy: {metrics['balanced_accuracy']:.4f}\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def plot_results(self, results_df: pd.DataFrame, model_name: str):\n",
        "        \"\"\"Create clear visualization of random search results with verified metrics.\"\"\"\n",
        "        plt.figure(figsize=(15, 12))\n",
        "\n",
        "        # Sort results by C parameter for smooth plotting\n",
        "        results_df = results_df.sort_values('param_C')\n",
        "\n",
        "        # Plot 1: Main Performance Plot\n",
        "        plt.subplot(211)\n",
        "\n",
        "        # Plot mean CV scores with error bands\n",
        "        plt.semilogx(results_df['param_C'],\n",
        "                     results_df['mean_test_score'],\n",
        "                     'b-',\n",
        "                     label='Cross-validation Score',\n",
        "                     linewidth=2)\n",
        "        plt.fill_between(results_df['param_C'],\n",
        "                        results_df['mean_test_score'] - results_df['std_test_score'],\n",
        "                        results_df['mean_test_score'] + results_df['std_test_score'],\n",
        "                        alpha=0.2,\n",
        "                        color='b')\n",
        "\n",
        "        plt.semilogx(results_df['param_C'],\n",
        "                     results_df['mean_train_score'],\n",
        "                     'r-',\n",
        "                     label='Training Score',\n",
        "                     linewidth=2)\n",
        "        plt.fill_between(results_df['param_C'],\n",
        "                        results_df['mean_train_score'] - results_df['std_train_score'],\n",
        "                        results_df['mean_train_score'] + results_df['std_train_score'],\n",
        "                        alpha=0.2,\n",
        "                        color='r')\n",
        "\n",
        "        # Highlight best performing point\n",
        "        best_idx = results_df['mean_test_score'].idxmax()\n",
        "        best_C = results_df.loc[best_idx, 'param_C']\n",
        "        best_score = results_df.loc[best_idx, 'mean_test_score']\n",
        "        best_score_std = results_df.loc[best_idx, 'std_test_score']\n",
        "\n",
        "        plt.plot(best_C, best_score, 'k*', markersize=15,\n",
        "                label=f'Best C = {best_C:.2e}')\n",
        "\n",
        "        plt.xlabel('C Parameter (log scale)')\n",
        "        plt.ylabel('Macro F1 Score')\n",
        "        plt.title(f'{model_name}: Impact of C Parameter on Model Performance')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Plot 2: Overfitting Analysis\n",
        "        plt.subplot(212)\n",
        "\n",
        "        # Calculate train-test gap\n",
        "        train_test_gap = results_df['mean_train_score'] - results_df['mean_test_score']\n",
        "\n",
        "        plt.semilogx(results_df['param_C'], train_test_gap, 'g-',\n",
        "                     label='Train-CV Gap', linewidth=2)\n",
        "        plt.fill_between(results_df['param_C'],\n",
        "                        train_test_gap - results_df['std_test_score'],\n",
        "                        train_test_gap + results_df['std_test_score'],\n",
        "                        alpha=0.2,\n",
        "                        color='g')\n",
        "\n",
        "        plt.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
        "        plt.xlabel('C Parameter (log scale)')\n",
        "        plt.ylabel('Train-CV Score Gap')\n",
        "        plt.title('Overfitting Analysis: Train-CV Score Gap vs C')\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Add text box with verified metrics\n",
        "        textstr = '\\n'.join([\n",
        "            f'Best Configuration:',\n",
        "            f'C = {best_C:.2e}',\n",
        "            f'CV Score = {best_score:.4f} ± {best_score_std:.4f}',\n",
        "            f'Train Score = {results_df.loc[best_idx, \"mean_train_score\"]:.4f}'\n",
        "        ])\n",
        "\n",
        "        plt.text(0.02, 0.98, textstr,\n",
        "                transform=plt.gca().transAxes,\n",
        "                bbox=dict(facecolor='white', alpha=0.8),\n",
        "                verticalalignment='top')\n",
        "\n",
        "\n",
        "\n",
        "class MultiSeedModelOptimizer(ModelOptimizer):\n",
        "    \"\"\"Extended ModelOptimizer class to handle multiple random seeds\"\"\"\n",
        "\n",
        "    def __init__(self, base_seed: int = 42, n_seeds: int = 10, n_iter: int = 30,\n",
        "                 class_names: List[str] = None):\n",
        "        # Generate random seeds\n",
        "        rng = np.random.RandomState(base_seed)\n",
        "        self.seeds = np.random.choice(np.arange(1, 101), size=n_seeds, replace=False)\n",
        "        self.n_seeds = n_seeds\n",
        "\n",
        "        # Initialize with first seed\n",
        "        super().__init__(random_state=self.seeds[0], n_iter=n_iter,\n",
        "                        class_names=class_names)\n",
        "\n",
        "        # Modify output directory to indicate multiple seeds\n",
        "        self.output_dir = f'{self.output_dir}_multiseed'\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "        # Save seeds information\n",
        "        with open(os.path.join(self.output_dir, 'random_seeds.json'), 'w') as f:\n",
        "            json.dump({'base_seed': base_seed, 'generated_seeds': self.seeds.tolist()}, f)\n",
        "\n",
        "    def run_multi_seed_optimization(self, model_class, X_train: np.ndarray,\n",
        "                                  y_train: np.ndarray, X_test: np.ndarray,\n",
        "                                  y_test: np.ndarray, model_name: str) -> Dict:\n",
        "        \"\"\"Run random search optimization across multiple seeds\"\"\"\n",
        "        all_seed_results = {}\n",
        "\n",
        "        for seed_idx, seed in enumerate(self.seeds):\n",
        "            logging.info(f\"\\nRunning optimization for {model_name} with seed {seed} \"\n",
        "                        f\"({seed_idx + 1}/{self.n_seeds})\")\n",
        "\n",
        "            # Update random state\n",
        "            self.random_state = seed\n",
        "\n",
        "            # Use existing seed directory if it exists, create if it doesn't\n",
        "            seed_dir = os.path.join(self.output_dir, f'seed_{seed}')\n",
        "            os.makedirs(seed_dir, exist_ok=True)\n",
        "\n",
        "            try:\n",
        "                # Run random search for this seed\n",
        "                random_search, results_df = self.run_random_search(\n",
        "                    model_class, X_train, y_train, X_test, y_test,\n",
        "                    model_name  # Removed the seed suffix from model name\n",
        "                )\n",
        "\n",
        "                # Get best model for this seed\n",
        "                best_model = random_search.best_estimator_\n",
        "\n",
        "                # Evaluate best model\n",
        "                train_metrics = self.evaluate_model(\n",
        "                    best_model, X_train, y_train, f\"{model_name}_train\"\n",
        "                )\n",
        "                test_metrics = self.evaluate_model(\n",
        "                    best_model, X_test, y_test, f\"{model_name}_test\"\n",
        "                )\n",
        "\n",
        "                # Store results for this seed\n",
        "                if seed not in all_seed_results:\n",
        "                    all_seed_results[seed] = {}\n",
        "\n",
        "                all_seed_results[seed][model_name] = {\n",
        "                    'best_model': best_model,\n",
        "                    'best_params': random_search.best_params_,\n",
        "                    'cv_score': random_search.best_score_,\n",
        "                    'train_metrics': train_metrics,\n",
        "                    'test_metrics': test_metrics,\n",
        "                    'results_df': results_df\n",
        "                }\n",
        "\n",
        "                # Save results for this model within the seed directory\n",
        "                self.save_seed_results(\n",
        "                    all_seed_results[seed][model_name],\n",
        "                    seed_dir,\n",
        "                    model_name\n",
        "                )\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error during {model_name} optimization with seed {seed}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Generate and save summary across seeds\n",
        "        self.generate_seed_summary(all_seed_results, model_name)\n",
        "\n",
        "        return all_seed_results\n",
        "\n",
        "    def save_seed_results(self, results: Dict, seed_dir: str, model_name: str):\n",
        "        \"\"\"Save results for a specific model within a seed directory\"\"\"\n",
        "        # Save metrics\n",
        "        metrics_summary = {\n",
        "            'best_params': results['best_params'],\n",
        "            'cv_score': results['cv_score'],\n",
        "            'train_metrics': results['train_metrics'],\n",
        "            'test_metrics': results['test_metrics']\n",
        "        }\n",
        "\n",
        "        # Save within the seed directory with model-specific names\n",
        "        with open(os.path.join(seed_dir, f'{model_name}_metrics.json'), 'w') as f:\n",
        "            json.dump(metrics_summary, f, indent=4, cls=NumpyEncoder)\n",
        "\n",
        "        # Save results DataFrame\n",
        "        results['results_df'].to_csv(\n",
        "            os.path.join(seed_dir, f'{model_name}_results.csv')\n",
        "        )\n",
        "\n",
        "    def plot_averaged_validation_curves(self, all_results: Dict[str, Dict]):\n",
        "        \"\"\"\n",
        "        Create a comprehensive plot showing averaged validation curves across all seeds\n",
        "        for both SVM and Logistic Regression models.\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(15, 8))\n",
        "\n",
        "        # Define colors and styles\n",
        "        colors = {\n",
        "            'SVM': 'blue',\n",
        "            'LogisticRegression': 'red'\n",
        "        }\n",
        "\n",
        "        # Process each model's results\n",
        "        for model_name in ['SVM', 'LogisticRegression']:\n",
        "            logging.info(f\"Processing validation curves for {model_name}\")\n",
        "            model_results = all_results[model_name]\n",
        "\n",
        "            # Initialize lists to store scores for each C value\n",
        "            c_values = set()\n",
        "            train_scores_dict = {}\n",
        "            val_scores_dict = {}\n",
        "\n",
        "            # Collect all unique C values and corresponding scores across seeds\n",
        "            for seed_results in model_results.values():\n",
        "                results_df = seed_results['results_df']\n",
        "\n",
        "                # Get all C values from this seed\n",
        "                for idx, row in results_df.iterrows():\n",
        "                    c = row['param_C']\n",
        "                    c_values.add(c)\n",
        "\n",
        "                    # Initialize lists for this C value if not exists\n",
        "                    if c not in train_scores_dict:\n",
        "                        train_scores_dict[c] = []\n",
        "                        val_scores_dict[c] = []\n",
        "\n",
        "                    # Append scores\n",
        "                    train_scores_dict[c].append(row['mean_train_score'])\n",
        "                    val_scores_dict[c].append(row['mean_test_score'])\n",
        "\n",
        "            # Convert to sorted list\n",
        "            c_values = sorted(list(c_values))\n",
        "\n",
        "            # Calculate means and stds\n",
        "            train_means = []\n",
        "            train_stds = []\n",
        "            val_means = []\n",
        "            val_stds = []\n",
        "\n",
        "            for c in c_values:\n",
        "                train_means.append(np.mean(train_scores_dict[c]))\n",
        "                train_stds.append(np.std(train_scores_dict[c]))\n",
        "                val_means.append(np.mean(val_scores_dict[c]))\n",
        "                val_stds.append(np.std(val_scores_dict[c]))\n",
        "\n",
        "            # Convert to numpy arrays\n",
        "            train_means = np.array(train_means)\n",
        "            train_stds = np.array(train_stds)\n",
        "            val_means = np.array(val_means)\n",
        "            val_stds = np.array(val_stds)\n",
        "\n",
        "            # Plot training scores with dashed lines\n",
        "            plt.semilogx(c_values, train_means, '--',\n",
        "                        color=colors[model_name],\n",
        "                        label=f'{model_name} Training',\n",
        "                        alpha=0.8)\n",
        "            plt.fill_between(c_values,\n",
        "                            train_means - train_stds,\n",
        "                            train_means + train_stds,\n",
        "                            color=colors[model_name],\n",
        "                            alpha=0.1)\n",
        "\n",
        "            # Plot validation scores with solid lines\n",
        "            plt.semilogx(c_values, val_means, '-',\n",
        "                        color=colors[model_name],\n",
        "                        label=f'{model_name} Validation',\n",
        "                        alpha=0.8)\n",
        "            plt.fill_between(c_values,\n",
        "                            val_means - val_stds,\n",
        "                            val_means + val_stds,\n",
        "                            color=colors[model_name],\n",
        "                            alpha=0.1)\n",
        "\n",
        "        plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
        "        plt.xlabel('C Parameter (log scale)')\n",
        "        plt.ylabel('Score')\n",
        "        plt.title('Validation Curves: Averaged Across Seeds\\n'\n",
        "                  'Solid: Validation, Dashed: Training')\n",
        "        plt.legend(loc='lower right')\n",
        "\n",
        "        # Add text box with number of seeds\n",
        "        plt.text(0.02, 0.98, f'Averaged across {self.n_seeds} seeds',\n",
        "                 transform=plt.gca().transAxes,\n",
        "                 bbox=dict(facecolor='white', alpha=0.8),\n",
        "                 verticalalignment='top')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save the plot\n",
        "        plot_path = os.path.join(self.output_dir, 'averaged_validation_curves.png')\n",
        "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "        logging.info(f\"Saved validation curves plot to {plot_path}\")\n",
        "        plt.close()\n",
        "\n",
        "    def generate_seed_summary(self, all_results: Dict, model_name: str):\n",
        "        \"\"\"Generate summary statistics across all seeds\"\"\"\n",
        "        # Collect metrics across seeds\n",
        "        cv_scores = []\n",
        "        test_scores = []\n",
        "        c_values = []\n",
        "\n",
        "        # Access the correct level of the dictionary\n",
        "        for seed_results in all_results.values():\n",
        "            model_results = seed_results[model_name]  # Get model-specific results\n",
        "            cv_scores.append(model_results['cv_score'])\n",
        "            test_scores.append(model_results['test_metrics']['weighted_f1'])\n",
        "            c_values.append(model_results['best_params']['C'])\n",
        "\n",
        "        # Calculate summary statistics\n",
        "        summary = {\n",
        "            'cv_score': {\n",
        "                'mean': np.mean(cv_scores),\n",
        "                'std': np.std(cv_scores),\n",
        "                'min': np.min(cv_scores),\n",
        "                'max': np.max(cv_scores)\n",
        "            },\n",
        "            'test_score': {\n",
        "                'mean': np.mean(test_scores),\n",
        "                'std': np.std(test_scores),\n",
        "                'min': np.min(test_scores),\n",
        "                'max': np.max(test_scores)\n",
        "            },\n",
        "            'c_value': {\n",
        "                'mean': np.mean(c_values),\n",
        "                'std': np.std(c_values),\n",
        "                'min': np.min(c_values),\n",
        "                'max': np.max(c_values)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Save summary\n",
        "        with open(os.path.join(self.output_dir, f'{model_name}_seed_summary.json'), 'w') as f:\n",
        "            json.dump(summary, f, indent=4)\n",
        "\n",
        "        # Create visualization of results across seeds\n",
        "        self.plot_seed_comparison(cv_scores, test_scores, c_values, model_name)\n",
        "\n",
        "    def plot_seed_comparison(self, cv_scores: List[float], test_scores: List[float],\n",
        "                           c_values: List[float], model_name: str):\n",
        "        \"\"\"Create visualization comparing results across seeds\"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "        # Plot 1: CV vs Test Scores\n",
        "        ax1.scatter(cv_scores, test_scores, alpha=0.6)\n",
        "        ax1.plot([min(cv_scores), max(cv_scores)], [min(cv_scores), max(cv_scores)],\n",
        "                 'k--', alpha=0.5)\n",
        "        ax1.set_xlabel('CV Score')\n",
        "        ax1.set_ylabel('Test Score')\n",
        "        ax1.set_title('CV vs Test Score Comparison')\n",
        "\n",
        "        # Plot 2: C Value Distribution\n",
        "        ax2.hist(np.log10(c_values), bins=10)\n",
        "        ax2.set_xlabel('log10(C)')\n",
        "        ax2.set_ylabel('Count')\n",
        "        ax2.set_title('Distribution of Best C Values')\n",
        "\n",
        "        plt.suptitle(f'{model_name}: Results Across {self.n_seeds} Seeds')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, f'{model_name}_seed_comparison.png'))\n",
        "        plt.close()\n",
        "\n",
        "def run_multi_seed_optimization(data_dir: str, class_names: List[str] = None,\n",
        "                              base_seed: int = 42, n_seeds: int = 10) -> Dict:\n",
        "    \"\"\"Run complete random search optimization pipeline across multiple seeds\"\"\"\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
        "    )\n",
        "\n",
        "    logging.info(\"Starting multi-seed optimization pipeline\")\n",
        "\n",
        "    try:\n",
        "        # Load and prepare data\n",
        "        data_loader = DataLoader(data_dir)\n",
        "        features, labels = data_loader.load_npz_files()\n",
        "\n",
        "        # Get unique class names if not provided\n",
        "        if class_names is None:\n",
        "            class_names = np.unique(labels).tolist()\n",
        "\n",
        "        # Initialize classifier for data preparation\n",
        "        classifier = FishClassifier()\n",
        "        X_train, X_test, y_train, y_test = classifier.prepare_data(features, labels)\n",
        "\n",
        "        # Initialize multi-seed optimizer\n",
        "        optimizer = MultiSeedModelOptimizer(\n",
        "            base_seed=base_seed,\n",
        "            n_seeds=n_seeds,\n",
        "            n_iter=30,\n",
        "            class_names=class_names\n",
        "        )\n",
        "\n",
        "        # Dictionary to store results\n",
        "        all_results = {}\n",
        "\n",
        "        # Run optimization for both models\n",
        "        models = {\n",
        "            'SVM': LinearSVC,\n",
        "            'LogisticRegression': LogisticRegression\n",
        "        }\n",
        "\n",
        "        for model_name, model_class in models.items():\n",
        "            logging.info(f\"\\nStarting multi-seed optimization for {model_name}\")\n",
        "\n",
        "            # Run multi-seed optimization\n",
        "            model_results = optimizer.run_multi_seed_optimization(\n",
        "                model_class, X_train, y_train, X_test, y_test, model_name\n",
        "            )\n",
        "\n",
        "            all_results[model_name] = model_results\n",
        "\n",
        "        # Generate averaged validation curves plot\n",
        "        logging.info(\"Generating averaged validation curves across seeds...\")\n",
        "        optimizer.plot_averaged_validation_curves(all_results)\n",
        "\n",
        "        # Save and zip results\n",
        "        output_dir = optimizer.output_dir\n",
        "        os.system(f'zip -r {output_dir}.zip {output_dir}')\n",
        "        logging.info(f\"\\nResults saved to {output_dir}.zip\")\n",
        "\n",
        "        return all_results\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in optimization pipeline: {str(e)}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJUXEvP7216P"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Directory settings\n",
        "data_dir = \"/path/to/Feature Extraction/ViT-SO400M-14-SigLIP-mean-frames\"\n",
        "class_names = ['Bleikja', 'Lax', 'Urridi']\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Run the optimization\n",
        "try:\n",
        "    print(\"Starting multi-seed optimization...\")\n",
        "\n",
        "    results = run_multi_seed_optimization(\n",
        "        data_dir=data_dir,\n",
        "        class_names=class_names,\n",
        "        base_seed=42,\n",
        "        n_seeds=10\n",
        "    )\n",
        "\n",
        "    print(\"\\nOptimization completed successfully!\")\n",
        "\n",
        "    # Print summary of results\n",
        "    for model_name in ['SVM', 'LogisticRegression']:\n",
        "        print(f\"\\nSummary for {model_name}:\")\n",
        "        summary_file = f\"model_optimization_multiseed/{model_name}_seed_summary.json\"\n",
        "\n",
        "        if os.path.exists(summary_file):\n",
        "            with open(summary_file, 'r') as f:\n",
        "                summary = json.load(f)\n",
        "\n",
        "            print(\"\\nCV Scores:\")\n",
        "            print(f\"Mean: {summary['cv_score']['mean']:.4f} ± {summary['cv_score']['std']:.4f}\")\n",
        "            print(f\"Range: [{summary['cv_score']['min']:.4f}, {summary['cv_score']['max']:.4f}]\")\n",
        "\n",
        "            print(\"\\nTest Scores:\")\n",
        "            print(f\"Mean: {summary['test_score']['mean']:.4f} ± {summary['test_score']['std']:.4f}\")\n",
        "            print(f\"Range: [{summary['test_score']['min']:.4f}, {summary['test_score']['max']:.4f}]\")\n",
        "\n",
        "            print(\"\\nC Values:\")\n",
        "            print(f\"Mean: {summary['c_value']['mean']:.4f} ± {summary['c_value']['std']:.4f}\")\n",
        "            print(f\"Range: [{summary['c_value']['min']:.4f}, {summary['c_value']['max']:.4f}]\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during optimization: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Display generated plots\n",
        "try:\n",
        "    output_dir = \"model_optimization_multiseed\"\n",
        "\n",
        "    # Display validation curves\n",
        "    validation_curves_path = os.path.join(output_dir, 'averaged_validation_curves.png')\n",
        "    if os.path.exists(validation_curves_path):\n",
        "        img = plt.imread(validation_curves_path)\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title('Averaged Validation Curves')\n",
        "        plt.show()\n",
        "\n",
        "    # Display seed comparison plots\n",
        "    for model_name in ['SVM', 'LogisticRegression']:\n",
        "        comparison_plot_path = os.path.join(output_dir, f'{model_name}_seed_comparison.png')\n",
        "        if os.path.exists(comparison_plot_path):\n",
        "            img = plt.imread(comparison_plot_path)\n",
        "            plt.figure(figsize=(15, 6))\n",
        "            plt.imshow(img)\n",
        "            plt.axis('off')\n",
        "            plt.title(f'{model_name} Seed Comparison')\n",
        "            plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error displaying results: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbzcqZZAFiND"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "def zip_folder(folder_path, output_zip_path):\n",
        "    \"\"\"\n",
        "    Create a zip file from a folder in Google Colab.\n",
        "\n",
        "    Args:\n",
        "        folder_path (str): Path to the folder you want to zip\n",
        "        output_zip_path (str): Path where you want to save the zip file\n",
        "    \"\"\"\n",
        "    # Make sure the folder exists\n",
        "    if not os.path.exists(folder_path):\n",
        "        raise ValueError(f\"Folder {folder_path} does not exist\")\n",
        "\n",
        "    # Create the zip file\n",
        "    shutil.make_archive(\n",
        "        base_name=output_zip_path.replace('.zip', ''),\n",
        "        format='zip',\n",
        "        root_dir=os.path.dirname(folder_path),\n",
        "        base_dir=os.path.basename(folder_path)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0pYSPJxFjLk"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "folder_to_zip = '/path/to/model_optimization_20241211_120612_multiseed'  # Path to your folder\n",
        "output_zip = '/path/to/model_optimization_20241211_120612_multiseed.zip'  # Where to save the zip file\n",
        "\n",
        "zip_folder(folder_to_zip, output_zip)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Full implementation"
      ],
      "metadata": {
        "id": "ivzrg8r-fhRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(os.listdir('/content/local_features'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DDe5mOqVKYd",
        "outputId": "bd0e1630-b839-41b4-f3e2-1449187cef67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7971"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('/content/local_features')[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pceS1RdYy8pK",
        "outputId": "9b706301-e2f5-4e76-c73e-745fe88088e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'krossa2022_vid_1199_Urriði_frame_104_resnet_features.npz'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and setup\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import balanced_accuracy_score, f1_score, precision_recall_fscore_support\n",
        "from sklearn.metrics import confusion_matrix, make_scorer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy.stats import loguniform\n",
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import json\n",
        "import glob\n",
        "from tqdm.notebook import tqdm, trange\n",
        "import shutil\n",
        "from joblib import Parallel, delayed, parallel_backend\n",
        "from typing import Dict, Tuple, List  # Added this import\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Custom scorer for weighted F1\n",
        "macro_f1_scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "class DataLoader:\n",
        "    \"\"\"Handle loading and processing of NPZ files with progress tracking\"\"\"\n",
        "    def __init__(self, data_dir: str):\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "    def load_npz_files(self) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Load NPZ files with progress bar\"\"\"\n",
        "        npz_files = glob.glob(os.path.join(self.data_dir, \"*.npz\"))\n",
        "        features_list = []\n",
        "        labels_list = []\n",
        "\n",
        "        # Set up progress bar\n",
        "        pbar = tqdm(npz_files, desc=\"Loading data\", unit=\"file\")\n",
        "\n",
        "        for npz_file in pbar:\n",
        "            try:\n",
        "                data = np.load(npz_file, allow_pickle=True)\n",
        "                features = data['features']\n",
        "                fish_species = str(data['fish_species'].item())\n",
        "\n",
        "                if features is not None:\n",
        "                    features_list.append(features)\n",
        "                    labels_list.append(fish_species)\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error processing {npz_file}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        features_array = np.array(features_list)\n",
        "        labels_array = np.array(labels_list)\n",
        "\n",
        "        # Log data distribution\n",
        "        unique_labels, counts = np.unique(labels_array, return_counts=True)\n",
        "        print(\"\\nFeature matrix shape:\", features_array.shape)\n",
        "        print(\"Label vector shape:\", labels_array.shape)\n",
        "        print(\"\\nClass distribution:\")\n",
        "        for label, count in zip(unique_labels, counts):\n",
        "            print(f\"{label}: {count} samples\")\n",
        "\n",
        "        return features_array, labels_array\n",
        "\n",
        "class FishClassifier:\n",
        "    \"\"\"Base classifier with optimized evaluation\"\"\"\n",
        "    def __init__(self, random_state: int = 42):\n",
        "        self.random_state = random_state\n",
        "        self.le = LabelEncoder()\n",
        "\n",
        "    def prepare_data(self, features: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"Prepare data with stratified split\"\"\"\n",
        "        y = self.le.fit_transform(labels)\n",
        "        return train_test_split(features, y, test_size=0.2, random_state=self.random_state, stratify=y)\n",
        "\n",
        "    @staticmethod\n",
        "    def evaluate_model(model, X: np.ndarray, y: np.ndarray, class_names: list) -> Dict:\n",
        "        \"\"\"Fast model evaluation with all metrics\"\"\"\n",
        "        y_pred = model.predict(X)\n",
        "\n",
        "        metrics = {\n",
        "            'balanced_accuracy': balanced_accuracy_score(y, y_pred),\n",
        "            'macro_f1': f1_score(y, y_pred, average='macro'),\n",
        "            'weighted_f1': f1_score(y, y_pred, average='weighted'),\n",
        "            'confusion_matrix': confusion_matrix(y, y_pred)\n",
        "        }\n",
        "\n",
        "        # Calculate per-class metrics\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y, y_pred)\n",
        "\n",
        "        for i, class_name in enumerate(class_names):\n",
        "            metrics[f'{class_name}_precision'] = precision[i]\n",
        "            metrics[f'{class_name}_recall'] = recall[i]\n",
        "            metrics[f'{class_name}_f1'] = f1[i]\n",
        "\n",
        "        return metrics\n",
        "\n",
        "def optimize_single_seed(X, y, model_type, seed, param_range, class_names, n_splits=5):\n",
        "    \"\"\"Optimized single seed evaluation\"\"\"\n",
        "    # Set random states\n",
        "    np.random.seed(seed)\n",
        "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "\n",
        "    # Initialize model\n",
        "    if model_type == \"SVM\":\n",
        "        base_model = LinearSVC(random_state=seed, max_iter=2000)\n",
        "    else:\n",
        "        base_model = LogisticRegression(random_state=seed, max_iter=2000)\n",
        "\n",
        "    # Setup random search with proper scoring\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=base_model,\n",
        "        param_distributions={'C': param_range, 'class_weight': ['balanced']},\n",
        "        n_iter=20,\n",
        "        cv=cv,\n",
        "        scoring='f1_macro',  # Changed to direct f1_macro scoring\n",
        "        n_jobs=-1,\n",
        "        random_state=seed,\n",
        "        verbose=0,\n",
        "        return_train_score=True\n",
        "    )\n",
        "\n",
        "    # Fit with manual progress tracking\n",
        "    with tqdm(total=20, desc=f\"Optimizing {model_type} (seed {seed})\") as pbar:\n",
        "        random_search.fit(X, y)\n",
        "        pbar.update(20)  # Update after completion\n",
        "\n",
        "    # Get best model and evaluate\n",
        "    final_model = random_search.best_estimator_\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
        "    final_model.fit(X_train, y_train)\n",
        "\n",
        "    # Calculate metrics\n",
        "    train_metrics = FishClassifier.evaluate_model(final_model, X_train, y_train, class_names)\n",
        "    test_metrics = FishClassifier.evaluate_model(final_model, X_test, y_test, class_names)\n",
        "\n",
        "    print(f\"\\nResults for {model_type}, seed {seed}:\")\n",
        "    print(f\"CV Score: {random_search.best_score_:.4f}\")\n",
        "    print(f\"Test Score: {test_metrics['macro_f1']:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'model': final_model,\n",
        "        'best_params': random_search.best_params_,\n",
        "        'cv_score': random_search.best_score_,\n",
        "        'train_metrics': train_metrics,\n",
        "        'test_metrics': test_metrics,\n",
        "        'all_cv_results': pd.DataFrame(random_search.cv_results_)\n",
        "    }\n",
        "\n",
        "def run_multi_seed_optimization(data_dir: str, class_names: List[str] = None,\n",
        "                              base_seed: int = 42, n_seeds: int = 10) -> Dict:\n",
        "    \"\"\"Main optimization pipeline with progress tracking\"\"\"\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "    print(\"Starting multi-seed optimization...\")\n",
        "\n",
        "    try:\n",
        "        # Load data\n",
        "        data_loader = DataLoader(data_dir)\n",
        "        features, labels = data_loader.load_npz_files()\n",
        "\n",
        "        if class_names is None:\n",
        "            class_names = np.unique(labels).tolist()\n",
        "\n",
        "        # Initialize structures\n",
        "        param_range = loguniform(1e-1, 3e2)\n",
        "        all_results = {'SVM': {}, 'LogisticRegression': {}}\n",
        "\n",
        "        # Generate seeds\n",
        "        np.random.seed(base_seed)\n",
        "        seeds = seeds = [4, 15, 29, 30, 32, 37, 38, 65, 88, 91]\n",
        "\n",
        "        # Setup output directory\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        output_dir = f'model_optimization_{timestamp}_multiseed'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Run optimization for each model type\n",
        "        for model_type in ['SVM', 'LogisticRegression']:\n",
        "            print(f\"\\nOptimizing {model_type} models across {n_seeds} seeds...\")\n",
        "\n",
        "            # Process seeds sequentially for better progress tracking\n",
        "            for seed in seeds:\n",
        "                result = optimize_single_seed(features, labels, model_type, seed, param_range, class_names)\n",
        "                all_results[model_type][seed] = result\n",
        "\n",
        "                # Save results\n",
        "                seed_dir = os.path.join(output_dir, f'seed_{seed}')\n",
        "                os.makedirs(seed_dir, exist_ok=True)\n",
        "\n",
        "                # Save metrics\n",
        "                metrics_path = os.path.join(seed_dir, f'{model_type}_metrics.json')\n",
        "                metrics = {\n",
        "                    'best_params': result['best_params'],\n",
        "                    'cv_score': result['cv_score'],\n",
        "                    'train_metrics': result['train_metrics'],\n",
        "                    'test_metrics': result['test_metrics']\n",
        "                }\n",
        "                with open(metrics_path, 'w') as f:\n",
        "                    json.dump(metrics, f, cls=NumpyEncoder)\n",
        "\n",
        "                # Save CV results\n",
        "                result['all_cv_results'].to_csv(os.path.join(seed_dir, f'{model_type}_results.csv'))\n",
        "\n",
        "        # Generate and save summary plots\n",
        "        plot_validation_curves(all_results, output_dir)\n",
        "        generate_seed_summary(all_results, output_dir, class_names)\n",
        "\n",
        "        # Zip results\n",
        "        shutil.make_archive(output_dir, 'zip', output_dir)\n",
        "        print(f\"\\nResults saved to {output_dir}.zip\")\n",
        "\n",
        "        return all_results\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in optimization pipeline: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "class NumpyEncoder(json.JSONEncoder):\n",
        "    \"\"\"JSON encoder for numpy types\"\"\"\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, (np.integer, np.floating, np.bool_)):\n",
        "            return obj.item()\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        return super().default(obj)\n",
        "\n",
        "def plot_validation_curves(all_results: Dict, output_dir: str):\n",
        "    \"\"\"Generate validation curves plot with mean and std bands\"\"\"\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    colors = {'SVM': 'blue', 'LogisticRegression': 'red'}\n",
        "\n",
        "    for model_name, results in all_results.items():\n",
        "        # Collect all unique C values across seeds\n",
        "        all_c_values = set()\n",
        "        for seed_results in results.values():\n",
        "            df = seed_results['all_cv_results']\n",
        "            all_c_values.update(df['param_C'])\n",
        "\n",
        "        c_values = sorted(list(all_c_values))\n",
        "        n_c_values = len(c_values)\n",
        "\n",
        "        # Initialize arrays for scores\n",
        "        train_scores = np.zeros((len(results), n_c_values))\n",
        "        val_scores = np.zeros((len(results), n_c_values))\n",
        "\n",
        "        # Fill score arrays\n",
        "        for i, (seed, seed_results) in enumerate(results.items()):\n",
        "            df = seed_results['all_cv_results']\n",
        "            for j, c in enumerate(c_values):\n",
        "                mask = df['param_C'] == c\n",
        "                if mask.any():\n",
        "                    train_scores[i, j] = df.loc[mask, 'mean_train_score'].iloc[0]\n",
        "                    val_scores[i, j] = df.loc[mask, 'mean_test_score'].iloc[0]\n",
        "\n",
        "        # Calculate mean and std\n",
        "        train_mean = np.mean(train_scores, axis=0)\n",
        "        train_std = np.std(train_scores, axis=0)\n",
        "        val_mean = np.mean(val_scores, axis=0)\n",
        "        val_std = np.std(val_scores, axis=0)\n",
        "\n",
        "        # Plot training scores\n",
        "        plt.semilogx(c_values, train_mean, '--', color=colors[model_name],\n",
        "                     alpha=0.8, label=f'{model_name} Training')\n",
        "        plt.fill_between(c_values,\n",
        "                        train_mean - train_std,\n",
        "                        train_mean + train_std,\n",
        "                        color=colors[model_name], alpha=0.1)\n",
        "\n",
        "        # Plot validation scores\n",
        "        plt.semilogx(c_values, val_mean, '-', color=colors[model_name],\n",
        "                     alpha=0.8, label=f'{model_name} Validation')\n",
        "        plt.fill_between(c_values,\n",
        "                        val_mean - val_std,\n",
        "                        val_mean + val_std,\n",
        "                        color=colors[model_name], alpha=0.2)\n",
        "\n",
        "    plt.grid(True)\n",
        "    plt.xlabel('C Parameter (log scale)')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Validation Curves: Averaged Across Seeds\\nSolid: Validation, Dashed: Training')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.savefig(os.path.join(output_dir, 'averaged_validation_curves.png'))\n",
        "    plt.close()\n",
        "\n",
        "def generate_seed_summary(all_results: Dict, output_dir: str, class_names: List[str]):\n",
        "    \"\"\"Generate summary statistics and plots\"\"\"\n",
        "    for model_name, results in all_results.items():\n",
        "        # Collect metrics\n",
        "        cv_scores = []\n",
        "        test_scores = []\n",
        "        c_values = []\n",
        "\n",
        "        for seed_results in results.values():\n",
        "            cv_scores.append(seed_results['cv_score'])\n",
        "            test_scores.append(seed_results['test_metrics']['weighted_f1'])\n",
        "            c_values.append(seed_results['best_params']['C'])\n",
        "\n",
        "        # Calculate summary statistics\n",
        "        summary = {\n",
        "            'cv_score': {\n",
        "                'mean': float(np.mean(cv_scores)),\n",
        "                'std': float(np.std(cv_scores)),\n",
        "                'min': float(np.min(cv_scores)),\n",
        "                'max': float(np.max(cv_scores))\n",
        "            },\n",
        "            'test_score': {\n",
        "                'mean': float(np.mean(test_scores)),\n",
        "                'std': float(np.std(test_scores)),\n",
        "                'min': float(np.min(test_scores)),\n",
        "                'max': float(np.max(test_scores))\n",
        "            },\n",
        "            'c_value': {\n",
        "                'mean': float(np.mean(c_values)),\n",
        "                'std': float(np.std(c_values)),\n",
        "                'min': float(np.min(c_values)),\n",
        "                'max': float(np.max(c_values))\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Save summary\n",
        "        with open(os.path.join(output_dir, f'{model_name}_seed_summary.json'), 'w') as f:\n",
        "            json.dump(summary, f, indent=4)\n",
        "\n",
        "        # Generate comparison plot\n",
        "        plt.figure(figsize=(15, 6))\n",
        "\n",
        "        plt.subplot(121)\n",
        "        plt.scatter(cv_scores, test_scores, alpha=0.6)\n",
        "        plt.plot([min(cv_scores), max(cv_scores)], [min(cv_scores), max(cv_scores)],\n",
        "                 'k--', alpha=0.5)\n",
        "        plt.xlabel('CV Score')\n",
        "        plt.ylabel('Test Score')\n",
        "        plt.title('CV vs Test Score Comparison')\n",
        "\n",
        "        plt.subplot(122)\n",
        "        plt.hist(np.log10(c_values), bins=10)\n",
        "        plt.xlabel('log10(C)')\n",
        "        plt.ylabel('Count')\n",
        "        plt.title('Distribution of Best C Values')\n",
        "\n",
        "        plt.suptitle(f'{model_name}: Results Across Seeds')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, f'{model_name}_seed_comparison.png'))\n",
        "        plt.close()\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    data_dir = \"/content/local_features\"\n",
        "    class_names = ['Bleikja', 'Lax', 'Urriði']\n",
        "\n",
        "    try:\n",
        "        results = run_multi_seed_optimization(\n",
        "            data_dir=data_dir,\n",
        "            class_names=class_names,\n",
        "            base_seed=42,\n",
        "            n_seeds=10\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during optimization: {str(e)}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "yaQoVIE7flG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "def zip_folder(folder_path, output_zip_path):\n",
        "    \"\"\"\n",
        "    Create a zip file from a folder in Google Colab.\n",
        "\n",
        "    Args:\n",
        "        folder_path (str): Path to the folder you want to zip\n",
        "        output_zip_path (str): Path where you want to save the zip file\n",
        "    \"\"\"\n",
        "    # Make sure the folder exists\n",
        "    if not os.path.exists(folder_path):\n",
        "        raise ValueError(f\"Folder {folder_path} does not exist\")\n",
        "\n",
        "    # Create the zip file\n",
        "    shutil.make_archive(\n",
        "        base_name=output_zip_path.replace('.zip', ''),\n",
        "        format='zip',\n",
        "        root_dir=os.path.dirname(folder_path),\n",
        "        base_dir=os.path.basename(folder_path)\n",
        "    )"
      ],
      "metadata": {
        "id": "qQ_Fv7WPBYkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "folder_to_zip = '/content/model_optimization_20250202_115215_multiseed'  # Path to your folder\n",
        "output_zip = '/content/model_optimization_20250202_115215_multiseed.zip'  # Where to save the zip file\n",
        "\n",
        "zip_folder(folder_to_zip, output_zip)"
      ],
      "metadata": {
        "id": "UspbDoSoBax9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "toc_visible": true,
      "mount_file_id": "12q5hfoJJMYLJJLvSIQnsOOLh3W7LVYdX",
      "authorship_tag": "ABX9TyObIlcpUd4n4jJlko4u97GL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}