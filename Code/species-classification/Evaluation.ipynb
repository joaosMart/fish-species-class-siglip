{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1qONgXL2NeS_RiGix1GEzuet-42pH0Jk5",
      "authorship_tag": "ABX9TyMGULJvZ3CP2p80K077tVW+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joaosMart/fish-species-class-siglip/blob/update-readme-comprehensive/Code/species-classification/Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation\n",
        "\n",
        "This notebook was used to perfrom the evaluation analysis of all the strategies that used extracted features. This includes the evaluation of tempral pooling, temporal voting, single frame and ResNET-50 as feature extractor.\n",
        "\n",
        "This code saves the required data for the learning curves and plot all the representations for the evaluation."
      ],
      "metadata": {
        "id": "vCxPQ6hgKH3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation - Temporal Pooling"
      ],
      "metadata": {
        "id": "eRN8FFDX0upb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import learning_curve, train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import json\n",
        "import os\n",
        "import logging\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import glob\n",
        "import pandas as pd\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "class NumpyEncoder(json.JSONEncoder):\n",
        "    \"\"\"Custom encoder for numpy data types\"\"\"\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        elif isinstance(obj, np.bool_):\n",
        "            return bool(obj)\n",
        "        return super(NumpyEncoder, self).default(obj)\n",
        "\n",
        "class DataLoader:\n",
        "    \"\"\"Handle loading and processing of NPZ files\"\"\"\n",
        "    def __init__(self, data_dir: str):\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "    def load_npz_files(self) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Load all NPZ files from directory and extract averaged_mean features and labels\n",
        "\n",
        "        Returns:\n",
        "            features: numpy array of averaged_mean features\n",
        "            labels: numpy array of fish species labels\n",
        "        \"\"\"\n",
        "        features_list = []\n",
        "        labels_list = []\n",
        "\n",
        "        # Get all NPZ files in directory\n",
        "        npz_files = glob.glob(os.path.join(self.data_dir, \"*.npz\"))\n",
        "\n",
        "        logging.info(f\"Found {len(npz_files)} NPZ files\")\n",
        "\n",
        "        for npz_file in npz_files:\n",
        "            try:\n",
        "                # Load NPZ file\n",
        "                data = np.load(npz_file, allow_pickle=True)\n",
        "\n",
        "                # Extract averaged_mean feature and label\n",
        "                features = data['averaged_features']  # Convert from np.ndarray to dict\n",
        "                fish_species = str(data['fish_species'].item())  # Convert to string\n",
        "\n",
        "                if features is not None:\n",
        "                    features_list.append(features)\n",
        "                    labels_list.append(fish_species)\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error processing file {npz_file}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Convert lists to numpy arrays\n",
        "        features_array = np.array(features_list)\n",
        "        labels_array = np.array(labels_list)\n",
        "\n",
        "        # Log data distribution\n",
        "        unique_labels, counts = np.unique(labels_array, return_counts=True)\n",
        "        for label, count in zip(unique_labels, counts):\n",
        "            percentage = (count / len(labels_array)) * 100\n",
        "            logging.info(f\"Class {label}: {count} samples ({percentage:.2f}%)\")\n",
        "\n",
        "        return features_array, labels_array\n",
        "\n",
        "\n",
        "class FishClassifier:\n",
        "    def __init__(self, random_state: int = 42):\n",
        "        self.random_state = random_state\n",
        "        self.le = LabelEncoder()\n",
        "        self.setup_logging()\n",
        "\n",
        "    def setup_logging(self):\n",
        "        \"\"\"Setup logging configuration\"\"\"\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            filename='fish_classifier.log'\n",
        "        )\n",
        "\n",
        "    def prepare_data(self, features: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Prepare data by splitting into train and test sets with stratification\n",
        "        \"\"\"\n",
        "        # Encode labels\n",
        "        y = self.le.fit_transform(labels)\n",
        "\n",
        "        # Create stratified train-test split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            features,\n",
        "            y,\n",
        "            test_size=0.2,\n",
        "            random_state=self.random_state,\n",
        "            stratify=y\n",
        "        )\n",
        "\n",
        "        logging.info(f\"Training set size: {X_train.shape[0]}\")\n",
        "        logging.info(f\"Test set size: {X_test.shape[0]}\")\n",
        "\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "    def create_baseline_models(self) -> Dict:\n",
        "        \"\"\"Create baseline models with default parameters\"\"\"\n",
        "        models = {\n",
        "            'svm': LinearSVC(\n",
        "                random_state=self.random_state,\n",
        "                class_weight='balanced',\n",
        "                max_iter=2000  # Increased to ensure convergence\n",
        "            ),\n",
        "            'logistic': LogisticRegression(\n",
        "                random_state=self.random_state,\n",
        "                class_weight='balanced',\n",
        "                max_iter=2000\n",
        "            )\n",
        "        }\n",
        "        return models\n",
        "\n",
        "    def evaluate_model(self, model, X: np.ndarray, y: np.ndarray, model_name: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Evaluate model performance with multiple metrics\n",
        "        \"\"\"\n",
        "        # Get predictions\n",
        "        y_pred = model.predict(X)\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = {\n",
        "            'accuracy': accuracy_score(y, y_pred),\n",
        "            'macro_f1': f1_score(y, y_pred, average='macro'),\n",
        "            'confusion_matrix': confusion_matrix(y, y_pred),\n",
        "        }\n",
        "\n",
        "        # Calculate per-class metrics\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y, y_pred)\n",
        "\n",
        "        # Add per-class metrics\n",
        "        for i, class_name in enumerate(self.le.classes_):\n",
        "            metrics[f'{class_name}_precision'] = precision[i]\n",
        "            metrics[f'{class_name}_recall'] = recall[i]\n",
        "            metrics[f'{class_name}_f1'] = f1[i]\n",
        "\n",
        "        # Log results\n",
        "        logging.info(f\"\\nResults for {model_name}:\")\n",
        "        logging.info(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
        "        logging.info(f\"Macro F1: {metrics['macro_f1']:.4f}\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def plot_confusion_matrix(self, confusion_mat: np.ndarray, model_name: str):\n",
        "        \"\"\"\n",
        "        Plot confusion matrix heatmap\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(\n",
        "            confusion_mat,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='Blues',\n",
        "            xticklabels=self.le.classes_,\n",
        "            yticklabels=self.le.classes_\n",
        "        )\n",
        "        plt.title(f'Confusion Matrix - {model_name}')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.savefig(f'confusion_matrix_{model_name}.png')\n",
        "        plt.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "_AL2kTqi2RW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features"
      ],
      "metadata": {
        "id": "6Ne8YcNA8JcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/model_optimization_20241212_124830_multiseed.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Q1Jbf1AiALF6",
        "outputId": "aee09fb2-3bd2-40d8-8061-78fa12785475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/model_optimization_20241212_124830_multiseed.zip\n",
            "   creating: model_optimization_20241212_124830_multiseed/\n",
            "   creating: model_optimization_20241212_124830_multiseed/LogisticRegression/\n",
            "   creating: model_optimization_20241212_124830_multiseed/SVM/\n",
            "   creating: model_optimization_20241212_124830_multiseed/seed_1/\n",
            "   creating: model_optimization_20241212_124830_multiseed/seed_11/\n",
            "   creating: model_optimization_20241212_124830_multiseed/seed_23/\n",
            "   creating: model_optimization_20241212_124830_multiseed/seed_40/\n",
            "   creating: model_optimization_20241212_124830_multiseed/seed_45/\n",
            "   creating: model_optimization_20241212_124830_multiseed/seed_46/\n",
            "   creating: model_optimization_20241212_124830_multiseed/seed_54/\n",
            "   creating: model_optimization_20241212_124830_multiseed/seed_71/\n",
            "   creating: model_optimization_20241212_124830_multiseed/seed_81/\n",
            "   creating: model_optimization_20241212_124830_multiseed/seed_84/\n",
            "  inflating: model_optimization_20241212_124830_multiseed/LogisticRegression_seed_summary.json  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/SVM_seed_comparison.png  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/random_seeds.json  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/SVM_seed_summary.json  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/LogisticRegression_seed_comparison.png  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_23/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_23/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_23/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_23/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_1/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_1/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_1/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_1/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_71/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_71/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_71/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_71/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/SVM/performance_summary.txt  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/SVM/best_params.json  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/SVM/random_search_results.csv  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_45/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_45/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_45/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_45/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/LogisticRegression/performance_summary.txt  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/LogisticRegression/best_params.json  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/LogisticRegression/random_search_results.csv  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_40/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_40/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_40/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_40/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_11/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_11/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_11/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_11/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_84/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_84/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_84/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_84/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_81/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_81/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_81/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_81/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_46/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_46/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_46/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_46/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_54/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_54/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_54/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_124830_multiseed/seed_54/SVM_metrics.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More Robust Learning Curve"
      ],
      "metadata": {
        "id": "rl7gT5tZ1_xh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import json\n",
        "import os\n",
        "import logging\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import glob\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "class ModelEvaluator:\n",
        "    \"\"\"Handles multi-seed evaluation of models using grid search results\"\"\"\n",
        "\n",
        "    def __init__(self, random_state: int = 42):\n",
        "        self.random_state = random_state\n",
        "        self.le = LabelEncoder()\n",
        "        self.setup_output_dir()\n",
        "        self.setup_logging()\n",
        "\n",
        "    def setup_output_dir(self):\n",
        "        \"\"\"Create output directory with timestamp\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        self.output_dir = f'model_evaluation_{timestamp}'\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "    def setup_logging(self):\n",
        "        \"\"\"Configure logging\"\"\"\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            handlers=[\n",
        "                logging.FileHandler(os.path.join(self.output_dir, 'evaluation.log')),\n",
        "                logging.StreamHandler()\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def load_grid_search_results(self, grid_search_dir: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Load results from multi-seed grid search\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "\n",
        "        # Load random seeds\n",
        "        with open(os.path.join(grid_search_dir, 'random_seeds.json'), 'r') as f:\n",
        "            seeds_info = json.load(f)\n",
        "            seeds = seeds_info['generated_seeds']\n",
        "\n",
        "        # Process each seed directory\n",
        "        for seed in seeds:\n",
        "            seed_dir = os.path.join(grid_search_dir, f'seed_{seed}')\n",
        "            results[seed] = {}\n",
        "\n",
        "            # Load results for each model type\n",
        "            for model_name in ['SVM', 'LogisticRegression']:\n",
        "                metrics_file = os.path.join(seed_dir, f'{model_name}_metrics.json')\n",
        "                with open(metrics_file, 'r') as f:\n",
        "                    results[seed][model_name] = json.load(f)\n",
        "\n",
        "        return results, seeds\n",
        "\n",
        "    def prepare_data(self, features: np.ndarray, labels: np.ndarray, seed: int) -> Tuple:\n",
        "        \"\"\"Prepare train-test split using specific seed\"\"\"\n",
        "        y = self.le.fit_transform(labels)\n",
        "        return train_test_split(features, y, test_size=0.2, random_state=seed, stratify=y)\n",
        "\n",
        "    def create_model(self, model_type: str, params: Dict, seed: int) -> Any:\n",
        "        \"\"\"Create model with specified parameters\"\"\"\n",
        "        if model_type == 'SVM':\n",
        "            return LinearSVC(random_state=seed, max_iter=2000, **params)\n",
        "        elif model_type == 'LogisticRegression':\n",
        "            return LogisticRegression(random_state=seed, max_iter=2000, **params)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "    def evaluate_model(self, model, X: np.ndarray, y: np.ndarray) -> Dict:\n",
        "        \"\"\"Comprehensive model evaluation\"\"\"\n",
        "        y_pred = model.predict(X)\n",
        "\n",
        "        metrics = {\n",
        "            'weighted_f1': f1_score(y, y_pred, average='weighted'),\n",
        "            'macro_f1': f1_score(y, y_pred, average='macro'),\n",
        "            'accuracy': accuracy_score(y, y_pred),\n",
        "            'confusion_matrix': confusion_matrix(y, y_pred)\n",
        "        }\n",
        "\n",
        "        # Calculate per-class metrics\n",
        "        precision, recall, f1, support = precision_recall_fscore_support(y, y_pred)\n",
        "\n",
        "        for i, class_name in enumerate(self.le.classes_):\n",
        "            metrics[f'{class_name}_precision'] = precision[i]\n",
        "            metrics[f'{class_name}_recall'] = recall[i]\n",
        "            metrics[f'{class_name}_f1'] = f1[i]\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def plot_confusion_matrices(self, confusion_matrices: List[np.ndarray],\n",
        "                              model_name: str, seed: int):\n",
        "        \"\"\"Plot confusion matrices for a specific seed\"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "        # Plot training confusion matrix\n",
        "        sns.heatmap(confusion_matrices[0], annot=True, fmt='d', ax=ax1, cmap='Blues',\n",
        "                   xticklabels=self.le.classes_, yticklabels=self.le.classes_)\n",
        "        ax1.set_title(f'Training Confusion Matrix\\nSeed: {seed}')\n",
        "        ax1.set_xlabel('Predicted Label')\n",
        "        ax1.set_ylabel('True Label')\n",
        "\n",
        "        # Plot test confusion matrix\n",
        "        sns.heatmap(confusion_matrices[1], annot=True, fmt='d', ax=ax2, cmap='Blues',\n",
        "                   xticklabels=self.le.classes_, yticklabels=self.le.classes_)\n",
        "        ax2.set_title(f'Test Confusion Matrix\\nSeed: {seed}')\n",
        "        ax2.set_xlabel('Predicted Label')\n",
        "        ax2.set_ylabel('True Label')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, f'{model_name}_confusion_matrices_seed_{seed}.png'))\n",
        "        plt.close()\n",
        "\n",
        "    def plot_aggregated_results(self, all_results: Dict, model_name: str):\n",
        "        \"\"\"Plot aggregated results across seeds\"\"\"\n",
        "        test_scores = {\n",
        "            'weighted_f1': [],\n",
        "            'macro_f1': [],\n",
        "            'accuracy': []\n",
        "        }\n",
        "\n",
        "        for seed_results in all_results.values():\n",
        "            metrics = seed_results['test_metrics']\n",
        "            for metric in test_scores.keys():\n",
        "                test_scores[metric].append(metrics[metric])\n",
        "\n",
        "        # Create box plots\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        data = [scores for scores in test_scores.values()]\n",
        "        plt.boxplot(data, labels=list(test_scores.keys()))\n",
        "        plt.title(f'{model_name}: Performance Distribution Across Seeds')\n",
        "        plt.ylabel('Score')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, f'{model_name}_performance_distribution.png'))\n",
        "        plt.close()\n",
        "\n",
        "    def calculate_learning_curve(self, features: np.ndarray, labels: np.ndarray,\n",
        "                           grid_results: Dict, seeds: List[int],\n",
        "                           train_sizes: np.ndarray = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Calculate learning curves across multiple seeds using incremental training data\n",
        "        and evaluating on the holdout test set.\n",
        "\n",
        "        Args:\n",
        "            features: Input features\n",
        "            labels: Target labels\n",
        "            grid_results: Grid search results containing best parameters for each seed\n",
        "            seeds: List of random seeds\n",
        "            train_sizes: Array of training set sizes to evaluate (proportions from 0 to 1)\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing learning curve data for both models\n",
        "        \"\"\"\n",
        "        if train_sizes is None:\n",
        "            train_sizes = np.linspace(0.05, 1.0, 15)\n",
        "\n",
        "        # Initialize storage for learning curve data\n",
        "        curve_data = {\n",
        "            'SVM': {size: [] for size in train_sizes},\n",
        "            'LogisticRegression': {size: [] for size in train_sizes}\n",
        "        }\n",
        "\n",
        "        # For each seed\n",
        "        for seed in seeds:\n",
        "            # Split data into train and test sets\n",
        "            X_train, X_test, y_train, y_test = self.prepare_data(features, labels, seed)\n",
        "            n_samples = len(y_train)\n",
        "\n",
        "            # For each model type\n",
        "            for model_name in ['SVM', 'LogisticRegression']:\n",
        "                # Get best parameters for this seed\n",
        "                params = grid_results[seed][model_name]['best_params']\n",
        "\n",
        "                # For each training set size\n",
        "                for train_size in train_sizes:\n",
        "                    # Calculate number of samples for this training size\n",
        "                    n_train = int(n_samples * train_size)\n",
        "\n",
        "                    # Create and train model on subset\n",
        "                    model = self.create_model(model_name, params, seed)\n",
        "                    model.fit(X_train[:n_train], y_train[:n_train])\n",
        "\n",
        "                    # Evaluate on test set\n",
        "                    metrics = self.evaluate_model(model, X_test, y_test)\n",
        "                    curve_data[model_name][train_size].append(metrics['macro_f1'])\n",
        "\n",
        "        # Calculate mean and std for each size\n",
        "        learning_curves = {\n",
        "            model_name: {\n",
        "                'train_sizes': train_sizes * len(y_train),\n",
        "                'test_scores_mean': [np.mean(curve_data[model_name][size])\n",
        "                                  for size in train_sizes],\n",
        "                'test_scores_std': [np.std(curve_data[model_name][size])\n",
        "                                  for size in train_sizes]\n",
        "            }\n",
        "            for model_name in ['SVM', 'LogisticRegression']\n",
        "        }\n",
        "\n",
        "        return learning_curves\n",
        "\n",
        "    def plot_averaged_confusion_matrices(self, all_results: Dict[str, Dict]):\n",
        "        \"\"\"\n",
        "        Plot averaged confusion matrices across all seeds for both models.\n",
        "\n",
        "        Args:\n",
        "            all_results: Dictionary containing results for all models and seeds\n",
        "        \"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "        for idx, model_name in enumerate(['SVM', 'LogisticRegression']):\n",
        "            # Get all test confusion matrices for this model\n",
        "            confusion_matrices = []\n",
        "            for seed_results in all_results[model_name].values():\n",
        "                cm = seed_results['test_metrics']['confusion_matrix']\n",
        "                confusion_matrices.append(cm)\n",
        "\n",
        "            # Calculate average confusion matrix\n",
        "            avg_cm = np.mean(confusion_matrices, axis=0)\n",
        "\n",
        "            # Calculate standard deviation for annotations\n",
        "            std_cm = np.std(confusion_matrices, axis=0)\n",
        "\n",
        "            # Create annotations with mean ± std\n",
        "            annotations = np.array([\n",
        "                [f'{avg:.1f} ± {std:.1f}'\n",
        "                for avg, std in zip(row_avg, row_std)]\n",
        "                for row_avg, row_std in zip(avg_cm, std_cm)\n",
        "            ])\n",
        "\n",
        "            # Plot heatmap\n",
        "            ax = ax1 if idx == 0 else ax2\n",
        "            sns.heatmap(\n",
        "                avg_cm,\n",
        "                annot=annotations,\n",
        "                fmt='',\n",
        "                cmap='Blues',\n",
        "                xticklabels=self.le.classes_,\n",
        "                yticklabels=self.le.classes_,\n",
        "                ax=ax\n",
        "            )\n",
        "\n",
        "            ax.set_xlabel('Predicted Label')\n",
        "            ax.set_ylabel('True Label')\n",
        "\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, 'averaged_confusion_matrices.png'),\n",
        "                    dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "    def plot_learning_curves(self, learning_curves: Dict):\n",
        "        \"\"\"\n",
        "        Plot learning curves with total samples info in top left box\n",
        "        and seeds info in top right.\n",
        "\n",
        "        Args:\n",
        "            learning_curves: Dictionary containing learning curve data for both models\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        # Get the current axes\n",
        "        ax = plt.gca()\n",
        "\n",
        "        # Remove the top and right spines\n",
        "        ax.spines['top'].set_visible(False)\n",
        "        ax.spines['right'].set_visible(False)\n",
        "        ax.spines['left'].set_visible(False)\n",
        "        ax.spines['bottom'].set_visible(False)\n",
        "\n",
        "\n",
        "        colors = {\n",
        "            'SVM': 'blue',\n",
        "            'LogisticRegression': 'red'\n",
        "        }\n",
        "\n",
        "        for model_name in ['SVM', 'LogisticRegression']:\n",
        "            data = learning_curves[model_name]\n",
        "\n",
        "            # Plot mean test scores\n",
        "            plt.plot(data['train_sizes'], data['test_scores_mean'],\n",
        "                    f'-', color=colors[model_name], label=f'{model_name}',\n",
        "                    linewidth=2)\n",
        "\n",
        "            # Plot standard deviation bands\n",
        "            plt.fill_between(data['train_sizes'],\n",
        "                            np.array(data['test_scores_mean']) - np.array(data['test_scores_std']),\n",
        "                            np.array(data['test_scores_mean']) + np.array(data['test_scores_std']),\n",
        "                            alpha=0.1, color=colors[model_name])\n",
        "\n",
        "        plt.xlabel('Number of Training Samples')\n",
        "        plt.ylabel('Macro F1 Score')\n",
        "\n",
        "        plt.legend(loc='lower right', frameon=True)\n",
        "\n",
        "        # Make grid lighter\n",
        "        plt.grid(True)\n",
        "\n",
        "\n",
        "        # Get total number of samples (maximum training size)\n",
        "        total_samples = max(learning_curves['SVM']['train_sizes'])\n",
        "\n",
        "        # Add text box with total samples at top left\n",
        "        plt.text(0.02, 0.98, f'Total samples: {total_samples:.0f}',\n",
        "                transform=plt.gca().transAxes,\n",
        "                bbox=dict(facecolor='white',\n",
        "                          alpha=0.8,\n",
        "                          boxstyle='round,pad=0.5'),\n",
        "                verticalalignment='top',\n",
        "                horizontalalignment='left',\n",
        "                fontsize=10)\n",
        "\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, 'learning_curves_mean_ensemble.png'),\n",
        "                    dpi=300,\n",
        "                    bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def save_learning_curves_data(self, learning_curves: Dict, output_dir: str):\n",
        "        \"\"\"\n",
        "        Save learning curves data to a JSON file for later plotting\n",
        "\n",
        "        Args:\n",
        "            learning_curves: Dictionary containing learning curve data for both models\n",
        "            output_dir: Directory to save the output file\n",
        "        \"\"\"\n",
        "        curves_data = {}\n",
        "\n",
        "        for model_name in learning_curves:\n",
        "            data = learning_curves[model_name]\n",
        "            curves_data[model_name] = {\n",
        "                'train_sizes': data['train_sizes'].tolist(),  # Convert numpy array to list\n",
        "                'test_scores_mean': data['test_scores_mean'],\n",
        "                'test_scores_std': data['test_scores_std']\n",
        "            }\n",
        "\n",
        "        output_file = os.path.join(output_dir, 'learning_curves_data.json')\n",
        "        with open(output_file, 'w') as f:\n",
        "            json.dump(curves_data, f, indent=2)\n",
        "\n",
        "\n",
        "def run_evaluation(data_dir: str, grid_search_dir: str) -> Dict:\n",
        "    \"\"\"Run complete evaluation pipeline\"\"\"\n",
        "    try:\n",
        "        # Initialize evaluator\n",
        "        evaluator = ModelEvaluator()\n",
        "\n",
        "        # Load data\n",
        "        data_loader = DataLoader(data_dir)\n",
        "        features, labels = data_loader.load_npz_files()\n",
        "\n",
        "        # Load grid search results\n",
        "        grid_results, seeds = evaluator.load_grid_search_results(grid_search_dir)\n",
        "\n",
        "        all_results = {}\n",
        "        for model_name in ['SVM', 'LogisticRegression']:\n",
        "            all_results[model_name] = {}\n",
        "\n",
        "            for seed in seeds:\n",
        "                # Prepare data using seed\n",
        "                X_train, X_test, y_train, y_test = evaluator.prepare_data(features, labels, seed)\n",
        "\n",
        "                # Get best parameters for this seed\n",
        "                params = grid_results[seed][model_name]['best_params']\n",
        "\n",
        "                # Create and train model\n",
        "                model = evaluator.create_model(model_name, params, seed)\n",
        "                model.fit(X_train, y_train)\n",
        "\n",
        "                # Evaluate model\n",
        "                train_metrics = evaluator.evaluate_model(model, X_train, y_train)\n",
        "                test_metrics = evaluator.evaluate_model(model, X_test, y_test)\n",
        "\n",
        "                # Store results\n",
        "                all_results[model_name][seed] = {\n",
        "                    'train_metrics': train_metrics,\n",
        "                    'test_metrics': test_metrics,\n",
        "                    'params': params\n",
        "                }\n",
        "\n",
        "        # Plot averaged confusion matrices\n",
        "        evaluator.plot_averaged_confusion_matrices(all_results)\n",
        "\n",
        "        # Calculate and saves learning curves\n",
        "        learning_curves = evaluator.calculate_learning_curve(\n",
        "            features, labels, grid_results, seeds\n",
        "        )\n",
        "        evaluator.save_learning_curves_data(learning_curves, evaluator.output_dir)\n",
        "\n",
        "        # Plot learning curves\n",
        "        evaluator.plot_learning_curves(learning_curves)\n",
        "\n",
        "        # Save all results\n",
        "        for model_name in all_results:\n",
        "            with open(os.path.join(evaluator.output_dir, f'{model_name}_evaluation.json'), 'w') as f:\n",
        "                json.dump(all_results[model_name], f, cls=NumpyEncoder)\n",
        "\n",
        "        return {\n",
        "            'model_results': all_results,\n",
        "            'learning_curves': learning_curves\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in evaluation pipeline: {str(e)}\")\n",
        "        raise\n",
        "\n"
      ],
      "metadata": {
        "id": "eHwYeAIl1-U8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/model_optimization_20241212_191723_multiseed.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hgTOgqPXhaoz",
        "outputId": "cf882528-2d7d-4632-e933-9794814dbbde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/model_optimization_20241212_191723_multiseed.zip\n",
            "   creating: model_optimization_20241212_191723_multiseed/\n",
            "   creating: model_optimization_20241212_191723_multiseed/LogisticRegression/\n",
            "   creating: model_optimization_20241212_191723_multiseed/SVM/\n",
            "   creating: model_optimization_20241212_191723_multiseed/seed_15/\n",
            "   creating: model_optimization_20241212_191723_multiseed/seed_29/\n",
            "   creating: model_optimization_20241212_191723_multiseed/seed_30/\n",
            "   creating: model_optimization_20241212_191723_multiseed/seed_32/\n",
            "   creating: model_optimization_20241212_191723_multiseed/seed_37/\n",
            "   creating: model_optimization_20241212_191723_multiseed/seed_38/\n",
            "   creating: model_optimization_20241212_191723_multiseed/seed_4/\n",
            "   creating: model_optimization_20241212_191723_multiseed/seed_65/\n",
            "   creating: model_optimization_20241212_191723_multiseed/seed_88/\n",
            "   creating: model_optimization_20241212_191723_multiseed/seed_91/\n",
            "  inflating: model_optimization_20241212_191723_multiseed/SVM_seed_summary.json  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/LogisticRegression_seed_summary.json  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/SVM_seed_comparison.png  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/random_seeds.json  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/LogisticRegression_seed_comparison.png  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/LogisticRegression/random_search_results.csv  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/LogisticRegression/performance_summary.txt  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/LogisticRegression/best_params.json  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_65/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_65/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_65/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_65/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_30/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_30/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_30/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_30/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_32/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_32/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_32/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_32/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_4/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_4/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_4/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_4/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_15/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_15/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_15/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_15/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/SVM/random_search_results.csv  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/SVM/performance_summary.txt  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/SVM/best_params.json  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_88/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_88/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_88/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_88/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_38/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_38/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_38/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_38/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_91/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_91/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_91/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_91/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_37/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_37/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_37/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_37/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_29/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_29/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_29/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_191723_multiseed/seed_29/LogisticRegression_results.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Directory containing your NPZ files\n",
        "    data_dir = \"/path/to/Feature Extraction/ViT-SO400M-14-SigLIP\"\n",
        "\n",
        "    # Directory containing grid search results\n",
        "    grid_search_dir = \"/path/to/Temporal Pooling/model_optimization_20250206_130548_multiseed\"\n",
        "\n",
        "    # Run evaluation\n",
        "    results = run_evaluation(data_dir, grid_search_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqSgObRnhZ6G",
        "outputId": "baf857d9-e077-4bc7-d874-a640d64385cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "def zip_folder(folder_path, output_zip_path):\n",
        "    \"\"\"\n",
        "    Create a zip file from a folder in Google Colab.\n",
        "\n",
        "    Args:\n",
        "        folder_path (str): Path to the folder you want to zip\n",
        "        output_zip_path (str): Path where you want to save the zip file\n",
        "    \"\"\"\n",
        "    # Make sure the folder exists\n",
        "    if not os.path.exists(folder_path):\n",
        "        raise ValueError(f\"Folder {folder_path} does not exist\")\n",
        "\n",
        "    # Create the zip file\n",
        "    shutil.make_archive(\n",
        "        base_name=output_zip_path.replace('.zip', ''),\n",
        "        format='zip',\n",
        "        root_dir=os.path.dirname(folder_path),\n",
        "        base_dir=os.path.basename(folder_path)\n",
        "    )"
      ],
      "metadata": {
        "id": "wEWIDWBBMkb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "folder_to_zip = '/content/model_evaluation_20241212_194724'  # Path to your folder\n",
        "output_zip = '/content/model_evaluation_20241212_194724.zip'  # Where to save the zip file\n",
        "\n",
        "zip_folder(folder_to_zip, output_zip)"
      ],
      "metadata": {
        "id": "em4oSENGMoys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation - Single Frame"
      ],
      "metadata": {
        "id": "mq3pPBiG7V0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import learning_curve, train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import json\n",
        "import os\n",
        "import logging\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import glob\n",
        "import pandas as pd\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "class NumpyEncoder(json.JSONEncoder):\n",
        "    \"\"\"Custom encoder for numpy data types\"\"\"\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        elif isinstance(obj, np.bool_):\n",
        "            return bool(obj)\n",
        "        return super(NumpyEncoder, self).default(obj)\n",
        "\n",
        "class DataLoader:\n",
        "    \"\"\"Handle loading and processing of NPZ files\"\"\"\n",
        "    def __init__(self, data_dir: str):\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "    def load_npz_files(self) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Load all NPZ files from directory and extract averaged_mean features and labels\n",
        "\n",
        "        Returns:\n",
        "            features: numpy array of averaged_mean features\n",
        "            labels: numpy array of fish species labels\n",
        "        \"\"\"\n",
        "        features_list = []\n",
        "        labels_list = []\n",
        "\n",
        "        # Get all NPZ files in directory\n",
        "        npz_files = glob.glob(os.path.join(self.data_dir, \"*.npz\"))\n",
        "\n",
        "        logging.info(f\"Found {len(npz_files)} NPZ files\")\n",
        "\n",
        "        for npz_file in npz_files:\n",
        "            try:\n",
        "                # Load NPZ file\n",
        "                data = np.load(npz_file, allow_pickle=True)\n",
        "\n",
        "                # Extract averaged_mean feature and label\n",
        "                middle_frame = data['middle_frame'].item()\n",
        "                frame_features = data['features'].item()[middle_frame]\n",
        "                fish_species = str(data['fish_species'].item())  # Convert to string\n",
        "\n",
        "                if frame_features is not None:\n",
        "                    features_list.append(frame_features)\n",
        "                    labels_list.append(fish_species)\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error processing file {npz_file}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Convert lists to numpy arrays\n",
        "        features_array = np.array(features_list)\n",
        "        labels_array = np.array(labels_list)\n",
        "\n",
        "        # Log data distribution\n",
        "        unique_labels, counts = np.unique(labels_array, return_counts=True)\n",
        "        for label, count in zip(unique_labels, counts):\n",
        "            percentage = (count / len(labels_array)) * 100\n",
        "            logging.info(f\"Class {label}: {count} samples ({percentage:.2f}%)\")\n",
        "\n",
        "        return features_array, labels_array\n",
        "\n",
        "\n",
        "class FishClassifier:\n",
        "    def __init__(self, random_state: int = 42):\n",
        "        self.random_state = random_state\n",
        "        self.le = LabelEncoder()\n",
        "        self.setup_logging()\n",
        "\n",
        "    def setup_logging(self):\n",
        "        \"\"\"Setup logging configuration\"\"\"\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            filename='fish_classifier.log'\n",
        "        )\n",
        "\n",
        "    def prepare_data(self, features: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Prepare data by splitting into train and test sets with stratification\n",
        "        \"\"\"\n",
        "        # Encode labels\n",
        "        y = self.le.fit_transform(labels)\n",
        "\n",
        "        # Create stratified train-test split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            features,\n",
        "            y,\n",
        "            test_size=0.2,\n",
        "            random_state=self.random_state,\n",
        "            stratify=y\n",
        "        )\n",
        "\n",
        "        logging.info(f\"Training set size: {X_train.shape[0]}\")\n",
        "        logging.info(f\"Test set size: {X_test.shape[0]}\")\n",
        "\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "    def create_baseline_models(self) -> Dict:\n",
        "        \"\"\"Create baseline models with default parameters\"\"\"\n",
        "        models = {\n",
        "            'svm': LinearSVC(\n",
        "                random_state=self.random_state,\n",
        "                class_weight='balanced',\n",
        "                max_iter=2000\n",
        "            ),\n",
        "            'logistic': LogisticRegression(\n",
        "                random_state=self.random_state,\n",
        "                class_weight='balanced',\n",
        "                max_iter=2000\n",
        "            )\n",
        "        }\n",
        "        return models\n",
        "\n",
        "    def evaluate_model(self, model, X: np.ndarray, y: np.ndarray, model_name: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Evaluate model performance with multiple metrics\n",
        "        \"\"\"\n",
        "        # Get predictions\n",
        "        y_pred = model.predict(X)\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = {\n",
        "            'accuracy': accuracy_score(y, y_pred),\n",
        "            'macro_f1': f1_score(y, y_pred, average='macro'),\n",
        "            'confusion_matrix': confusion_matrix(y, y_pred),\n",
        "        }\n",
        "\n",
        "        # Calculate per-class metrics\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y, y_pred)\n",
        "\n",
        "        # Add per-class metrics\n",
        "        for i, class_name in enumerate(self.le.classes_):\n",
        "            metrics[f'{class_name}_precision'] = precision[i]\n",
        "            metrics[f'{class_name}_recall'] = recall[i]\n",
        "            metrics[f'{class_name}_f1'] = f1[i]\n",
        "\n",
        "        # Log results\n",
        "        logging.info(f\"\\nResults for {model_name}:\")\n",
        "        logging.info(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
        "        logging.info(f\"Macro F1: {metrics['macro_f1']:.4f}\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def plot_confusion_matrix(self, confusion_mat: np.ndarray, model_name: str):\n",
        "        \"\"\"\n",
        "        Plot confusion matrix heatmap\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(\n",
        "            confusion_mat,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='Blues',\n",
        "            xticklabels=self.le.classes_,\n",
        "            yticklabels=self.le.classes_\n",
        "        )\n",
        "        plt.title(f'Confusion Matrix - {model_name}')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.savefig(f'confusion_matrix_{model_name}.png')\n",
        "        plt.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "ac3eD-WzxoDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Robust Learning Curve"
      ],
      "metadata": {
        "id": "wf7IA1Fry-iw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, balanced_accuracy_score, f1_score\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import json\n",
        "import os\n",
        "import logging\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import glob\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "class ModelEvaluator:\n",
        "    \"\"\"Handles multi-seed evaluation of models using grid search results\"\"\"\n",
        "\n",
        "    def __init__(self, random_state: int = 42):\n",
        "        self.random_state = random_state\n",
        "        self.le = LabelEncoder()\n",
        "        self.setup_output_dir()\n",
        "        self.setup_logging()\n",
        "\n",
        "    def setup_output_dir(self):\n",
        "        \"\"\"Create output directory with timestamp\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        self.output_dir = f'model_evaluation_{timestamp}'\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "    def setup_logging(self):\n",
        "        \"\"\"Configure logging\"\"\"\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            handlers=[\n",
        "                logging.FileHandler(os.path.join(self.output_dir, 'evaluation.log')),\n",
        "                logging.StreamHandler()\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def load_grid_search_results(self, grid_search_dir: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Load results from multi-seed grid search\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "\n",
        "        # Load random seeds\n",
        "        with open(os.path.join(grid_search_dir, 'random_seeds.json'), 'r') as f:\n",
        "            seeds_info = json.load(f)\n",
        "            seeds = seeds_info['generated_seeds']\n",
        "\n",
        "        # Process each seed directory\n",
        "        for seed in seeds:\n",
        "            seed_dir = os.path.join(grid_search_dir, f'seed_{seed}')\n",
        "            results[seed] = {}\n",
        "\n",
        "            # Load results for each model type\n",
        "            for model_name in ['SVM', 'LogisticRegression']:\n",
        "                metrics_file = os.path.join(seed_dir, f'{model_name}_metrics.json')\n",
        "                with open(metrics_file, 'r') as f:\n",
        "                    results[seed][model_name] = json.load(f)\n",
        "\n",
        "        return results, seeds\n",
        "\n",
        "    def prepare_data(self, features: np.ndarray, labels: np.ndarray, seed: int) -> Tuple:\n",
        "        \"\"\"Prepare train-test split using specific seed\"\"\"\n",
        "        y = self.le.fit_transform(labels)\n",
        "        return train_test_split(features, y, test_size=0.2, random_state=seed, stratify=y)\n",
        "\n",
        "    def create_model(self, model_type: str, params: Dict, seed: int) -> Any:\n",
        "        \"\"\"Create model with specified parameters\"\"\"\n",
        "        if model_type == 'SVM':\n",
        "            return LinearSVC(random_state=seed, max_iter=2000, **params)\n",
        "        elif model_type == 'LogisticRegression':\n",
        "            return LogisticRegression(random_state=seed, max_iter=2000, **params)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "    def evaluate_model(self, model, X: np.ndarray, y: np.ndarray) -> Dict:\n",
        "        \"\"\"Comprehensive model evaluation\"\"\"\n",
        "        y_pred = model.predict(X)\n",
        "\n",
        "        metrics = {\n",
        "            'weighted_f1': f1_score(y, y_pred, average='weighted'),\n",
        "            'macro_f1': f1_score(y, y_pred, average='macro'),\n",
        "            'accuracy': accuracy_score(y, y_pred),\n",
        "            'confusion_matrix': confusion_matrix(y, y_pred)\n",
        "        }\n",
        "\n",
        "        # Calculate per-class metrics\n",
        "        precision, recall, f1, support = precision_recall_fscore_support(y, y_pred)\n",
        "\n",
        "        for i, class_name in enumerate(self.le.classes_):\n",
        "            metrics[f'{class_name}_precision'] = precision[i]\n",
        "            metrics[f'{class_name}_recall'] = recall[i]\n",
        "            metrics[f'{class_name}_f1'] = f1[i]\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def plot_confusion_matrices(self, confusion_matrices: List[np.ndarray],\n",
        "                              model_name: str, seed: int):\n",
        "        \"\"\"Plot confusion matrices for a specific seed\"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "        # Plot training confusion matrix\n",
        "        sns.heatmap(confusion_matrices[0], annot=True, fmt='d', ax=ax1, cmap='Blues',\n",
        "                   xticklabels=self.le.classes_, yticklabels=self.le.classes_)\n",
        "        ax1.set_title(f'Training Confusion Matrix\\nSeed: {seed}')\n",
        "        ax1.set_xlabel('Predicted Label')\n",
        "        ax1.set_ylabel('True Label')\n",
        "\n",
        "        # Plot test confusion matrix\n",
        "        sns.heatmap(confusion_matrices[1], annot=True, fmt='d', ax=ax2, cmap='Blues',\n",
        "                   xticklabels=self.le.classes_, yticklabels=self.le.classes_)\n",
        "        ax2.set_title(f'Test Confusion Matrix\\nSeed: {seed}')\n",
        "        ax2.set_xlabel('Predicted Label')\n",
        "        ax2.set_ylabel('True Label')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, f'{model_name}_confusion_matrices_seed_{seed}.png'))\n",
        "        plt.close()\n",
        "\n",
        "    def plot_aggregated_results(self, all_results: Dict, model_name: str):\n",
        "        \"\"\"Plot aggregated results across seeds\"\"\"\n",
        "        test_scores = {\n",
        "            'weighted_f1': [],\n",
        "            'macro_f1': [],\n",
        "            'accuracy': []\n",
        "        }\n",
        "\n",
        "        for seed_results in all_results.values():\n",
        "            metrics = seed_results['test_metrics']\n",
        "            for metric in test_scores.keys():\n",
        "                test_scores[metric].append(metrics[metric])\n",
        "\n",
        "        # Create box plots\n",
        "        plt.figure(figsize=(1C0, 6))\n",
        "        data = [scores for scores in test_scores.values()]\n",
        "        plt.boxplot(data, labels=list(test_scores.keys()))\n",
        "        plt.title(f'{model_name}: Performance Distribution Across Seeds')\n",
        "        plt.ylabel('Score')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, f'{model_name}_performance_distribution.png'))\n",
        "        plt.close()\n",
        "\n",
        "    def calculate_learning_curve(self, features: np.ndarray, labels: np.ndarray,\n",
        "                           grid_results: Dict, seeds: List[int],\n",
        "                           train_sizes: np.ndarray = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Calculate learning curves across multiple seeds using incremental training data\n",
        "        and evaluating on the holdout test set.\n",
        "\n",
        "        Args:\n",
        "            features: Input features\n",
        "            labels: Target labels\n",
        "            grid_results: Grid search results containing best parameters for each seed\n",
        "            seeds: List of random seeds\n",
        "            train_sizes: Array of training set sizes to evaluate (proportions from 0 to 1)\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing learning curve data for both models\n",
        "        \"\"\"\n",
        "        if train_sizes is None:\n",
        "            train_sizes = np.linspace(0.05, 1.0, 15)\n",
        "\n",
        "        # Initialize storage for learning curve data\n",
        "        curve_data = {\n",
        "            'SVM': {size: [] for size in train_sizes},\n",
        "            'LogisticRegression': {size: [] for size in train_sizes}\n",
        "        }\n",
        "\n",
        "        # For each seed\n",
        "        for seed in seeds:\n",
        "            # Split data into train and test sets\n",
        "            X_train, X_test, y_train, y_test = self.prepare_data(features, labels, seed)\n",
        "            n_samples = len(y_train)\n",
        "\n",
        "            # For each model type\n",
        "            for model_name in ['SVM', 'LogisticRegression']:\n",
        "                # Get best parameters for this seed\n",
        "                params = grid_results[seed][model_name]['best_params']\n",
        "\n",
        "                # For each training set size\n",
        "                for train_size in train_sizes:\n",
        "                    # Calculate number of samples for this training size\n",
        "                    n_train = int(n_samples * train_size)\n",
        "\n",
        "                    # Create and train model on subset\n",
        "                    model = self.create_model(model_name, params, seed)\n",
        "                    model.fit(X_train[:n_train], y_train[:n_train])\n",
        "\n",
        "                    # Evaluate on test set\n",
        "                    metrics = self.evaluate_model(model, X_test, y_test)\n",
        "                    curve_data[model_name][train_size].append(metrics['macro_f1'])\n",
        "\n",
        "        # Calculate mean and std for each size\n",
        "        learning_curves = {\n",
        "            model_name: {\n",
        "                'train_sizes': train_sizes * len(y_train),\n",
        "                'test_scores_mean': [np.mean(curve_data[model_name][size])\n",
        "                                  for size in train_sizes],\n",
        "                'test_scores_std': [np.std(curve_data[model_name][size])\n",
        "                                  for size in train_sizes]\n",
        "            }\n",
        "            for model_name in ['SVM', 'LogisticRegression']\n",
        "        }\n",
        "\n",
        "        return learning_curves\n",
        "\n",
        "    def plot_averaged_confusion_matrices(self, all_results: Dict[str, Dict]):\n",
        "        \"\"\"\n",
        "        Plot averaged confusion matrices across all seeds for both models.\n",
        "\n",
        "        Args:\n",
        "            all_results: Dictionary containing results for all models and seeds\n",
        "        \"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "        for idx, model_name in enumerate(['SVM', 'LogisticRegression']):\n",
        "            # Get all test confusion matrices for this model\n",
        "            confusion_matrices = []\n",
        "            for seed_results in all_results[model_name].values():\n",
        "                cm = seed_results['test_metrics']['confusion_matrix']\n",
        "                confusion_matrices.append(cm)\n",
        "\n",
        "            # Calculate average confusion matrix\n",
        "            avg_cm = np.mean(confusion_matrices, axis=0)\n",
        "\n",
        "            # Calculate standard deviation for annotations\n",
        "            std_cm = np.std(confusion_matrices, axis=0)\n",
        "\n",
        "            # Create annotations with mean ± std\n",
        "            annotations = np.array([\n",
        "                [f'{avg:.1f} ± {std:.1f}'\n",
        "                for avg, std in zip(row_avg, row_std)]\n",
        "                for row_avg, row_std in zip(avg_cm, std_cm)\n",
        "            ])\n",
        "\n",
        "            # Plot heatmap\n",
        "            ax = ax1 if idx == 0 else ax2\n",
        "            sns.heatmap(\n",
        "                avg_cm,\n",
        "                annot=annotations,\n",
        "                fmt='',\n",
        "                cmap='Blues',\n",
        "                xticklabels=self.le.classes_,\n",
        "                yticklabels=self.le.classes_,\n",
        "                ax=ax\n",
        "            )\n",
        "\n",
        "            ax.set_xlabel('Predicted Label')\n",
        "            ax.set_ylabel('True Label')\n",
        "\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, 'averaged_confusion_matrices.png'),\n",
        "                    dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "    def plot_learning_curves(self, learning_curves: Dict):\n",
        "        \"\"\"\n",
        "        Plot learning curves with total samples info in top left box\n",
        "        and seeds info in top right.\n",
        "\n",
        "        Args:\n",
        "            learning_curves: Dictionary containing learning curve data for both models\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        # Get the current axes\n",
        "        ax = plt.gca()\n",
        "\n",
        "        # Remove the top and right spines\n",
        "        ax.spines['top'].set_visible(False)\n",
        "        ax.spines['right'].set_visible(False)\n",
        "        ax.spines['left'].set_visible(False)\n",
        "        ax.spines['bottom'].set_visible(False)\n",
        "\n",
        "\n",
        "        colors = {\n",
        "            'SVM': 'blue',\n",
        "            'LogisticRegression': 'red'\n",
        "        }\n",
        "\n",
        "        for model_name in ['SVM', 'LogisticRegression']:\n",
        "            data = learning_curves[model_name]\n",
        "\n",
        "            # Plot mean test scores\n",
        "            plt.plot(data['train_sizes'], data['test_scores_mean'],\n",
        "                    f'-', color=colors[model_name], label=f'{model_name}',\n",
        "                    linewidth=2)\n",
        "\n",
        "            # Plot standard deviation bands\n",
        "            plt.fill_between(data['train_sizes'],\n",
        "                            np.array(data['test_scores_mean']) - np.array(data['test_scores_std']),\n",
        "                            np.array(data['test_scores_mean']) + np.array(data['test_scores_std']),\n",
        "                            alpha=0.1, color=colors[model_name])\n",
        "\n",
        "        plt.xlabel('Number of Training Samples')\n",
        "        plt.ylabel('Macro F1 Score')\n",
        "\n",
        "        plt.legend(loc='lower right', frameon=True)\n",
        "\n",
        "        # Make grid lighter\n",
        "        plt.grid(True)\n",
        "\n",
        "\n",
        "        # Get total number of samples (maximum training size)\n",
        "        total_samples = max(learning_curves['SVM']['train_sizes'])\n",
        "\n",
        "        # Add text box with total samples at top left\n",
        "        plt.text(0.02, 0.98, f'Total samples: {total_samples:.0f}',\n",
        "                transform=plt.gca().transAxes,\n",
        "                bbox=dict(facecolor='white',\n",
        "                          alpha=0.8,\n",
        "                          boxstyle='round,pad=0.5'),\n",
        "                verticalalignment='top',\n",
        "                horizontalalignment='left',\n",
        "                fontsize=10)\n",
        "\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, 'learning_curves_mean_ensemble.png'),\n",
        "                    dpi=300,\n",
        "                    bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def save_learning_curves_data(self, learning_curves: Dict, output_dir: str):\n",
        "        \"\"\"\n",
        "        Save learning curves data to a JSON file for later plotting\n",
        "\n",
        "        Args:\n",
        "            learning_curves: Dictionary containing learning curve data for both models\n",
        "            output_dir: Directory to save the output file\n",
        "        \"\"\"\n",
        "        curves_data = {}\n",
        "\n",
        "        for model_name in learning_curves:\n",
        "            data = learning_curves[model_name]\n",
        "            curves_data[model_name] = {\n",
        "                'train_sizes': data['train_sizes'].tolist(),  # Convert numpy array to list\n",
        "                'test_scores_mean': data['test_scores_mean'],\n",
        "                'test_scores_std': data['test_scores_std']\n",
        "            }\n",
        "\n",
        "        output_file = os.path.join(output_dir, 'learning_curves_data.json')\n",
        "        with open(output_file, 'w') as f:\n",
        "            json.dump(curves_data, f, indent=2)\n",
        "\n",
        "\n",
        "def run_evaluation(data_dir: str, grid_search_dir: str) -> Dict:\n",
        "    \"\"\"Run complete evaluation pipeline\"\"\"\n",
        "    try:\n",
        "        # Initialize evaluator\n",
        "        evaluator = ModelEvaluator()\n",
        "\n",
        "        # Load data\n",
        "        data_loader = DataLoader(data_dir)\n",
        "        features, labels = data_loader.load_npz_files()\n",
        "\n",
        "        # Load grid search results\n",
        "        grid_results, seeds = evaluator.load_grid_search_results(grid_search_dir)\n",
        "\n",
        "        all_results = {}\n",
        "        for model_name in ['SVM', 'LogisticRegression']:\n",
        "            all_results[model_name] = {}\n",
        "\n",
        "            for seed in seeds:\n",
        "                # Prepare data using seed\n",
        "                X_train, X_test, y_train, y_test = evaluator.prepare_data(features, labels, seed)\n",
        "\n",
        "                # Get best parameters for this seed\n",
        "                params = grid_results[seed][model_name]['best_params']\n",
        "\n",
        "                # Create and train model\n",
        "                model = evaluator.create_model(model_name, params, seed)\n",
        "                model.fit(X_train, y_train)\n",
        "\n",
        "                # Evaluate model\n",
        "                train_metrics = evaluator.evaluate_model(model, X_train, y_train)\n",
        "                test_metrics = evaluator.evaluate_model(model, X_test, y_test)\n",
        "\n",
        "                # Store results\n",
        "                all_results[model_name][seed] = {\n",
        "                    'train_metrics': train_metrics,\n",
        "                    'test_metrics': test_metrics,\n",
        "                    'params': params\n",
        "                }\n",
        "\n",
        "        # Plot averaged confusion matrices\n",
        "        evaluator.plot_averaged_confusion_matrices(all_results)\n",
        "\n",
        "        # Calculate and saves learning curves\n",
        "        learning_curves = evaluator.calculate_learning_curve(\n",
        "            features, labels, grid_results, seeds\n",
        "        )\n",
        "        evaluator.save_learning_curves_data(learning_curves, evaluator.output_dir)\n",
        "\n",
        "        # Plot learning curves\n",
        "        evaluator.plot_learning_curves(learning_curves)\n",
        "\n",
        "        # Save all results\n",
        "        for model_name in all_results:\n",
        "            with open(os.path.join(evaluator.output_dir, f'{model_name}_evaluation.json'), 'w') as f:\n",
        "                json.dump(all_results[model_name], f, cls=NumpyEncoder)\n",
        "\n",
        "        return {\n",
        "            'model_results': all_results,\n",
        "            'learning_curves': learning_curves\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in evaluation pipeline: {str(e)}\")\n",
        "        raise\n",
        "\n"
      ],
      "metadata": {
        "id": "DKvnb_yOzBr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/model_optimization_20241212_201232_multiseed.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8bg2EEuF2j4q",
        "outputId": "57974fc5-6d58-41d2-92cc-e242d36d9b51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/model_optimization_20241212_201232_multiseed.zip\n",
            "   creating: model_optimization_20241212_201232_multiseed/\n",
            "   creating: model_optimization_20241212_201232_multiseed/LogisticRegression/\n",
            "   creating: model_optimization_20241212_201232_multiseed/SVM/\n",
            "   creating: model_optimization_20241212_201232_multiseed/seed_15/\n",
            "   creating: model_optimization_20241212_201232_multiseed/seed_29/\n",
            "   creating: model_optimization_20241212_201232_multiseed/seed_30/\n",
            "   creating: model_optimization_20241212_201232_multiseed/seed_32/\n",
            "   creating: model_optimization_20241212_201232_multiseed/seed_37/\n",
            "   creating: model_optimization_20241212_201232_multiseed/seed_38/\n",
            "   creating: model_optimization_20241212_201232_multiseed/seed_4/\n",
            "   creating: model_optimization_20241212_201232_multiseed/seed_65/\n",
            "   creating: model_optimization_20241212_201232_multiseed/seed_88/\n",
            "   creating: model_optimization_20241212_201232_multiseed/seed_91/\n",
            "  inflating: model_optimization_20241212_201232_multiseed/SVM_seed_summary.json  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/LogisticRegression_seed_summary.json  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/SVM_seed_comparison.png  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/random_seeds.json  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/LogisticRegression_seed_comparison.png  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/LogisticRegression/random_search_results.csv  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/LogisticRegression/performance_summary.txt  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/LogisticRegression/best_params.json  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_65/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_65/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_65/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_65/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_30/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_30/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_30/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_30/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_32/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_32/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_32/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_32/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_4/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_4/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_4/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_4/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_15/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_15/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_15/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_15/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/SVM/random_search_results.csv  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/SVM/performance_summary.txt  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/SVM/best_params.json  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_88/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_88/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_88/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_88/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_38/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_38/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_38/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_38/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_91/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_91/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_91/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_91/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_37/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_37/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_37/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_37/LogisticRegression_results.csv  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_29/LogisticRegression_metrics.json  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_29/SVM_results.csv  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_29/SVM_metrics.json  \n",
            "  inflating: model_optimization_20241212_201232_multiseed/seed_29/LogisticRegression_results.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Directory containing your NPZ files\n",
        "    data_dir = \"/path/to/Feature Extraction/ViT-SO400M-14-SigLIP\"\n",
        "\n",
        "    # Directory containing grid search results\n",
        "    grid_search_dir = \"/path/to/Single Frame/model_optimization_20250206_115650_multiseed\"\n",
        "\n",
        "    # Run evaluation\n",
        "    results = run_evaluation(data_dir, grid_search_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhG6Q61Y2dcE",
        "outputId": "ae26f28d-cdc3-4567-a3e5-7155a0150e81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation - ResNet"
      ],
      "metadata": {
        "id": "bgIazzaFGgzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, learning_curve\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import balanced_accuracy_score, f1_score, precision_recall_fscore_support\n",
        "from sklearn.metrics import confusion_matrix, make_scorer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy.stats import loguniform\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from typing import Dict, Tuple, List\n",
        "import json\n",
        "import os\n",
        "import logging\n",
        "import glob\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import balanced_accuracy_score, f1_score, precision_recall_fscore_support\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import seaborn as sns\n",
        "from typing import Dict, Tuple, List\n",
        "import logging\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Custom scorer for weighted F1\n",
        "macro_f1_scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "class DataLoader:\n",
        "    \"\"\"Handle loading and processing of NPZ files\"\"\"\n",
        "    def __init__(self, data_dir: str):\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "    def load_npz_files(self) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Load all NPZ files from directory and extract averaged_mean features and labels\n",
        "\n",
        "        Returns:\n",
        "            features: numpy array of averaged_mean features\n",
        "            labels: numpy array of fish species labels\n",
        "        \"\"\"\n",
        "        features_list = []\n",
        "        labels_list = []\n",
        "\n",
        "        # Get all NPZ files in directory\n",
        "        npz_files = glob.glob(os.path.join(self.data_dir, \"*.npz\"))\n",
        "\n",
        "        logging.info(f\"Found {len(npz_files)} NPZ files\")\n",
        "\n",
        "        for npz_file in npz_files:\n",
        "            try:\n",
        "                # Load NPZ file\n",
        "                data = np.load(npz_file, allow_pickle=True)\n",
        "\n",
        "                # Extract averaged_mean feature and label\n",
        "                features = data['features']  # Convert from np.ndarray to dict\n",
        "                fish_species = str(data['fish_species'].item())  # Convert to string\n",
        "\n",
        "                if features is not None:\n",
        "                    features_list.append(features)\n",
        "                    labels_list.append(fish_species)\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error processing file {npz_file}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Convert lists to numpy arrays\n",
        "        features_array = np.array(features_list)\n",
        "        labels_array = np.array(labels_list)\n",
        "\n",
        "        # Log data distribution\n",
        "        unique_labels, counts = np.unique(labels_array, return_counts=True)\n",
        "        for label, count in zip(unique_labels, counts):\n",
        "            percentage = (count / len(labels_array)) * 100\n",
        "            logging.info(f\"Class {label}: {count} samples ({percentage:.2f}%)\")\n",
        "\n",
        "        return features_array, labels_array\n",
        "\n",
        "\n",
        "class NumpyEncoder(json.JSONEncoder):\n",
        "    \"\"\"Custom encoder for numpy data types\"\"\"\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        elif isinstance(obj, np.bool_):\n",
        "            return bool(obj)\n",
        "        return super(NumpyEncoder, self).default(obj)\n",
        "\n",
        "\n",
        "class FishClassifier:\n",
        "    def __init__(self, random_state: int = 42):\n",
        "        self.random_state = random_state\n",
        "        self.le = LabelEncoder()\n",
        "        self.setup_logging()\n",
        "\n",
        "    def setup_logging(self):\n",
        "        \"\"\"Setup logging configuration\"\"\"\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            filename='fish_classifier.log'\n",
        "        )\n",
        "\n",
        "    def prepare_data(self, features: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Prepare data by splitting into train and test sets with stratification\n",
        "        \"\"\"\n",
        "        # Encode labels\n",
        "        y = self.le.fit_transform(labels)\n",
        "\n",
        "        # Create stratified train-test split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            features,\n",
        "            y,\n",
        "            test_size=0.2,\n",
        "            random_state=self.random_state,\n",
        "            stratify=y\n",
        "        )\n",
        "\n",
        "        logging.info(f\"Training set size: {X_train.shape[0]}\")\n",
        "        logging.info(f\"Test set size: {X_test.shape[0]}\")\n",
        "\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "    def create_baseline_models(self) -> Dict:\n",
        "        \"\"\"Create baseline models with default parameters\"\"\"\n",
        "        models = {\n",
        "            'svm': LinearSVC(\n",
        "                random_state=self.random_state,\n",
        "                class_weight='balanced',\n",
        "                max_iter=2000  # Increased to ensure convergence\n",
        "            ),\n",
        "            'logistic': LogisticRegression(\n",
        "                random_state=self.random_state,\n",
        "                class_weight='balanced',\n",
        "                max_iter=2000\n",
        "            )\n",
        "        }\n",
        "        return models\n",
        "\n",
        "    def evaluate_model(self, model, X: np.ndarray, y: np.ndarray, model_name: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Evaluate model performance with multiple metrics\n",
        "        \"\"\"\n",
        "        # Get predictions\n",
        "        y_pred = model.predict(X)\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = {\n",
        "            'accuracy': accuracy_score(y, y_pred),\n",
        "            'macro_f1': f1_score(y, y_pred, average='macro'),\n",
        "            'confusion_matrix': confusion_matrix(y, y_pred),\n",
        "        }\n",
        "\n",
        "        # Calculate per-class metrics\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y, y_pred)\n",
        "\n",
        "        # Add per-class metrics\n",
        "        for i, class_name in enumerate(self.le.classes_):\n",
        "            metrics[f'{class_name}_precision'] = precision[i]\n",
        "            metrics[f'{class_name}_recall'] = recall[i]\n",
        "            metrics[f'{class_name}_f1'] = f1[i]\n",
        "\n",
        "        # Log results\n",
        "        logging.info(f\"\\nResults for {model_name}:\")\n",
        "        logging.info(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
        "        logging.info(f\"Macro F1: {metrics['macro_f1']:.4f}\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def plot_confusion_matrix(self, confusion_mat: np.ndarray, model_name: str):\n",
        "        \"\"\"\n",
        "        Plot confusion matrix heatmap\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(\n",
        "            confusion_mat,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='Blues',\n",
        "            xticklabels=self.le.classes_,\n",
        "            yticklabels=self.le.classes_\n",
        "        )\n",
        "        plt.title(f'Confusion Matrix - {model_name}')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.savefig(f'confusion_matrix_{model_name}.png')\n",
        "        plt.close()\n"
      ],
      "metadata": {
        "id": "wsguZAycGjOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Robust Learning Curve"
      ],
      "metadata": {
        "id": "gyf9kH9GITWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import json\n",
        "import os\n",
        "import logging\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import glob\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "class ModelEvaluator:\n",
        "    \"\"\"Handles multi-seed evaluation of models using grid search results\"\"\"\n",
        "\n",
        "    def __init__(self, random_state: int = 42):\n",
        "        self.random_state = random_state\n",
        "        self.le = LabelEncoder()\n",
        "        self.setup_output_dir()\n",
        "        self.setup_logging()\n",
        "\n",
        "    def setup_output_dir(self):\n",
        "        \"\"\"Create output directory with timestamp\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        self.output_dir = f'model_evaluation_{timestamp}'\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "    def setup_logging(self):\n",
        "        \"\"\"Configure logging\"\"\"\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            handlers=[\n",
        "                logging.FileHandler(os.path.join(self.output_dir, 'evaluation.log')),\n",
        "                logging.StreamHandler()\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def load_grid_search_results(self, grid_search_dir: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Load results from multi-seed grid search\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "\n",
        "        # Load random seeds\n",
        "        with open(os.path.join(grid_search_dir, 'random_seeds.json'), 'r') as f:\n",
        "            seeds_info = json.load(f)\n",
        "            seeds = seeds_info['generated_seeds']\n",
        "\n",
        "        # Process each seed directory\n",
        "        for seed in seeds:\n",
        "            seed_dir = os.path.join(grid_search_dir, f'seed_{seed}')\n",
        "            results[seed] = {}\n",
        "\n",
        "            # Load results for each model type\n",
        "            for model_name in ['SVM', 'LogisticRegression']:\n",
        "                metrics_file = os.path.join(seed_dir, f'{model_name}_metrics.json')\n",
        "                with open(metrics_file, 'r') as f:\n",
        "                    results[seed][model_name] = json.load(f)\n",
        "\n",
        "        return results, seeds\n",
        "\n",
        "    def prepare_data(self, features: np.ndarray, labels: np.ndarray, seed: int) -> Tuple:\n",
        "        \"\"\"Prepare train-test split using specific seed\"\"\"\n",
        "        y = self.le.fit_transform(labels)\n",
        "        return train_test_split(features, y, test_size=0.2, random_state=seed, stratify=y)\n",
        "\n",
        "    def create_model(self, model_type: str, params: Dict, seed: int) -> Any:\n",
        "        \"\"\"Create model with specified parameters\"\"\"\n",
        "        if model_type == 'SVM':\n",
        "            return LinearSVC(random_state=seed, max_iter=2000, **params)\n",
        "        elif model_type == 'LogisticRegression':\n",
        "            return LogisticRegression(random_state=seed, max_iter=2000, **params)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "    def evaluate_model(self, model, X: np.ndarray, y: np.ndarray) -> Dict:\n",
        "        \"\"\"Comprehensive model evaluation\"\"\"\n",
        "        y_pred = model.predict(X)\n",
        "\n",
        "        metrics = {\n",
        "            'weighted_f1': f1_score(y, y_pred, average='weighted'),\n",
        "            'macro_f1': f1_score(y, y_pred, average='macro'),\n",
        "            'accuracy': accuracy_score(y, y_pred),\n",
        "            'confusion_matrix': confusion_matrix(y, y_pred)\n",
        "        }\n",
        "\n",
        "        # Calculate per-class metrics\n",
        "        precision, recall, f1, support = precision_recall_fscore_support(y, y_pred)\n",
        "\n",
        "        for i, class_name in enumerate(self.le.classes_):\n",
        "            metrics[f'{class_name}_precision'] = precision[i]\n",
        "            metrics[f'{class_name}_recall'] = recall[i]\n",
        "            metrics[f'{class_name}_f1'] = f1[i]\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def plot_confusion_matrices(self, confusion_matrices: List[np.ndarray],\n",
        "                              model_name: str, seed: int):\n",
        "        \"\"\"Plot confusion matrices for a specific seed\"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "        # Plot training confusion matrix\n",
        "        sns.heatmap(confusion_matrices[0], annot=True, fmt='d', ax=ax1, cmap='Blues',\n",
        "                   xticklabels=self.le.classes_, yticklabels=self.le.classes_)\n",
        "        ax1.set_title(f'Training Confusion Matrix\\nSeed: {seed}')\n",
        "        ax1.set_xlabel('Predicted Label')\n",
        "        ax1.set_ylabel('True Label')\n",
        "\n",
        "        # Plot test confusion matrix\n",
        "        sns.heatmap(confusion_matrices[1], annot=True, fmt='d', ax=ax2, cmap='Blues',\n",
        "                   xticklabels=self.le.classes_, yticklabels=self.le.classes_)\n",
        "        ax2.set_title(f'Test Confusion Matrix\\nSeed: {seed}')\n",
        "        ax2.set_xlabel('Predicted Label')\n",
        "        ax2.set_ylabel('True Label')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, f'{model_name}_confusion_matrices_seed_{seed}.png'))\n",
        "        plt.close()\n",
        "\n",
        "    def plot_aggregated_results(self, all_results: Dict, model_name: str):\n",
        "        \"\"\"Plot aggregated results across seeds\"\"\"\n",
        "        test_scores = {\n",
        "            'weighted_f1': [],\n",
        "            'macro_f1': [],\n",
        "            'accuracy': []\n",
        "        }\n",
        "\n",
        "        for seed_results in all_results.values():\n",
        "            metrics = seed_results['test_metrics']\n",
        "            for metric in test_scores.keys():\n",
        "                test_scores[metric].append(metrics[metric])\n",
        "\n",
        "        # Create box plots\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        data = [scores for scores in test_scores.values()]\n",
        "        plt.boxplot(data, labels=list(test_scores.keys()))\n",
        "        plt.title(f'{model_name}: Performance Distribution Across Seeds')\n",
        "        plt.ylabel('Score')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, f'{model_name}_performance_distribution.png'))\n",
        "        plt.close()\n",
        "\n",
        "    def calculate_learning_curve(self, features: np.ndarray, labels: np.ndarray,\n",
        "                           grid_results: Dict, seeds: List[int],\n",
        "                           train_sizes: np.ndarray = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Calculate learning curves across multiple seeds using incremental training data\n",
        "        and evaluating on the holdout test set.\n",
        "\n",
        "        Args:\n",
        "            features: Input features\n",
        "            labels: Target labels\n",
        "            grid_results: Grid search results containing best parameters for each seed\n",
        "            seeds: List of random seeds\n",
        "            train_sizes: Array of training set sizes to evaluate (proportions from 0 to 1)\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing learning curve data for both models\n",
        "        \"\"\"\n",
        "        if train_sizes is None:\n",
        "            train_sizes = np.linspace(0.05, 1.0, 15)\n",
        "\n",
        "        # Initialize storage for learning curve data\n",
        "        curve_data = {\n",
        "            'SVM': {size: [] for size in train_sizes},\n",
        "            'LogisticRegression': {size: [] for size in train_sizes}\n",
        "        }\n",
        "\n",
        "        # For each seed\n",
        "        for seed in seeds:\n",
        "            # Split data into train and test sets\n",
        "            X_train, X_test, y_train, y_test = self.prepare_data(features, labels, seed)\n",
        "            n_samples = len(y_train)\n",
        "\n",
        "            # For each model type\n",
        "            for model_name in ['SVM', 'LogisticRegression']:\n",
        "                # Get best parameters for this seed\n",
        "                params = grid_results[seed][model_name]['best_params']\n",
        "\n",
        "                # For each training set size\n",
        "                for train_size in train_sizes:\n",
        "                    # Calculate number of samples for this training size\n",
        "                    n_train = int(n_samples * train_size)\n",
        "\n",
        "                    # Create and train model on subset\n",
        "                    model = self.create_model(model_name, params, seed)\n",
        "                    model.fit(X_train[:n_train], y_train[:n_train])\n",
        "\n",
        "                    # Evaluate on test set\n",
        "                    metrics = self.evaluate_model(model, X_test, y_test)\n",
        "                    curve_data[model_name][train_size].append(metrics['macro_f1'])\n",
        "\n",
        "        # Calculate mean and std for each size\n",
        "        learning_curves = {\n",
        "            model_name: {\n",
        "                'train_sizes': train_sizes * len(y_train),\n",
        "                'test_scores_mean': [np.mean(curve_data[model_name][size])\n",
        "                                  for size in train_sizes],\n",
        "                'test_scores_std': [np.std(curve_data[model_name][size])\n",
        "                                  for size in train_sizes]\n",
        "            }\n",
        "            for model_name in ['SVM', 'LogisticRegression']\n",
        "        }\n",
        "\n",
        "        return learning_curves\n",
        "\n",
        "    def plot_averaged_confusion_matrices(self, all_results: Dict[str, Dict]):\n",
        "        \"\"\"\n",
        "        Plot averaged confusion matrices across all seeds for both models.\n",
        "\n",
        "        Args:\n",
        "            all_results: Dictionary containing results for all models and seeds\n",
        "        \"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "        for idx, model_name in enumerate(['SVM', 'LogisticRegression']):\n",
        "            # Get all test confusion matrices for this model\n",
        "            confusion_matrices = []\n",
        "            for seed_results in all_results[model_name].values():\n",
        "                cm = seed_results['test_metrics']['confusion_matrix']\n",
        "                confusion_matrices.append(cm)\n",
        "\n",
        "            # Calculate average confusion matrix\n",
        "            avg_cm = np.mean(confusion_matrices, axis=0)\n",
        "\n",
        "            # Calculate standard deviation for annotations\n",
        "            std_cm = np.std(confusion_matrices, axis=0)\n",
        "\n",
        "            # Create annotations with mean ± std\n",
        "            annotations = np.array([\n",
        "                [f'{avg:.1f} ± {std:.1f}'\n",
        "                for avg, std in zip(row_avg, row_std)]\n",
        "                for row_avg, row_std in zip(avg_cm, std_cm)\n",
        "            ])\n",
        "\n",
        "            # Plot heatmap\n",
        "            ax = ax1 if idx == 0 else ax2\n",
        "            sns.heatmap(\n",
        "                avg_cm,\n",
        "                annot=annotations,\n",
        "                fmt='',\n",
        "                cmap='Blues',\n",
        "                xticklabels=self.le.classes_,\n",
        "                yticklabels=self.le.classes_,\n",
        "                ax=ax\n",
        "            )\n",
        "\n",
        "            ax.set_xlabel('Predicted Label')\n",
        "            ax.set_ylabel('True Label')\n",
        "\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, 'averaged_confusion_matrices.png'),\n",
        "                    dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "    def plot_learning_curves(self, learning_curves: Dict):\n",
        "        \"\"\"\n",
        "        Plot learning curves with total samples info in top left box\n",
        "        and seeds info in top right.\n",
        "\n",
        "        Args:\n",
        "            learning_curves: Dictionary containing learning curve data for both models\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        # Get the current axes\n",
        "        ax = plt.gca()\n",
        "\n",
        "        # Remove the top and right spines\n",
        "        ax.spines['top'].set_visible(False)\n",
        "        ax.spines['right'].set_visible(False)\n",
        "        ax.spines['left'].set_visible(False)\n",
        "        ax.spines['bottom'].set_visible(False)\n",
        "\n",
        "\n",
        "        colors = {\n",
        "            'SVM': 'blue',\n",
        "            'LogisticRegression': 'red'\n",
        "        }\n",
        "\n",
        "        for model_name in ['SVM', 'LogisticRegression']:\n",
        "            data = learning_curves[model_name]\n",
        "\n",
        "            # Plot mean test scores\n",
        "            plt.plot(data['train_sizes'], data['test_scores_mean'],\n",
        "                    f'-', color=colors[model_name], label=f'{model_name}',\n",
        "                    linewidth=2)\n",
        "\n",
        "            # Plot standard deviation bands\n",
        "            plt.fill_between(data['train_sizes'],\n",
        "                            np.array(data['test_scores_mean']) - np.array(data['test_scores_std']),\n",
        "                            np.array(data['test_scores_mean']) + np.array(data['test_scores_std']),\n",
        "                            alpha=0.1, color=colors[model_name])\n",
        "\n",
        "        plt.xlabel('Number of Training Samples')\n",
        "        plt.ylabel('Macro F1 Score')\n",
        "\n",
        "        plt.legend(loc='lower right', frameon=True)\n",
        "\n",
        "        # Make grid lighter\n",
        "        plt.grid(True)\n",
        "\n",
        "\n",
        "        # Get total number of samples (maximum training size)\n",
        "        total_samples = max(learning_curves['SVM']['train_sizes'])\n",
        "\n",
        "        # Add text box with total samples at top left\n",
        "        plt.text(0.02, 0.98, f'Total samples: {total_samples:.0f}',\n",
        "                transform=plt.gca().transAxes,\n",
        "                bbox=dict(facecolor='white',\n",
        "                          alpha=0.8,\n",
        "                          boxstyle='round,pad=0.5'),\n",
        "                verticalalignment='top',\n",
        "                horizontalalignment='left',\n",
        "                fontsize=10)\n",
        "\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, 'learning_curves_mean_ensemble.png'),\n",
        "                    dpi=300,\n",
        "                    bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def save_learning_curves_data(self, learning_curves: Dict, output_dir: str):\n",
        "        \"\"\"\n",
        "        Save learning curves data to a JSON file for later plotting\n",
        "\n",
        "        Args:\n",
        "            learning_curves: Dictionary containing learning curve data for both models\n",
        "            output_dir: Directory to save the output file\n",
        "        \"\"\"\n",
        "        curves_data = {}\n",
        "\n",
        "        for model_name in learning_curves:\n",
        "            data = learning_curves[model_name]\n",
        "            curves_data[model_name] = {\n",
        "                'train_sizes': data['train_sizes'].tolist(),  # Convert numpy array to list\n",
        "                'test_scores_mean': data['test_scores_mean'],\n",
        "                'test_scores_std': data['test_scores_std']\n",
        "            }\n",
        "\n",
        "        output_file = os.path.join(output_dir, 'learning_curves_data.json')\n",
        "        with open(output_file, 'w') as f:\n",
        "            json.dump(curves_data, f, indent=2)\n",
        "\n",
        "\n",
        "def run_evaluation(data_dir: str, grid_search_dir: str) -> Dict:\n",
        "    \"\"\"Run complete evaluation pipeline\"\"\"\n",
        "    try:\n",
        "        # Initialize evaluator\n",
        "        evaluator = ModelEvaluator()\n",
        "\n",
        "        # Load data\n",
        "        data_loader = DataLoader(data_dir)\n",
        "        features, labels = data_loader.load_npz_files()\n",
        "\n",
        "        # Load grid search results\n",
        "        grid_results, seeds = evaluator.load_grid_search_results(grid_search_dir)\n",
        "\n",
        "        all_results = {}\n",
        "        for model_name in ['SVM', 'LogisticRegression']:\n",
        "            all_results[model_name] = {}\n",
        "\n",
        "            for seed in seeds:\n",
        "                # Prepare data using seed\n",
        "                X_train, X_test, y_train, y_test = evaluator.prepare_data(features, labels, seed)\n",
        "\n",
        "                # Get best parameters for this seed\n",
        "                params = grid_results[seed][model_name]['best_params']\n",
        "\n",
        "                # Create and train model\n",
        "                model = evaluator.create_model(model_name, params, seed)\n",
        "                model.fit(X_train, y_train)\n",
        "\n",
        "                # Evaluate model\n",
        "                train_metrics = evaluator.evaluate_model(model, X_train, y_train)\n",
        "                test_metrics = evaluator.evaluate_model(model, X_test, y_test)\n",
        "\n",
        "                # Store results\n",
        "                all_results[model_name][seed] = {\n",
        "                    'train_metrics': train_metrics,\n",
        "                    'test_metrics': test_metrics,\n",
        "                    'params': params\n",
        "                }\n",
        "\n",
        "        # Plot averaged confusion matrices\n",
        "        evaluator.plot_averaged_confusion_matrices(all_results)\n",
        "\n",
        "        # Calculate and saves learning curves\n",
        "        learning_curves = evaluator.calculate_learning_curve(\n",
        "            features, labels, grid_results, seeds\n",
        "        )\n",
        "        evaluator.save_learning_curves_data(learning_curves, evaluator.output_dir)\n",
        "\n",
        "        # Plot learning curves\n",
        "        evaluator.plot_learning_curves(learning_curves)\n",
        "\n",
        "        # Save all results\n",
        "        for model_name in all_results:\n",
        "            with open(os.path.join(evaluator.output_dir, f'{model_name}_evaluation.json'), 'w') as f:\n",
        "                json.dump(all_results[model_name], f, cls=NumpyEncoder)\n",
        "\n",
        "        return {\n",
        "            'model_results': all_results,\n",
        "            'learning_curves': learning_curves\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in evaluation pipeline: {str(e)}\")\n",
        "        raise\n",
        "\n"
      ],
      "metadata": {
        "id": "ttGczbv_IW5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir \"/content/model_optimization_20241211_152718_multiseed\"\n",
        "!unzip \"/content/model_optimization_20241211_152718_multiseed.zip\" -d \"/content/model_optimization_20241211_152718_multiseedy\""
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiLo_hZbIg-e",
        "outputId": "1348b987-ef2d-48c6-db39-c97650eda98a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/model_optimization_20241211_152718_multiseed.zip\n",
            "replace averaged_validation_curves.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: averaged_validation_curves.png  \n",
            "replace SVM_seed_summary.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: SVM_seed_summary.json   \n",
            "replace SVM_seed_comparison.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: SVM_seed_comparison.png  \n",
            "replace LogisticRegression_seed_summary.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LogisticRegression_seed_summary.json  \n",
            "replace LogisticRegression_seed_comparison.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LogisticRegression_seed_comparison.png  \n",
            "replace seed_37/SVM_metrics.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: \n",
            "error:  invalid response [{ENTER}]\n",
            "replace seed_37/SVM_metrics.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: yy\n",
            "  inflating: seed_37/SVM_metrics.json  \n",
            "replace seed_37/LogisticRegression_results.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_37/LogisticRegression_results.csv  \n",
            "replace seed_37/LogisticRegression_metrics.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_37/LogisticRegression_metrics.json  \n",
            "replace seed_37/SVM_results.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_37/SVM_results.csv  \n",
            "replace seed_88/SVM_metrics.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_88/SVM_metrics.json  \n",
            "replace seed_88/LogisticRegression_results.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_88/LogisticRegression_results.csv  \n",
            "replace seed_88/LogisticRegression_metrics.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_88/LogisticRegression_metrics.json  \n",
            "replace seed_88/SVM_results.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_88/SVM_results.csv  \n",
            "replace seed_29/SVM_metrics.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_29/SVM_metrics.json  \n",
            "replace seed_29/LogisticRegression_results.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_29/LogisticRegression_results.csv  \n",
            "replace seed_29/LogisticRegression_metrics.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_29/LogisticRegression_metrics.json  \n",
            "replace seed_29/SVM_results.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_29/SVM_results.csv  \n",
            "replace seed_30/SVM_metrics.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_30/SVM_metrics.json  \n",
            "replace seed_30/LogisticRegression_results.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_30/LogisticRegression_results.csv  \n",
            "replace seed_30/LogisticRegression_metrics.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_30/LogisticRegression_metrics.json  \n",
            "replace seed_30/SVM_results.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_30/SVM_results.csv  \n",
            "replace seed_65/SVM_metrics.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_65/SVM_metrics.json  \n",
            "replace seed_65/LogisticRegression_results.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_65/LogisticRegression_results.csv  \n",
            "replace seed_65/LogisticRegression_metrics.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_65/LogisticRegression_metrics.json  \n",
            "replace seed_65/SVM_results.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_65/SVM_results.csv  \n",
            "replace seed_91/SVM_metrics.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_91/SVM_metrics.json  \n",
            "replace seed_91/LogisticRegression_results.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_91/LogisticRegression_results.csv  \n",
            "replace seed_91/LogisticRegression_metrics.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_91/LogisticRegression_metrics.json  \n",
            "replace seed_91/SVM_results.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_91/SVM_results.csv  \n",
            "replace seed_38/SVM_metrics.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_38/SVM_metrics.json  \n",
            "replace seed_38/LogisticRegression_results.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_38/LogisticRegression_results.csv  \n",
            "replace seed_38/LogisticRegression_metrics.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_38/LogisticRegression_metrics.json  \n",
            "replace seed_38/SVM_results.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_38/SVM_results.csv  \n",
            "replace seed_15/SVM_metrics.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_15/SVM_metrics.json  \n",
            "replace seed_15/LogisticRegression_results.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_15/LogisticRegression_results.csv  \n",
            "replace seed_15/LogisticRegression_metrics.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_15/LogisticRegression_metrics.json  \n",
            "replace seed_15/SVM_results.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_15/SVM_results.csv  \n",
            "replace seed_32/SVM_metrics.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_32/SVM_metrics.json  \n",
            "replace seed_32/LogisticRegression_results.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_32/LogisticRegression_results.csv  \n",
            "replace seed_32/LogisticRegression_metrics.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_32/LogisticRegression_metrics.json  \n",
            "replace seed_32/SVM_results.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_32/SVM_results.csv  \n",
            "replace seed_4/SVM_metrics.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_4/SVM_metrics.json  \n",
            "replace seed_4/LogisticRegression_results.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_4/LogisticRegression_results.csv  \n",
            "replace seed_4/LogisticRegression_metrics.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_4/LogisticRegression_metrics.json  \n",
            "replace seed_4/SVM_results.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: seed_4/SVM_results.csv  \n",
            "Archive:  /content/model_optimization_20241211_152718_multiseed.zip\n",
            "   creating: /content/model_optimization_20241211_152718_multiseed/seed_15/\n",
            "   creating: /content/model_optimization_20241211_152718_multiseed/seed_29/\n",
            "   creating: /content/model_optimization_20241211_152718_multiseed/seed_30/\n",
            "   creating: /content/model_optimization_20241211_152718_multiseed/seed_32/\n",
            "   creating: /content/model_optimization_20241211_152718_multiseed/seed_37/\n",
            "   creating: /content/model_optimization_20241211_152718_multiseed/seed_38/\n",
            "   creating: /content/model_optimization_20241211_152718_multiseed/seed_4/\n",
            "   creating: /content/model_optimization_20241211_152718_multiseed/seed_65/\n",
            "   creating: /content/model_optimization_20241211_152718_multiseed/seed_88/\n",
            "   creating: /content/model_optimization_20241211_152718_multiseed/seed_91/\n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/averaged_validation_curves.png  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/SVM_seed_summary.json  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/SVM_seed_comparison.png  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/LogisticRegression_seed_summary.json  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/LogisticRegression_seed_comparison.png  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_37/SVM_metrics.json  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_37/LogisticRegression_results.csv  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_37/LogisticRegression_metrics.json  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_37/SVM_results.csv  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_88/SVM_metrics.json  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_88/LogisticRegression_results.csv  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_88/LogisticRegression_metrics.json  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_88/SVM_results.csv  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_29/SVM_metrics.json  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_29/LogisticRegression_results.csv  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_29/LogisticRegression_metrics.json  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_29/SVM_results.csv  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_30/SVM_metrics.json  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_30/LogisticRegression_results.csv  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_30/LogisticRegression_metrics.json  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_30/SVM_results.csv  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_65/SVM_metrics.json  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_65/LogisticRegression_results.csv  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_65/LogisticRegression_metrics.json  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_65/SVM_results.csv  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_91/SVM_metrics.json  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_91/LogisticRegression_results.csv  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_91/LogisticRegression_metrics.json  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_91/SVM_results.csv  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_38/SVM_metrics.json  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_38/LogisticRegression_results.csv  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_38/LogisticRegression_metrics.json  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_38/SVM_results.csv  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_15/SVM_metrics.json  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_15/LogisticRegression_results.csv  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_15/LogisticRegression_metrics.json  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_15/SVM_results.csv  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_32/SVM_metrics.json  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_32/LogisticRegression_results.csv  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_32/LogisticRegression_metrics.json  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_32/SVM_results.csv  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_4/SVM_metrics.json  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_4/LogisticRegression_results.csv  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_4/LogisticRegression_metrics.json  \n",
            "  inflating: /content/model_optimization_20241211_152718_multiseed/seed_4/SVM_results.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "def create_local_copy(drive_path: str) -> str:\n",
        "    # Create directory in /content/\n",
        "    local_path = '/content/local_features'\n",
        "\n",
        "    # Remove if already exists\n",
        "    if os.path.exists(local_path):\n",
        "        shutil.rmtree(local_path)\n",
        "\n",
        "    # Copy data from Drive to local\n",
        "    print(f\"Copying data to: {local_path}\")\n",
        "    shutil.copytree(drive_path, local_path)\n",
        "\n",
        "    return local_path\n",
        "\n",
        "# Example usage\n",
        "drive_path = '/path/to/Feature Extraction/ResNet-50'\n",
        "local_path = create_local_copy(drive_path)\n",
        "print(f\"Data copied to: {local_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5lDX62Aqpmp",
        "outputId": "eb359ef7-cd7e-4398-eeac-2e801b166707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying data to: /content/local_features\n",
            "Data copied to: /content/local_features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Directory containing your NPZ files\n",
        "    data_dir = \"/content/local_features\"\n",
        "\n",
        "    # Directory containing grid search results\n",
        "    grid_search_dir = \"/path/to/ResNet_Benchmark/model_optimization_20250203_223027_multiseed\"\n",
        "\n",
        "    # Run evaluation\n",
        "    results = run_evaluation(data_dir, grid_search_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jxTawBFQIcgb",
        "outputId": "443e761a-f6fe-4e97-ee26-93d883f4b87a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation - Temporal Voting"
      ],
      "metadata": {
        "id": "DzalPfb9aZfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import balanced_accuracy_score, f1_score, precision_recall_fscore_support\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import logging\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "class NumpyEncoder(json.JSONEncoder):\n",
        "    \"\"\"Custom encoder for numpy data types\"\"\"\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        elif isinstance(obj, np.bool_):\n",
        "            return bool(obj)\n",
        "        return super(NumpyEncoder, self).default(obj)\n",
        "\n",
        "class TemporalVotingEvaluator:\n",
        "    \"\"\"Evaluates models using temporal voting across video frames\"\"\"\n",
        "\n",
        "    def __init__(self, random_state: int = 42):\n",
        "        self.random_state = random_state\n",
        "        self.le = LabelEncoder()\n",
        "        self.setup_output_dir()\n",
        "        self.setup_logging()\n",
        "\n",
        "    def setup_output_dir(self):\n",
        "        \"\"\"Create output directory with timestamp\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        self.output_dir = f'temporal_voting_evaluation_{timestamp}'\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "    def setup_logging(self):\n",
        "        \"\"\"Configure logging\"\"\"\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            handlers=[\n",
        "                logging.FileHandler(os.path.join(self.output_dir, 'temporal_voting.log')),\n",
        "                logging.StreamHandler()\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def load_video_features(self, data_dir: str) -> Tuple[Dict[str, Dict], List[str]]:\n",
        "        \"\"\"\n",
        "        Load features for all frames from all videos\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping video IDs to their frame features and labels\n",
        "            List of all unique labels\n",
        "        \"\"\"\n",
        "        video_data = {}\n",
        "        all_labels = set()\n",
        "\n",
        "        npz_files = glob.glob(os.path.join(data_dir, \"*.npz\"))\n",
        "        logging.info(f\"Found {len(npz_files)} NPZ files\")\n",
        "\n",
        "        for npz_file in npz_files:\n",
        "            try:\n",
        "                # Extract video ID from filename\n",
        "                video_id = os.path.basename(npz_file).split('_features.npz')[0]\n",
        "\n",
        "                # Load NPZ file\n",
        "                data = np.load(npz_file, allow_pickle=True)\n",
        "\n",
        "                # Get all frame features and label\n",
        "                frame_features = data['features'].item()  # Dictionary of frame features\n",
        "                fish_species = str(data['fish_species'].item())\n",
        "                middle_frame = data['middle_frame'].item()\n",
        "\n",
        "                video_data[video_id] = {\n",
        "                    'features': frame_features,\n",
        "                    'label': fish_species,\n",
        "                    'middle_frame': middle_frame\n",
        "                }\n",
        "                all_labels.add(fish_species)\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error processing file {npz_file}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        return video_data, sorted(list(all_labels))\n",
        "\n",
        "    def prepare_data_split(self, video_data: Dict, seed: int) -> Tuple[List[str], List[str]]:\n",
        "        \"\"\"\n",
        "        Split video IDs into train and test sets while maintaining class distribution\n",
        "\n",
        "        Args:\n",
        "            video_data: Dictionary of video data\n",
        "            seed: Random seed for reproducibility\n",
        "\n",
        "        Returns:\n",
        "            Lists of video IDs for training and testing\n",
        "        \"\"\"\n",
        "        # Prepare video IDs and labels\n",
        "        video_ids = list(video_data.keys())\n",
        "        labels = [video_data[vid]['label'] for vid in video_ids]\n",
        "\n",
        "        # Perform stratified split on video IDs\n",
        "        train_ids, test_ids = train_test_split(\n",
        "            video_ids,\n",
        "            test_size=0.2,\n",
        "            random_state=seed,\n",
        "            stratify=labels\n",
        "        )\n",
        "\n",
        "        # Log split information\n",
        "        train_labels = [video_data[vid]['label'] for vid in train_ids]\n",
        "        test_labels = [video_data[vid]['label'] for vid in test_ids]\n",
        "\n",
        "        for label in set(labels):\n",
        "            train_count = train_labels.count(label)\n",
        "            test_count = test_labels.count(label)\n",
        "            total = train_count + test_count\n",
        "            logging.info(f\"Class {label}:\")\n",
        "            logging.info(f\"  Train: {train_count} ({train_count/total*100:.2f}%)\")\n",
        "            logging.info(f\"  Test: {test_count} ({test_count/total*100:.2f}%)\")\n",
        "\n",
        "        return train_ids, test_ids\n",
        "\n",
        "    def prepare_central_frame_data(self, video_data: Dict) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Extract central frame features and labels for initial model training\"\"\"\n",
        "        features_list = []\n",
        "        labels_list = []\n",
        "\n",
        "        for video_info in video_data.values():\n",
        "            middle_frame = video_info['middle_frame']\n",
        "            features = video_info['features'][middle_frame]\n",
        "            label = video_info['label']\n",
        "\n",
        "            features_list.append(features)\n",
        "            labels_list.append(label)\n",
        "\n",
        "        return np.array(features_list), np.array(labels_list)\n",
        "\n",
        "    def train_model(self, model_type: str, params: Dict,\n",
        "                   X_train: np.ndarray, y_train: np.ndarray) -> Any:\n",
        "        \"\"\"Train model with specified parameters\"\"\"\n",
        "        if model_type == 'SVM':\n",
        "            model = LinearSVC(max_iter=2000, **params)\n",
        "        elif model_type == 'LogisticRegression':\n",
        "            model = LogisticRegression(max_iter=2000, **params)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        return model\n",
        "\n",
        "    def get_prediction_probabilities(self, model, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Get prediction probabilities from model\"\"\"\n",
        "        if isinstance(model, LogisticRegression):\n",
        "            return model.predict_proba(X)\n",
        "        else:  # SVM\n",
        "            decision_values = model.decision_function(X)\n",
        "            if decision_values.ndim == 1:  # Binary classification\n",
        "                decision_values = np.column_stack([-decision_values, decision_values])\n",
        "            return self._softmax(decision_values)\n",
        "\n",
        "    def _softmax(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Apply softmax to array\"\"\"\n",
        "        exp_X = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
        "        return exp_X / np.sum(exp_X, axis=1, keepdims=True)\n",
        "\n",
        "    def temporal_voting_predict(self, model, video_features: Dict) -> Tuple[int, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Perform temporal voting on all frames in a video\n",
        "\n",
        "        Returns:\n",
        "            Predicted class index and aggregated probabilities\n",
        "        \"\"\"\n",
        "        # Get predictions for all frames\n",
        "        frame_predictions = []\n",
        "        for frame_num, features in video_features.items():\n",
        "            probs = self.get_prediction_probabilities(model, features.reshape(1, -1))\n",
        "            frame_predictions.append(probs)\n",
        "\n",
        "        # Average probabilities across frames\n",
        "        avg_probs = np.mean(frame_predictions, axis=0)\n",
        "        return np.argmax(avg_probs), avg_probs\n",
        "\n",
        "    def evaluate_temporal_voting(self, model, video_data: Dict,\n",
        "                           video_indices: List[str]) -> Dict:\n",
        "        \"\"\"Evaluate model using temporal voting on specified videos\"\"\"\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        all_probs = []\n",
        "\n",
        "        for video_id in video_indices:\n",
        "            video_info = video_data[video_id]\n",
        "            true_label = self.le.transform([video_info['label']])[0]\n",
        "\n",
        "            # Get prediction using temporal voting\n",
        "            pred_label, probs = self.temporal_voting_predict(\n",
        "                model, video_info['features']\n",
        "            )\n",
        "\n",
        "            y_true.append(true_label)\n",
        "            y_pred.append(pred_label)\n",
        "            all_probs.append(probs)\n",
        "\n",
        "        y_true = np.array(y_true)\n",
        "        y_pred = np.array(y_pred)\n",
        "\n",
        "        # Calculate all metrics\n",
        "        metrics = {\n",
        "            'weighted_f1': f1_score(y_true, y_pred, average='weighted'),\n",
        "            'macro_f1': f1_score(y_true, y_pred, average='macro'),\n",
        "            'accuracy': accuracy_score(y_true, y_pred),\n",
        "            'confusion_matrix': confusion_matrix(y_true, y_pred).tolist()  # Convert to list for JSON serialization\n",
        "        }\n",
        "\n",
        "        # Calculate per-class metrics\n",
        "        precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred)\n",
        "\n",
        "        for i, class_name in enumerate(self.le.classes_):\n",
        "            metrics[f'{class_name}_precision'] = precision[i]\n",
        "            metrics[f'{class_name}_recall'] = recall[i]\n",
        "            metrics[f'{class_name}_f1'] = f1[i]\n",
        "            metrics[f'{class_name}_support'] = int(support[i])\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def plot_confusion_matrix(self, cm: np.ndarray, model_name: str, seed: int):\n",
        "        \"\"\"Plot confusion matrix\"\"\"\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=self.le.classes_,\n",
        "                   yticklabels=self.le.classes_)\n",
        "        plt.title(f'Temporal Voting Confusion Matrix\\n{model_name} - Seed {seed}')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.savefig(os.path.join(self.output_dir,\n",
        "                                f'{model_name}_confusion_matrix_seed_{seed}.png'))\n",
        "        plt.close()\n",
        "\n",
        "    def plot_averaged_confusion_matrices(self, results: Dict):\n",
        "        \"\"\"\n",
        "        Plot averaged confusion matrices across all seeds for both models.\n",
        "\n",
        "        Args:\n",
        "            results: Dictionary containing results for all models and seeds\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(20, 8))\n",
        "\n",
        "        for idx, model_name in enumerate(['SVM', 'LogisticRegression']):\n",
        "            # Get all confusion matrices for this model\n",
        "            confusion_matrices = []\n",
        "            for seed_results in results[model_name].values():\n",
        "                if isinstance(seed_results, dict) and 'confusion_matrix' in seed_results:\n",
        "                    cm = np.array(seed_results['confusion_matrix'])\n",
        "                    confusion_matrices.append(cm)\n",
        "\n",
        "            if not confusion_matrices:  # Skip if no valid confusion matrices\n",
        "                logging.warning(f\"No confusion matrices found for {model_name}\")\n",
        "                continue\n",
        "\n",
        "            # Calculate average and standard deviation\n",
        "            avg_cm = np.mean(confusion_matrices, axis=0)\n",
        "            std_cm = np.std(confusion_matrices, axis=0)\n",
        "\n",
        "            # Create subplot\n",
        "            plt.subplot(1, 2, idx + 1)\n",
        "\n",
        "            # Create annotations with mean ± std\n",
        "            annotations = np.array([\n",
        "                [f'{avg:.1f}±{std:.1f}'\n",
        "                for avg, std in zip(row_avg, row_std)]\n",
        "                for row_avg, row_std in zip(avg_cm, std_cm)\n",
        "            ])\n",
        "\n",
        "            # Plot heatmap\n",
        "            sns.heatmap(\n",
        "                avg_cm,\n",
        "                annot=annotations,\n",
        "                fmt='',\n",
        "                cmap='Blues',\n",
        "                xticklabels=self.le.classes_,\n",
        "                yticklabels=self.le.classes_\n",
        "            )\n",
        "\n",
        "            plt.title(f'Average Confusion Matrix - {model_name}\\n(across {len(confusion_matrices)} seeds)')\n",
        "            plt.ylabel('True Label')\n",
        "            plt.xlabel('Predicted Label')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, 'averaged_confusion_matrices.png'),\n",
        "                    dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "    def calculate_learning_curves(self, video_data: Dict, seeds: List[int],\n",
        "                            model_type: str, params_by_seed: Dict,\n",
        "                            train_sizes: np.ndarray = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Calculate learning curves for temporal voting across multiple seeds\n",
        "\n",
        "        Args:\n",
        "            video_data: Dictionary of video data\n",
        "            seeds: List of random seeds\n",
        "            model_type: Type of model ('SVM' or 'LogisticRegression')\n",
        "            params_by_seed: Dictionary of best parameters for each seed\n",
        "            train_sizes: Array of training set proportions (0 to 1)\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing learning curve data\n",
        "        \"\"\"\n",
        "        if train_sizes is None:\n",
        "            train_sizes = np.linspace(0.05, 1.0, 15)\n",
        "\n",
        "        scores_by_size = {size: [] for size in train_sizes}\n",
        "\n",
        "        # Get central frame data for initial training\n",
        "        X, y = self.prepare_central_frame_data(video_data)\n",
        "\n",
        "        for seed in seeds:\n",
        "            logging.info(f\"Calculating learning curve for seed {seed}\")\n",
        "\n",
        "            # Split data consistently for this seed\n",
        "            train_videos, test_videos = self.prepare_data_split(video_data, seed)\n",
        "\n",
        "            # Get indices for full training set\n",
        "            train_indices = [list(video_data.keys()).index(vid) for vid in train_videos]\n",
        "            X_train_full = X[train_indices]\n",
        "            y_train_full = self.le.transform([video_data[vid]['label'] for vid in train_videos])\n",
        "\n",
        "            # For each training set size\n",
        "            for train_size in train_sizes:\n",
        "                # Calculate number of samples for this size\n",
        "                n_samples = int(len(train_videos) * train_size)\n",
        "\n",
        "                # Get indices for each class while preserving order\n",
        "                subset_indices = []\n",
        "                labels_array = np.array(y_train_full)\n",
        "                unique_labels = np.unique(labels_array)\n",
        "\n",
        "                # Calculate target samples per class\n",
        "                total_per_class = {label: np.sum(labels_array == label) for label in unique_labels}\n",
        "                target_per_class = {\n",
        "                    label: int(n_samples * (count / len(labels_array)))\n",
        "                    for label, count in total_per_class.items()\n",
        "                }\n",
        "\n",
        "                # Adjust for rounding errors to match n_samples exactly\n",
        "                remaining = n_samples - sum(target_per_class.values())\n",
        "                if remaining > 0:\n",
        "                    # Add remaining samples to classes proportionally\n",
        "                    for label in sorted(unique_labels,\n",
        "                                    key=lambda x: total_per_class[x],\n",
        "                                    reverse=True):\n",
        "                        if remaining <= 0:\n",
        "                            break\n",
        "                        target_per_class[label] += 1\n",
        "                        remaining -= 1\n",
        "\n",
        "                # Get stratified indices\n",
        "                for label in unique_labels:\n",
        "                    label_indices = np.where(labels_array == label)[0]\n",
        "                    n_label_samples = target_per_class[label]\n",
        "                    subset_indices.extend(label_indices[:n_label_samples])\n",
        "\n",
        "                # Sort indices to maintain order\n",
        "                subset_indices = np.array(sorted(subset_indices))\n",
        "\n",
        "                # Take subset of training data\n",
        "                X_train_subset = X_train_full[subset_indices]\n",
        "                y_train_subset = y_train_full[subset_indices]\n",
        "\n",
        "                # Train model\n",
        "                model = self.train_model(\n",
        "                    model_type,\n",
        "                    params_by_seed[seed],\n",
        "                    X_train_subset,\n",
        "                    y_train_subset\n",
        "                )\n",
        "\n",
        "                # Evaluate on test set using temporal voting\n",
        "                metrics = self.evaluate_temporal_voting(model, video_data, test_videos)\n",
        "                scores_by_size[train_size].append(metrics['macro_f1'])\n",
        "\n",
        "            # Calculate mean and std for each training size\n",
        "            learning_curve_data = {\n",
        "                'train_sizes': train_sizes * len(y_train_full),\n",
        "                'test_scores_mean': [np.mean(scores_by_size[size]) for size in train_sizes],\n",
        "                'test_scores_std': [np.std(scores_by_size[size]) for size in train_sizes]\n",
        "            }\n",
        "\n",
        "        return learning_curve_data\n",
        "\n",
        "\n",
        "    def plot_learning_curves(self, learning_curves: Dict):\n",
        "        \"\"\"\n",
        "        Plot learning curves for both models\n",
        "\n",
        "        Args:\n",
        "            learning_curves: Dictionary containing learning curve data for both models\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        ax = plt.gca()\n",
        "\n",
        "        colors = {\n",
        "            'SVM': 'blue',\n",
        "            'LogisticRegression': 'red'\n",
        "        }\n",
        "\n",
        "        for model_name in ['SVM', 'LogisticRegression']:\n",
        "            data = learning_curves[model_name]\n",
        "            train_sizes = data['train_sizes']\n",
        "            test_scores_mean = data['test_scores_mean']\n",
        "            test_scores_std = data['test_scores_std']\n",
        "\n",
        "            # Plot mean test scores\n",
        "            plt.plot(train_sizes, test_scores_mean,\n",
        "                    label=f'{model_name}',\n",
        "                    color=colors[model_name],\n",
        "                    linewidth=2)\n",
        "\n",
        "            # Plot standard deviation bands\n",
        "            plt.fill_between(train_sizes,\n",
        "                            np.array(test_scores_mean) - np.array(test_scores_std),\n",
        "                            np.array(test_scores_mean) + np.array(test_scores_std),\n",
        "                            alpha=0.1, color=colors[model_name])\n",
        "\n",
        "        ax.spines['top'].set_visible(False)\n",
        "        ax.spines['right'].set_visible(False)\n",
        "        ax.spines['bottom'].set_visible(False)\n",
        "        ax.spines['left'].set_visible(False)\n",
        "\n",
        "        plt.xlabel('Training Set Size')\n",
        "        plt.ylabel('Macro F1 Score')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend(loc='lower right')\n",
        "\n",
        "        # Add total samples info\n",
        "        total_samples = int(np.max(train_sizes))\n",
        "        plt.text(0.02, 0.98, f'Total videos: {total_samples}',\n",
        "                transform=plt.gca().transAxes,\n",
        "                bbox=dict(facecolor='white', alpha=0.8),\n",
        "                verticalalignment='top')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, 'learning_curves.png'),\n",
        "                    dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def save_learning_curves_data(self, learning_curves: Dict):\n",
        "        \"\"\"\n",
        "        Save learning curves data to a JSON file for later plotting\n",
        "\n",
        "        Args:\n",
        "            learning_curves: Dictionary containing learning curve data for both models\n",
        "        \"\"\"\n",
        "        curves_data = {}\n",
        "\n",
        "        for model_name in learning_curves:\n",
        "            data = learning_curves[model_name]\n",
        "            curves_data[model_name] = {\n",
        "                'train_sizes': data['train_sizes'].tolist(),  # Convert numpy array to list for JSON serialization\n",
        "                'test_scores_mean': data['test_scores_mean'],\n",
        "                'test_scores_std': data['test_scores_std']\n",
        "            }\n",
        "\n",
        "        output_file = os.path.join(self.output_dir, 'learning_curves_data.json')\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(curves_data, f, indent=2, cls=NumpyEncoder)  # Using NumpyEncoder for numpy arrays\n",
        "\n",
        "def run_temporal_voting_evaluation(data_dir: str, random_search_dir: str) -> Dict:\n",
        "    \"\"\"Run complete temporal voting evaluation pipeline\"\"\"\n",
        "    try:\n",
        "        # Initialize evaluator\n",
        "        evaluator = TemporalVotingEvaluator()\n",
        "\n",
        "        # Load video data\n",
        "        logging.info(\"Loading video data...\")\n",
        "        video_data, unique_labels = evaluator.load_video_features(data_dir)\n",
        "        evaluator.le.fit(unique_labels)\n",
        "\n",
        "        # Load random seeds information\n",
        "        with open(os.path.join(random_search_dir, 'random_seeds.json'), 'r') as f:\n",
        "            seeds_info = json.load(f)\n",
        "            seeds = seeds_info['generated_seeds']\n",
        "\n",
        "        logging.info(f\"Found {len(seeds)} seeds for evaluation\")\n",
        "\n",
        "        # Prepare central frame data for initial model training\n",
        "        X, y = evaluator.prepare_central_frame_data(video_data)\n",
        "\n",
        "        # Initialize results containers\n",
        "        results = {\n",
        "            'SVM': {},\n",
        "            'LogisticRegression': {}\n",
        "        }\n",
        "        learning_curves = {}\n",
        "\n",
        "        # Store parameters by seed for learning curves\n",
        "        params_by_seed = {\n",
        "            'SVM': {},\n",
        "            'LogisticRegression': {}\n",
        "        }\n",
        "\n",
        "        # First pass: Model evaluation\n",
        "        for model_name in ['SVM', 'LogisticRegression']:\n",
        "            logging.info(f\"\\nEvaluating {model_name} across {len(seeds)} seeds\")\n",
        "\n",
        "            for seed in seeds:\n",
        "                logging.info(f\"\\nProcessing {model_name} with seed {seed}\")\n",
        "\n",
        "                try:\n",
        "                    # Load best parameters for this seed\n",
        "                    metrics_file = os.path.join(random_search_dir, f'seed_{seed}',\n",
        "                                              f'{model_name}_metrics.json')\n",
        "                    with open(metrics_file, 'r') as f:\n",
        "                        seed_results = json.load(f)\n",
        "                        params = seed_results['best_params']\n",
        "                        params_by_seed[model_name][seed] = params\n",
        "\n",
        "                    # Split data using stratified split\n",
        "                    train_videos, test_videos = evaluator.prepare_data_split(video_data, seed)\n",
        "\n",
        "                    # Get training data\n",
        "                    train_indices = [list(video_data.keys()).index(vid) for vid in train_videos]\n",
        "                    X_train = X[train_indices]\n",
        "                    y_train = evaluator.le.transform([video_data[vid]['label']\n",
        "                                                    for vid in train_videos])\n",
        "\n",
        "                    # Train model on central frames\n",
        "                    model = evaluator.train_model(model_name, params, X_train, y_train)\n",
        "\n",
        "                    # Evaluate using temporal voting\n",
        "                    metrics = evaluator.evaluate_temporal_voting(\n",
        "                        model, video_data, test_videos\n",
        "                    )\n",
        "\n",
        "                    # Store results for this seed\n",
        "                    results[model_name][seed] = metrics\n",
        "\n",
        "                    # Log seed-specific results\n",
        "                    logging.info(f\"Results for {model_name} (seed {seed}):\")\n",
        "                    logging.info(f\"Weighted F1: {metrics['weighted_f1']:.4f}\")\n",
        "                    logging.info(f\"Macro F1: {metrics['macro_f1']:.4f}\")\n",
        "                    logging.info(\"Per-class F1 scores:\")\n",
        "                    for class_name in evaluator.le.classes_:\n",
        "                        logging.info(f\"  {class_name}: {metrics[f'{class_name}_f1']:.4f}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"Error processing seed {seed} for {model_name}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            # Calculate average metrics\n",
        "            seed_metrics = [metrics for metrics in results[model_name].values()\n",
        "                          if isinstance(metrics, dict)]\n",
        "\n",
        "            if seed_metrics:\n",
        "                avg_metrics = {\n",
        "                    'weighted_f1': np.mean([m['weighted_f1'] for m in seed_metrics]),\n",
        "                    'weighted_f1_std': np.std([m['weighted_f1'] for m in seed_metrics]),\n",
        "                    'macro_f1': np.mean([m['macro_f1'] for m in seed_metrics]),\n",
        "                    'macro_f1_std': np.std([m['macro_f1'] for m in seed_metrics]),\n",
        "                }\n",
        "\n",
        "                # Add per-class average metrics\n",
        "                for class_name in evaluator.le.classes_:\n",
        "                    avg_metrics[f'{class_name}_f1'] = np.mean([\n",
        "                        m[f'{class_name}_f1'] for m in seed_metrics\n",
        "                    ])\n",
        "                    avg_metrics[f'{class_name}_f1_std'] = np.std([\n",
        "                        m[f'{class_name}_f1'] for m in seed_metrics\n",
        "                    ])\n",
        "\n",
        "                results[model_name]['average_metrics'] = avg_metrics\n",
        "\n",
        "                # Log average results\n",
        "                logging.info(f\"\\nAverage metrics for {model_name} across \"\n",
        "                           f\"{len(seed_metrics)} seeds:\")\n",
        "                logging.info(f\"Weighted F1: {avg_metrics['weighted_f1']:.4f} ± \"\n",
        "                           f\"{avg_metrics['weighted_f1_std']:.4f}\")\n",
        "                logging.info(f\"Macro F1: {avg_metrics['macro_f1']:.4f} ± \"\n",
        "                           f\"{avg_metrics['macro_f1_std']:.4f}\")\n",
        "                logging.info(\"Per-class F1 scores:\")\n",
        "                for class_name in evaluator.le.classes_:\n",
        "                    logging.info(f\"  {class_name}: {avg_metrics[f'{class_name}_f1']:.4f} ± \"\n",
        "                               f\"{avg_metrics[f'{class_name}_f1_std']:.4f}\")\n",
        "\n",
        "        # Create averaged confusion matrices plot\n",
        "        evaluator.plot_averaged_confusion_matrices(results)\n",
        "\n",
        "        # Calculate and plot learning curves\n",
        "        learning_curves = {}\n",
        "        for model_name in ['SVM', 'LogisticRegression']:\n",
        "            if params_by_seed[model_name]:  # Only if we have parameters for this model\n",
        "                learning_curves[model_name] = evaluator.calculate_learning_curves(\n",
        "                    video_data,\n",
        "                    seeds,\n",
        "                    model_name,\n",
        "                    params_by_seed[model_name]\n",
        "                )\n",
        "\n",
        "        if learning_curves:\n",
        "            evaluator.save_learning_curves_data(learning_curves)\n",
        "            evaluator.plot_learning_curves(learning_curves)\n",
        "\n",
        "        # Save all results\n",
        "        output_files = {\n",
        "            'temporal_voting_results.json': results,\n",
        "            'learning_curves.json': learning_curves\n",
        "        }\n",
        "\n",
        "        for filename, data in output_files.items():\n",
        "            filepath = os.path.join(evaluator.output_dir, filename)\n",
        "            with open(filepath, 'w') as f:\n",
        "                json.dump(data, f, cls=NumpyEncoder, indent=4)\n",
        "\n",
        "        logging.info(f\"All results saved to {evaluator.output_dir}\")\n",
        "\n",
        "        return {\n",
        "            'evaluation_results': results,\n",
        "            'learning_curves': learning_curves\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in temporal voting evaluation: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Directory containing your NPZ files\n",
        "    data_dir = \"/path/to/Feature Extraction/ViT-SO400M-14-SigLIP\"\n",
        "\n",
        "    # Directory containing random search results\n",
        "    grid_search_dir = \"/path/to/Single Frame/model_optimization_20250206_115650_multiseed\"\n",
        "\n",
        "    # Run evaluation\n",
        "    results = run_temporal_voting_evaluation(data_dir, grid_search_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_tWXG3Wacq-",
        "outputId": "644ee8c0-a958-494e-a33b-a03f16e92931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vqvEyQYTdTtA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}