{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joaosMart/fish-species-class-siglip/blob/update-readme-comprehensive/Code/species-classification/Temporal_Pooling_Training_and_validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Temporal Pooling Fish Species Classification\n",
        "\n",
        "This notebook implements the temporal pooling approach for fish species classification as described in \"Temporal Aggregation of Vision-Language Features for High-Accuracy Fish Classification in Automated Monitoring\".\n",
        "\n",
        "Includes implementation of temporal pooling strategy with random hyperparameter search for SVM and Logistic Regression, including cross-validation and seed analysis\n",
        "\n",
        "The approach uses SigLIP vision-language model features from the middle frame of video segments to classify three salmonid species: Trout, Salmon, and Arctic Char.\n"
      ],
      "metadata": {
        "id": "EhQdC1syGqmQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wagA3ZOAA6ds"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRNu17MnA5Ll"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, learning_curve\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import balanced_accuracy_score, f1_score, precision_recall_fscore_support\n",
        "from sklearn.metrics import confusion_matrix, make_scorer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy.stats import loguniform\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from typing import Dict, Tuple, List\n",
        "import json\n",
        "import os\n",
        "import logging\n",
        "import glob\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import balanced_accuracy_score, f1_score, precision_recall_fscore_support\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import seaborn as sns\n",
        "from typing import Dict, Tuple, List\n",
        "import logging\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader"
      ],
      "metadata": {
        "id": "X2GNynVfENXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader:\n",
        "    \"\"\"Handle loading and processing of NPZ files\"\"\"\n",
        "    def __init__(self, data_dir: str):\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "    def load_npz_files(self) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Load all NPZ files from directory and extract averaged_mean features and labels\n",
        "\n",
        "        Returns:\n",
        "            features: numpy array of averaged_mean features\n",
        "            labels: numpy array of fish species labels\n",
        "        \"\"\"\n",
        "        features_list = []\n",
        "        labels_list = []\n",
        "\n",
        "        # Get all NPZ files in directory\n",
        "        npz_files = glob.glob(os.path.join(self.data_dir, \"*.npz\"))\n",
        "\n",
        "        logging.info(f\"Found {len(npz_files)} NPZ files\")\n",
        "\n",
        "        for npz_file in npz_files:\n",
        "            try:\n",
        "                # Load NPZ file\n",
        "                data = np.load(npz_file, allow_pickle=True)\n",
        "\n",
        "                # Extract averaged_mean feature and label\n",
        "                frame_features = data['averaged_features']\n",
        "                fish_species = str(data['fish_species'].item())  # Convert to string\n",
        "\n",
        "                if frame_features is not None:\n",
        "                    features_list.append(frame_features)\n",
        "                    labels_list.append(fish_species)\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error processing file {npz_file}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Convert lists to numpy arrays\n",
        "        features_array = np.array(features_list)\n",
        "        labels_array = np.array(labels_list)\n",
        "\n",
        "        # Log data distribution\n",
        "        unique_labels, counts = np.unique(labels_array, return_counts=True)\n",
        "        for label, count in zip(unique_labels, counts):\n",
        "            percentage = (count / len(labels_array)) * 100\n",
        "            logging.info(f\"Class {label}: {count} samples ({percentage:.2f}%)\")\n",
        "\n",
        "        return features_array, labels_array\n",
        "\n",
        "\n",
        "class NumpyEncoder(json.JSONEncoder):\n",
        "    \"\"\"Custom encoder for numpy data types\"\"\"\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        elif isinstance(obj, np.bool_):\n",
        "            return bool(obj)\n",
        "        return super(NumpyEncoder, self).default(obj)"
      ],
      "metadata": {
        "id": "pGiIcbmpEO6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqFQcF9mA8Wb"
      },
      "source": [
        "# FishClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENUmWKzAAzOh"
      },
      "outputs": [],
      "source": [
        "# Custom scorer for weighted F1\n",
        "macro_f1_scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "\n",
        "class FishClassifier:\n",
        "    def __init__(self, random_state: int = 42):\n",
        "        self.random_state = random_state\n",
        "        self.le = LabelEncoder()\n",
        "        self.setup_logging()\n",
        "\n",
        "    def setup_logging(self):\n",
        "        \"\"\"Setup logging configuration\"\"\"\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            filename='fish_classifier.log'\n",
        "        )\n",
        "\n",
        "    def prepare_data(self, features: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Prepare data by splitting into train and test sets with stratification\n",
        "        \"\"\"\n",
        "        # Encode labels\n",
        "        y = self.le.fit_transform(labels)\n",
        "\n",
        "        # Create stratified train-test split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            features,\n",
        "            y,\n",
        "            test_size=0.2,\n",
        "            random_state=self.random_state,\n",
        "            stratify=y\n",
        "        )\n",
        "\n",
        "        logging.info(f\"Training set size: {X_train.shape[0]}\")\n",
        "        logging.info(f\"Test set size: {X_test.shape[0]}\")\n",
        "\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "    def create_baseline_models(self) -> Dict:\n",
        "        \"\"\"Create baseline models with default parameters\"\"\"\n",
        "        models = {\n",
        "            'svm': LinearSVC(\n",
        "                random_state=self.random_state,\n",
        "                class_weight='balanced',\n",
        "                max_iter=2000  # Increased to ensure convergence\n",
        "            ),\n",
        "            'logistic': LogisticRegression(\n",
        "                random_state=self.random_state,\n",
        "                class_weight='balanced',\n",
        "                max_iter=2000\n",
        "            )\n",
        "        }\n",
        "        return models\n",
        "\n",
        "    def evaluate_model(self, model, X: np.ndarray, y: np.ndarray, model_name: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Evaluate model performance with multiple metrics\n",
        "        \"\"\"\n",
        "        # Get predictions\n",
        "        y_pred = model.predict(X)\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = {\n",
        "            'balanced_accuracy': balanced_accuracy_score(y, y_pred),\n",
        "            'macro_f1': f1_score(y, y_pred, average='macro'),\n",
        "            'confusion_matrix': confusion_matrix(y, y_pred),\n",
        "        }\n",
        "\n",
        "        # Calculate per-class metrics\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y, y_pred)\n",
        "\n",
        "        # Add per-class metrics\n",
        "        for i, class_name in enumerate(self.le.classes_):\n",
        "            metrics[f'{class_name}_precision'] = precision[i]\n",
        "            metrics[f'{class_name}_recall'] = recall[i]\n",
        "            metrics[f'{class_name}_f1'] = f1[i]\n",
        "\n",
        "        # Log results\n",
        "        logging.info(f\"\\nResults for {model_name}:\")\n",
        "        logging.info(f\"Balanced Accuracy: {metrics['balanced_accuracy']:.4f}\")\n",
        "        logging.info(f\"Macro F1: {metrics['macro_f1']:.4f}\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def plot_confusion_matrix(self, confusion_mat: np.ndarray, model_name: str):\n",
        "        \"\"\"\n",
        "        Plot confusion matrix heatmap\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(\n",
        "            confusion_mat,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='Blues',\n",
        "            xticklabels=self.le.classes_,\n",
        "            yticklabels=self.le.classes_\n",
        "        )\n",
        "        plt.title(f'Confusion Matrix - {model_name}')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.savefig(f'confusion_matrix_{model_name}.png')\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqG4adquPYU5"
      },
      "source": [
        "# Random Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JdsBPVvPIgj"
      },
      "outputs": [],
      "source": [
        "class ModelOptimizer:\n",
        "    \"\"\"\n",
        "    A class to handle model optimization for both SVM and Logistic Regression models.\n",
        "    Uses random search with balanced class weights.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, random_state: int = 42, n_iter: int = 100, class_names: List[str] = None):\n",
        "        self.random_state = random_state\n",
        "        self.n_iter = n_iter\n",
        "        self.class_names = class_names or ['Bleikja', 'Lax', 'Urridi']\n",
        "        self.setup_output_dir()\n",
        "        self.setup_logging()\n",
        "\n",
        "    def setup_logging(self):\n",
        "        \"\"\"Configure logging settings\"\"\"\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            handlers=[\n",
        "                logging.FileHandler(os.path.join(self.output_dir, 'optimization.log')),\n",
        "                logging.StreamHandler()\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def setup_output_dir(self):\n",
        "        \"\"\"Setup directory for saving results with timestamp\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        self.output_dir = f'model_optimization_{timestamp}'\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "        logging.info(f\"Created output directory: {self.output_dir}\")\n",
        "\n",
        "    def create_param_distributions(self) -> Dict:\n",
        "        \"\"\"\n",
        "        Create parameter distributions for random search.\n",
        "        Only varies C parameter, using balanced class weights.\n",
        "        \"\"\"\n",
        "        param_distributions = {\n",
        "            'C': loguniform(1e-1, 3e2),  # Wide range for C\n",
        "            'class_weight': ['balanced', None]\n",
        "        }\n",
        "\n",
        "        logging.info(\"Created parameter distributions for random search\")\n",
        "        return param_distributions\n",
        "\n",
        "    def run_random_search(self, model_class, X_train: np.ndarray, y_train: np.ndarray,\n",
        "                         X_test: np.ndarray, y_test: np.ndarray, model_name: str) -> Tuple[RandomizedSearchCV, pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Run random search with cross-validation for model optimization.\n",
        "        \"\"\"\n",
        "        logging.info(f\"Starting random search for {model_name}\")\n",
        "\n",
        "        # Create base model with appropriate parameters\n",
        "        if model_class == LinearSVC:\n",
        "            base_model = model_class(random_state=self.random_state, max_iter=2000)\n",
        "        else:  # LogisticRegression\n",
        "            base_model = model_class(random_state=self.random_state, max_iter=2000,\n",
        "                                   solver='lbfgs', penalty='l2')\n",
        "\n",
        "        # Create parameter distributions\n",
        "        param_distributions = self.create_param_distributions()\n",
        "\n",
        "        # Setup cross-validation\n",
        "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)\n",
        "\n",
        "        # Initialize random search\n",
        "        random_search = RandomizedSearchCV(\n",
        "            estimator=base_model,\n",
        "            param_distributions=param_distributions,\n",
        "            n_iter=self.n_iter,\n",
        "            cv=cv,\n",
        "            scoring=macro_f1_scorer,\n",
        "            n_jobs=-1,\n",
        "            random_state=self.random_state,\n",
        "            verbose=2,\n",
        "            return_train_score=True\n",
        "        )\n",
        "\n",
        "        # Fit random search\n",
        "        random_search.fit(X_train, y_train)\n",
        "\n",
        "        # Create results DataFrame\n",
        "        results_df = pd.DataFrame(random_search.cv_results_)\n",
        "\n",
        "        # Save results\n",
        "        self.save_results(random_search, results_df, model_name)\n",
        "\n",
        "        logging.info(f\"Completed random search for {model_name}\")\n",
        "        return random_search, results_df\n",
        "\n",
        "    def save_results(self, random_search: RandomizedSearchCV, results_df: pd.DataFrame, model_name: str):\n",
        "        \"\"\"Save random search results to files with consistent metrics.\"\"\"\n",
        "        model_dir = os.path.join(self.output_dir, model_name)\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "        # Get best parameters and scores\n",
        "        best_params = random_search.best_params_\n",
        "        best_cv_score = random_search.best_score_\n",
        "\n",
        "        # Find the row with best CV score for consistency check\n",
        "        best_idx = results_df['mean_test_score'].idxmax()\n",
        "        best_row = results_df.loc[best_idx]\n",
        "\n",
        "        # Create comprehensive summary\n",
        "        summary = {\n",
        "            'best_parameters': best_params,\n",
        "            'cross_validation_performance': {\n",
        "                'best_score': best_cv_score,\n",
        "                'std_score': best_row['std_test_score'],\n",
        "                'train_score': best_row['mean_train_score'],\n",
        "                'train_std': best_row['std_train_score']\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Save summary as JSON\n",
        "        with open(os.path.join(model_dir, 'best_params.json'), 'w') as f:\n",
        "            json.dump(summary, f, indent=4)\n",
        "\n",
        "        # Save full results DataFrame\n",
        "        results_df.to_csv(os.path.join(model_dir, 'random_search_results.csv'))\n",
        "\n",
        "        # Save readable summary\n",
        "        with open(os.path.join(model_dir, 'performance_summary.txt'), 'w') as f:\n",
        "            f.write(\"Best Model Configuration\\n\")\n",
        "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "            f.write(\"Parameters:\\n\")\n",
        "            for param, value in best_params.items():\n",
        "                f.write(f\"{param}: {value}\\n\")\n",
        "            f.write(\"\\nCross-validation Performance:\\n\")\n",
        "            f.write(f\"Best CV Score (Macro F1): {best_cv_score:.4f} ± {best_row['std_test_score']:.4f}\\n\")\n",
        "            f.write(f\"CV Training Score: {best_row['mean_train_score']:.4f} ± {best_row['std_train_score']:.4f}\\n\")\n",
        "\n",
        "        logging.info(f\"Saved optimization results for {model_name} to {model_dir}\")\n",
        "\n",
        "\n",
        "    def evaluate_model(self, model, X: np.ndarray, y: np.ndarray, model_name: str) -> Dict:\n",
        "        \"\"\"Evaluate model performance with multiple metrics.\"\"\"\n",
        "        # Get predictions\n",
        "        y_pred = model.predict(X)\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = {\n",
        "            'weighted_f1': f1_score(y, y_pred, average='weighted'),\n",
        "            'macro_f1': f1_score(y, y_pred, average='macro'),\n",
        "            'balanced_accuracy': balanced_accuracy_score(y, y_pred),\n",
        "            'confusion_matrix': confusion_matrix(y, y_pred)\n",
        "        }\n",
        "\n",
        "        # Calculate per-class metrics\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y, y_pred)\n",
        "\n",
        "        # Add per-class metrics\n",
        "        for i, class_name in enumerate(self.class_names):\n",
        "            metrics[f'{class_name}_precision'] = precision[i]\n",
        "            metrics[f'{class_name}_recall'] = recall[i]\n",
        "            metrics[f'{class_name}_f1'] = f1[i]\n",
        "\n",
        "        # Log results\n",
        "        logging.info(f\"\\nResults for {model_name}:\")\n",
        "        logging.info(f\"Weighted F1: {metrics['weighted_f1']:.4f}\")\n",
        "        logging.info(f\"Macro F1: {metrics['macro_f1']:.4f}\")\n",
        "        logging.info(f\"Balanced Accuracy: {metrics['balanced_accuracy']:.4f}\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def plot_results(self, results_df: pd.DataFrame, model_name: str):\n",
        "        \"\"\"Create clear visualization of random search results with verified metrics.\"\"\"\n",
        "        plt.figure(figsize=(15, 12))\n",
        "\n",
        "        # Sort results by C parameter for smooth plotting\n",
        "        results_df = results_df.sort_values('param_C')\n",
        "\n",
        "        # Plot 1: Main Performance Plot\n",
        "        plt.subplot(211)\n",
        "\n",
        "        # Plot mean CV scores with error bands\n",
        "        plt.semilogx(results_df['param_C'],\n",
        "                     results_df['mean_test_score'],\n",
        "                     'b-',\n",
        "                     label='Cross-validation Score',\n",
        "                     linewidth=2)\n",
        "        plt.fill_between(results_df['param_C'],\n",
        "                        results_df['mean_test_score'] - results_df['std_test_score'],\n",
        "                        results_df['mean_test_score'] + results_df['std_test_score'],\n",
        "                        alpha=0.2,\n",
        "                        color='b')\n",
        "\n",
        "        plt.semilogx(results_df['param_C'],\n",
        "                     results_df['mean_train_score'],\n",
        "                     'r-',\n",
        "                     label='Training Score',\n",
        "                     linewidth=2)\n",
        "        plt.fill_between(results_df['param_C'],\n",
        "                        results_df['mean_train_score'] - results_df['std_train_score'],\n",
        "                        results_df['mean_train_score'] + results_df['std_train_score'],\n",
        "                        alpha=0.2,\n",
        "                        color='r')\n",
        "\n",
        "        # Highlight best performing point\n",
        "        best_idx = results_df['mean_test_score'].idxmax()\n",
        "        best_C = results_df.loc[best_idx, 'param_C']\n",
        "        best_score = results_df.loc[best_idx, 'mean_test_score']\n",
        "        best_score_std = results_df.loc[best_idx, 'std_test_score']\n",
        "\n",
        "        plt.plot(best_C, best_score, 'k*', markersize=15,\n",
        "                label=f'Best C = {best_C:.2e}')\n",
        "\n",
        "        plt.xlabel('C Parameter (log scale)')\n",
        "        plt.ylabel('Macro F1 Score')\n",
        "        plt.title(f'{model_name}: Impact of C Parameter on Model Performance')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Plot 2: Overfitting Analysis\n",
        "        plt.subplot(212)\n",
        "\n",
        "        # Calculate train-test gap\n",
        "        train_test_gap = results_df['mean_train_score'] - results_df['mean_test_score']\n",
        "\n",
        "        plt.semilogx(results_df['param_C'], train_test_gap, 'g-',\n",
        "                     label='Train-CV Gap', linewidth=2)\n",
        "        plt.fill_between(results_df['param_C'],\n",
        "                        train_test_gap - results_df['std_test_score'],\n",
        "                        train_test_gap + results_df['std_test_score'],\n",
        "                        alpha=0.2,\n",
        "                        color='g')\n",
        "\n",
        "        plt.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
        "        plt.xlabel('C Parameter (log scale)')\n",
        "        plt.ylabel('Train-CV Score Gap')\n",
        "        plt.title('Overfitting Analysis: Train-CV Score Gap vs C')\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Add text box with verified metrics\n",
        "        textstr = '\\n'.join([\n",
        "            f'Best Configuration:',\n",
        "            f'C = {best_C:.2e}',\n",
        "            f'CV Score = {best_score:.4f} ± {best_score_std:.4f}',\n",
        "            f'Train Score = {results_df.loc[best_idx, \"mean_train_score\"]:.4f}'\n",
        "        ])\n",
        "\n",
        "        plt.text(0.02, 0.98, textstr,\n",
        "                transform=plt.gca().transAxes,\n",
        "                bbox=dict(facecolor='white', alpha=0.8),\n",
        "                verticalalignment='top')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfRBAElPd7_A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, learning_curve\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import balanced_accuracy_score, f1_score, precision_recall_fscore_support\n",
        "from sklearn.metrics import confusion_matrix, make_scorer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy.stats import loguniform\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from typing import Dict, Tuple, List\n",
        "import json\n",
        "import os\n",
        "import logging\n",
        "import glob\n",
        "from copy import deepcopy\n",
        "\n",
        "\n",
        "class MultiSeedModelOptimizer(ModelOptimizer):\n",
        "    \"\"\"Extended ModelOptimizer class to handle multiple random seeds\"\"\"\n",
        "\n",
        "    def __init__(self, base_seed: int = 42, n_seeds: int = 10, n_iter: int = 30,\n",
        "                 class_names: List[str] = None):\n",
        "        # Generate random seeds\n",
        "        rng = np.random.RandomState(base_seed)\n",
        "        self.seeds = np.array([4, 15, 29, 30, 32, 37, 38, 65, 88, 91])\n",
        "        self.n_seeds = n_seeds\n",
        "\n",
        "        # Initialize with first seed\n",
        "        super().__init__(random_state=self.seeds[0], n_iter=n_iter,\n",
        "                        class_names=class_names)\n",
        "\n",
        "        # Modify output directory to indicate multiple seeds\n",
        "        self.output_dir = f'{self.output_dir}_multiseed'\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "        # Save seeds information\n",
        "        with open(os.path.join(self.output_dir, 'random_seeds.json'), 'w') as f:\n",
        "            json.dump({'base_seed': base_seed, 'generated_seeds': self.seeds.tolist()}, f)\n",
        "\n",
        "    def run_multi_seed_optimization(self, model_class, X_train: np.ndarray,\n",
        "                                  y_train: np.ndarray, X_test: np.ndarray,\n",
        "                                  y_test: np.ndarray, model_name: str) -> Dict:\n",
        "        \"\"\"Run random search optimization across multiple seeds\"\"\"\n",
        "        all_seed_results = {}\n",
        "\n",
        "        for seed_idx, seed in enumerate(self.seeds):\n",
        "            logging.info(f\"\\nRunning optimization for {model_name} with seed {seed} \"\n",
        "                        f\"({seed_idx + 1}/{self.n_seeds})\")\n",
        "\n",
        "            # Update random state\n",
        "            self.random_state = seed\n",
        "\n",
        "            # Use existing seed directory if it exists, create if it doesn't\n",
        "            seed_dir = os.path.join(self.output_dir, f'seed_{seed}')\n",
        "            os.makedirs(seed_dir, exist_ok=True)\n",
        "\n",
        "            try:\n",
        "                # Run random search for this seed\n",
        "                random_search, results_df = self.run_random_search(\n",
        "                    model_class, X_train, y_train, X_test, y_test,\n",
        "                    model_name  # Removed the seed suffix from model name\n",
        "                )\n",
        "\n",
        "                # Get best model for this seed\n",
        "                best_model = random_search.best_estimator_\n",
        "\n",
        "                # Evaluate best model\n",
        "                train_metrics = self.evaluate_model(\n",
        "                    best_model, X_train, y_train, f\"{model_name}_train\"\n",
        "                )\n",
        "                test_metrics = self.evaluate_model(\n",
        "                    best_model, X_test, y_test, f\"{model_name}_test\"\n",
        "                )\n",
        "\n",
        "                # Store results for this seed\n",
        "                if seed not in all_seed_results:\n",
        "                    all_seed_results[seed] = {}\n",
        "\n",
        "                all_seed_results[seed][model_name] = {\n",
        "                    'best_model': best_model,\n",
        "                    'best_params': random_search.best_params_,\n",
        "                    'cv_score': random_search.best_score_,\n",
        "                    'train_metrics': train_metrics,\n",
        "                    'test_metrics': test_metrics,\n",
        "                    'results_df': results_df\n",
        "                }\n",
        "\n",
        "                # Save results for this model within the seed directory\n",
        "                self.save_seed_results(\n",
        "                    all_seed_results[seed][model_name],\n",
        "                    seed_dir,\n",
        "                    model_name\n",
        "                )\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error during {model_name} optimization with seed {seed}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Generate and save summary across seeds\n",
        "        self.generate_seed_summary(all_seed_results, model_name)\n",
        "\n",
        "        return all_seed_results\n",
        "\n",
        "    def save_seed_results(self, results: Dict, seed_dir: str, model_name: str):\n",
        "        \"\"\"Save results for a specific model within a seed directory\"\"\"\n",
        "        # Save metrics\n",
        "        metrics_summary = {\n",
        "            'best_params': results['best_params'],\n",
        "            'cv_score': results['cv_score'],\n",
        "            'train_metrics': results['train_metrics'],\n",
        "            'test_metrics': results['test_metrics']\n",
        "        }\n",
        "\n",
        "        # Save within the seed directory with model-specific names\n",
        "        with open(os.path.join(seed_dir, f'{model_name}_metrics.json'), 'w') as f:\n",
        "            json.dump(metrics_summary, f, indent=4, cls=NumpyEncoder)\n",
        "\n",
        "        # Save results DataFrame\n",
        "        results['results_df'].to_csv(\n",
        "            os.path.join(seed_dir, f'{model_name}_results.csv')\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def generate_seed_summary(self, all_results: Dict, model_name: str):\n",
        "        \"\"\"Generate summary statistics across all seeds\"\"\"\n",
        "        # Collect metrics across seeds\n",
        "        cv_scores = []\n",
        "        test_scores = []\n",
        "        c_values = []\n",
        "\n",
        "        # Access the correct level of the dictionary\n",
        "        for seed_results in all_results.values():\n",
        "            model_results = seed_results[model_name]  # Get model-specific results\n",
        "            cv_scores.append(model_results['cv_score'])\n",
        "            test_scores.append(model_results['test_metrics']['weighted_f1'])\n",
        "            c_values.append(model_results['best_params']['C'])\n",
        "\n",
        "        # Calculate summary statistics\n",
        "        summary = {\n",
        "            'cv_score': {\n",
        "                'mean': np.mean(cv_scores),\n",
        "                'std': np.std(cv_scores),\n",
        "                'min': np.min(cv_scores),\n",
        "                'max': np.max(cv_scores)\n",
        "            },\n",
        "            'test_score': {\n",
        "                'mean': np.mean(test_scores),\n",
        "                'std': np.std(test_scores),\n",
        "                'min': np.min(test_scores),\n",
        "                'max': np.max(test_scores)\n",
        "            },\n",
        "            'c_value': {\n",
        "                'mean': np.mean(c_values),\n",
        "                'std': np.std(c_values),\n",
        "                'min': np.min(c_values),\n",
        "                'max': np.max(c_values)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Save summary\n",
        "        with open(os.path.join(self.output_dir, f'{model_name}_seed_summary.json'), 'w') as f:\n",
        "            json.dump(summary, f, indent=4)\n",
        "\n",
        "        # Create visualization of results across seeds\n",
        "        self.plot_seed_comparison(cv_scores, test_scores, c_values, model_name)\n",
        "\n",
        "    def plot_seed_comparison(self, cv_scores: List[float], test_scores: List[float],\n",
        "                           c_values: List[float], model_name: str):\n",
        "        \"\"\"Create visualization comparing results across seeds\"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "        # Plot 1: CV vs Test Scores\n",
        "        ax1.scatter(cv_scores, test_scores, alpha=0.6)\n",
        "        ax1.plot([min(cv_scores), max(cv_scores)], [min(cv_scores), max(cv_scores)],\n",
        "                 'k--', alpha=0.5)\n",
        "        ax1.set_xlabel('CV Score')\n",
        "        ax1.set_ylabel('Test Score')\n",
        "        ax1.set_title('CV vs Test Score Comparison')\n",
        "\n",
        "        # Plot 2: C Value Distribution\n",
        "        ax2.hist(np.log10(c_values), bins=10)\n",
        "        ax2.set_xlabel('log10(C)')\n",
        "        ax2.set_ylabel('Count')\n",
        "        ax2.set_title('Distribution of Best C Values')\n",
        "\n",
        "        plt.suptitle(f'{model_name}: Results Across {self.n_seeds} Seeds')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, f'{model_name}_seed_comparison.png'))\n",
        "        plt.close()\n",
        "\n",
        "def run_multi_seed_optimization(data_dir: str, class_names: List[str] = None,\n",
        "                              base_seed: int = 42, n_seeds: int = 10) -> Dict:\n",
        "    \"\"\"Run complete random search optimization pipeline across multiple seeds\"\"\"\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
        "    )\n",
        "\n",
        "    logging.info(\"Starting multi-seed optimization pipeline\")\n",
        "\n",
        "    try:\n",
        "        # Load and prepare data\n",
        "        data_loader = DataLoader(data_dir)\n",
        "        features, labels = data_loader.load_npz_files()\n",
        "\n",
        "        # Get unique class names if not provided\n",
        "        if class_names is None:\n",
        "            class_names = np.unique(labels).tolist()\n",
        "\n",
        "        # Initialize classifier for data preparation\n",
        "        classifier = FishClassifier()\n",
        "        X_train, X_test, y_train, y_test = classifier.prepare_data(features, labels)\n",
        "\n",
        "        # Initialize multi-seed optimizer\n",
        "        optimizer = MultiSeedModelOptimizer(\n",
        "            base_seed=base_seed,\n",
        "            n_seeds=n_seeds,\n",
        "            n_iter=30,\n",
        "            class_names=class_names\n",
        "        )\n",
        "\n",
        "        # Dictionary to store results\n",
        "        all_results = {}\n",
        "\n",
        "        # Run optimization for both models\n",
        "        models = {\n",
        "            'SVM': LinearSVC,\n",
        "            'LogisticRegression': LogisticRegression\n",
        "        }\n",
        "\n",
        "        for model_name, model_class in models.items():\n",
        "            logging.info(f\"\\nStarting multi-seed optimization for {model_name}\")\n",
        "\n",
        "            # Run multi-seed optimization\n",
        "            model_results = optimizer.run_multi_seed_optimization(\n",
        "                model_class, X_train, y_train, X_test, y_test, model_name\n",
        "            )\n",
        "\n",
        "            all_results[model_name] = model_results\n",
        "\n",
        "        # Generate averaged validation curves plot\n",
        "        logging.info(\"Generating averaged validation curves across seeds...\")\n",
        "        optimizer.plot_averaged_validation_curves(all_results)\n",
        "\n",
        "        # Save and zip results\n",
        "        output_dir = optimizer.output_dir\n",
        "        os.system(f'zip -r {output_dir}.zip {output_dir}')\n",
        "        logging.info(f\"\\nResults saved to {output_dir}.zip\")\n",
        "\n",
        "        return all_results\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in optimization pipeline: {str(e)}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Directory settings\n",
        "data_dir = \"path/to/Feature Extraction/ViT-SO400M-14-SigLIP\"\n",
        "class_names = ['Bleikja', 'Lax', 'Urridi']\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Run the optimization\n",
        "try:\n",
        "    print(\"Starting multi-seed optimization...\")\n",
        "\n",
        "    results = run_multi_seed_optimization(\n",
        "        data_dir=data_dir,\n",
        "        class_names=class_names,\n",
        "        base_seed=42,\n",
        "        n_seeds=10\n",
        "    )\n",
        "\n",
        "    print(\"\\nOptimization completed successfully!\")\n",
        "\n",
        "    # Print summary of results\n",
        "    for model_name in ['SVM', 'LogisticRegression']:\n",
        "        print(f\"\\nSummary for {model_name}:\")\n",
        "        summary_file = f\"model_optimization_multiseed/{model_name}_seed_summary.json\"\n",
        "\n",
        "        if os.path.exists(summary_file):\n",
        "            with open(summary_file, 'r') as f:\n",
        "                summary = json.load(f)\n",
        "\n",
        "            print(\"\\nCV Scores:\")\n",
        "            print(f\"Mean: {summary['cv_score']['mean']:.4f} ± {summary['cv_score']['std']:.4f}\")\n",
        "            print(f\"Range: [{summary['cv_score']['min']:.4f}, {summary['cv_score']['max']:.4f}]\")\n",
        "\n",
        "            print(\"\\nTest Scores:\")\n",
        "            print(f\"Mean: {summary['test_score']['mean']:.4f} ± {summary['test_score']['std']:.4f}\")\n",
        "            print(f\"Range: [{summary['test_score']['min']:.4f}, {summary['test_score']['max']:.4f}]\")\n",
        "\n",
        "            print(\"\\nC Values:\")\n",
        "            print(f\"Mean: {summary['c_value']['mean']:.4f} ± {summary['c_value']['std']:.4f}\")\n",
        "            print(f\"Range: [{summary['c_value']['min']:.4f}, {summary['c_value']['max']:.4f}]\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during optimization: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Display generated plots\n",
        "try:\n",
        "    output_dir = \"path/to/Temporal Pooling/model_optimization_multiseed\"\n",
        "\n",
        "    # Display validation curves\n",
        "    validation_curves_path = os.path.join(output_dir, 'averaged_validation_curves.png')\n",
        "    if os.path.exists(validation_curves_path):\n",
        "        img = plt.imread(validation_curves_path)\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title('Averaged Validation Curves')\n",
        "        plt.show()\n",
        "\n",
        "    # Display seed comparison plots\n",
        "    for model_name in ['SVM', 'LogisticRegression']:\n",
        "        comparison_plot_path = os.path.join(output_dir, f'{model_name}_seed_comparison.png')\n",
        "        if os.path.exists(comparison_plot_path):\n",
        "            img = plt.imread(comparison_plot_path)\n",
        "            plt.figure(figsize=(15, 6))\n",
        "            plt.imshow(img)\n",
        "            plt.axis('off')\n",
        "            plt.title(f'{model_name} Seed Comparison')\n",
        "            plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error displaying results: {str(e)}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "myMxEXrRFOt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "def zip_folder(folder_path, output_zip_path):\n",
        "    \"\"\"\n",
        "    Create a zip file from a folder in Google Colab.\n",
        "\n",
        "    Args:\n",
        "        folder_path (str): Path to the folder you want to zip\n",
        "        output_zip_path (str): Path where you want to save the zip file\n",
        "    \"\"\"\n",
        "    # Make sure the folder exists\n",
        "    if not os.path.exists(folder_path):\n",
        "        raise ValueError(f\"Folder {folder_path} does not exist\")\n",
        "\n",
        "    # Create the zip file\n",
        "    shutil.make_archive(\n",
        "        base_name=output_zip_path.replace('.zip', ''),\n",
        "        format='zip',\n",
        "        root_dir=os.path.dirname(folder_path),\n",
        "        base_dir=os.path.basename(folder_path)\n",
        "    )"
      ],
      "metadata": {
        "id": "rFKNWCHdZ1Aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "folder_to_zip = '/path/to/model_optimization_20250202_115215_multiseed'  # Path to your folder\n",
        "output_zip = '/path/to/model_optimization_20250202_115215_multiseed.zip'  # Where to save the zip file\n",
        "\n",
        "zip_folder(folder_to_zip, output_zip)"
      ],
      "metadata": {
        "id": "9E8saEXNZ3br"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1WDx938w8lvM5FzYlMSXj3DPgI9ELK2zx",
      "authorship_tag": "ABX9TyM1g15Zp2O4kNYtI1CH2Olg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}