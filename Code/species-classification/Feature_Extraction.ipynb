{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOZJ8R+S193kf64pOSKw8ma",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joaosMart/fish-species-class-siglip/blob/update-readme-comprehensive/Code/species-classification/Feature_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction for Fish Species Classification\n",
        "\n",
        "From: \"Temporal Aggregation of Vision-Language Features for High-Accuracy Fish Classification in Automated Monitoring\"\n",
        "\n",
        "This notebook implements the feature extraction phase of our fish classification pipeline, extracting SigLIP vision-language features from selected video frames for temporal aggregation and species classification.\n",
        "\n",
        "## REQUIRED INPUTS:\n",
        "- filtered_data_class.json: JSON file containing video metadata with selected frames from detection phase\n",
        "```\n",
        "  Structure: {\n",
        "    video_path: {\n",
        "        selected_frames: [list],\n",
        "        middle_frame: int,\n",
        "        fish_species: str,\n",
        "        mean_probability: float\n",
        "        }\n",
        "}\n",
        "```\n",
        "\n",
        "## OUTPUTS:\n",
        "- SigLIP Features: NPZ files containing normalized feature vectors for each video\n",
        "  - features: dict mapping frame_number -> feature_vector\n",
        "  - frame_numbers: list of selected frame numbers\n",
        "  - middle_frame: central frame number for single-frame baseline\n",
        "  - fish_species: ground truth species label\n",
        "- ResNet-50 Features: NPZ files following the same format containing ResNet-50 features for comparison for central frames only\n",
        "- Extracted Frames: JPEG images of middle frames for ResNet-50 processing. This is not required for the model. It is only used for obtaining the features for the ResNET model\n"
      ],
      "metadata": {
        "id": "lB2ZBKccwOoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install transformers open_clip_torch decord --quiet"
      ],
      "metadata": {
        "id": "1VuedE9r7XA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import open_clip\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from decord import VideoReader\n",
        "from decord import cpu, gpu\n",
        "import decord\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import cv2\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "import concurrent.futures\n",
        "import multiprocessing\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "HSSjKOuA7UMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading and Configuration\n"
      ],
      "metadata": {
        "id": "OaHuhTzF7ZPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_filtered_data(data_path):\n",
        "    \"\"\"\n",
        "    Load filtered video data from JSON file.\n",
        "\n",
        "    Args:\n",
        "        data_path (str): Path to filtered_data_class.json\n",
        "\n",
        "    Returns:\n",
        "        dict: Video metadata with selected frames\n",
        "    \"\"\"\n",
        "    with open(data_path, 'r') as file:\n",
        "        filtered_data = json.load(file)\n",
        "\n",
        "    print(f\"Loaded {len(filtered_data)} videos from {data_path}\")\n",
        "    return filtered_data\n",
        "\n",
        "# Load filtered data from detection phase\n",
        "# Update this path to your actual data location\n",
        "filtered_data_file = 'filtered_data_class.json'\n",
        "filtered_data = load_filtered_data(filtered_data_file)\n",
        "\n"
      ],
      "metadata": {
        "id": "Adp56zvh7flD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Frame Selection Strategy: Central Frame Expansion\n"
      ],
      "metadata": {
        "id": "YSsmycU47hgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_frames_by_center_expansion(video_data, window_size=11):\n",
        "    \"\"\"\n",
        "    Select frames by expanding from center frame, finding sequence with highest mean probability.\n",
        "    This implements the temporal frame selection strategy described in the paper.\n",
        "\n",
        "    Args:\n",
        "        video_data (dict): Dictionary containing frame_predictions list\n",
        "        window_size (int): Number of consecutive frames to analyze (default 11)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (selected frames list, middle frame number, sequence mean probability)\n",
        "    \"\"\"\n",
        "    # Get frame predictions sorted by frame number\n",
        "    frame_preds = sorted(\n",
        "        video_data['frame_predictions'],\n",
        "        key=lambda x: x['frame_number']\n",
        "    )\n",
        "\n",
        "    if len(frame_preds) < window_size:\n",
        "        # If we have fewer frames than window size, return all frames\n",
        "        selected_frames = [pred['frame_number'] for pred in frame_preds]\n",
        "        middle_frame = selected_frames[len(selected_frames)//2] if selected_frames else None\n",
        "        mean_prob = sum(pred['original_probability'] for pred in frame_preds) / len(frame_preds) if frame_preds else 0\n",
        "        return selected_frames, middle_frame, mean_prob\n",
        "\n",
        "    # Find center index\n",
        "    center_idx = len(frame_preds) // 2\n",
        "    half_window = window_size // 2\n",
        "\n",
        "    max_mean = float('-inf')\n",
        "    best_start_idx = 0\n",
        "\n",
        "    # Expand from center to find best sequence\n",
        "    for offset in range(len(frame_preds)):\n",
        "        # Try sequences centered around center_idx + offset and center_idx - offset\n",
        "        for center in [center_idx + offset, center_idx - offset]:\n",
        "            # Skip if we've gone too far\n",
        "            if center < half_window or center >= len(frame_preds) - half_window:\n",
        "                continue\n",
        "\n",
        "            # Get sequence centered at this position\n",
        "            start_idx = center - half_window\n",
        "            window = frame_preds[start_idx:start_idx + window_size]\n",
        "\n",
        "            current_mean = sum(pred['original_probability'] for pred in window) / window_size\n",
        "\n",
        "            if current_mean > max_mean:\n",
        "                max_mean = current_mean\n",
        "                best_start_idx = start_idx\n",
        "\n",
        "            # If checking the same position twice, skip\n",
        "            if center_idx + offset == center_idx - offset:\n",
        "                break\n",
        "\n",
        "    # Get the best sequence\n",
        "    best_sequence = frame_preds[best_start_idx:best_start_idx + window_size]\n",
        "    selected_frames = [pred['frame_number'] for pred in best_sequence]\n",
        "    middle_frame = selected_frames[window_size//2]\n",
        "\n",
        "    return selected_frames, middle_frame, max_mean\n",
        "\n",
        "def process_videos_dict_with_center_expansion(videos_dict, window_size=11):\n",
        "    \"\"\"\n",
        "    Process videos dictionary using center expansion approach.\n",
        "\n",
        "    Args:\n",
        "        videos_dict (dict): Dictionary of video data\n",
        "        window_size (int): Number of consecutive frames to analyze\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with selected frames and mean probability for each video\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for video_path, video_data in videos_dict.items():\n",
        "        selected_frames, middle_frame, mean_prob = select_frames_by_center_expansion(\n",
        "            video_data,\n",
        "            window_size\n",
        "        )\n",
        "\n",
        "        results[video_path] = {\n",
        "            'selected_frames': selected_frames,\n",
        "            'middle_frame': middle_frame,\n",
        "            'mean_probability': mean_prob,\n",
        "            'fish_species': video_data['fish_species']\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "# Process videos to select optimal frame sequences\n",
        "print(\"Selecting optimal frame sequences using center expansion strategy...\")\n",
        "window_mean_frames = process_videos_dict_with_center_expansion(filtered_data)\n",
        "\n",
        "print(f\"Processed {len(window_mean_frames)} videos for frame selection\")\n"
      ],
      "metadata": {
        "id": "4fax_VZ47pyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Quality Analysis and Filtering\n"
      ],
      "metadata": {
        "id": "4Cf7iFPe7sHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code was used only for intitial iteration of the experiments but for the later stages of the project the threshold was set to 0 to not filter any data out. We kept this because we thoguht it could be interesting to have available."
      ],
      "metadata": {
        "id": "h_44itV082Ak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_video_quality(results_dict, threshold=0.999999, num_samples=10):\n",
        "    \"\"\"\n",
        "    Analyze video quality based on mean probability threshold.\n",
        "\n",
        "    Args:\n",
        "        results_dict (dict): Dictionary from process_videos_dict_with_center_expansion\n",
        "        threshold (float): Threshold for high quality classification\n",
        "        num_samples (int): Number of low quality videos to sample\n",
        "    \"\"\"\n",
        "    # Separate videos into high and low quality\n",
        "    high_quality = {}\n",
        "    low_quality = {}\n",
        "\n",
        "    for path, data in results_dict.items():\n",
        "        if data['mean_probability'] >= threshold:\n",
        "            high_quality[path] = data['mean_probability']\n",
        "        else:\n",
        "            low_quality[path] = data['mean_probability']\n",
        "\n",
        "    # Print results\n",
        "    print(f\"High quality videos (score >= {threshold}): {len(high_quality)}\")\n",
        "    print(f\"Low quality videos (score < {threshold}): {len(low_quality)}\")\n",
        "\n",
        "    # Sample low quality videos if any exist\n",
        "    sample_size = min(num_samples, len(low_quality))\n",
        "    if sample_size > 0:\n",
        "        print(f\"\\nSample of lowest scoring videos (path, score):\")\n",
        "        import random\n",
        "        sampled_items = random.sample(list(low_quality.items()), sample_size)\n",
        "        for path, score in sampled_items:\n",
        "            print(f\"{Path(path).name}: {score:.10f}\")\n",
        "\n",
        "def filter_high_quality_videos(results_dict, threshold=0.0):\n",
        "    \"\"\"\n",
        "    Filter the results dictionary to keep only high quality videos.\n",
        "    Note: Using threshold=0.0 to keep all videos as described in paper methodology.\n",
        "\n",
        "    Args:\n",
        "        results_dict (dict): Dictionary from process_videos_dict_with_center_expansion\n",
        "        threshold (float): Threshold for high quality classification\n",
        "\n",
        "    Returns:\n",
        "        dict: Filtered dictionary containing only high quality videos\n",
        "    \"\"\"\n",
        "    high_quality_dict = {\n",
        "        path: data\n",
        "        for path, data in results_dict.items()\n",
        "        if data['mean_probability'] >= threshold\n",
        "    }\n",
        "\n",
        "    print(f\"Original number of videos: {len(results_dict)}\")\n",
        "    print(f\"Number of videos kept (score >= {threshold}): {len(high_quality_dict)}\")\n",
        "    print(f\"Number of videos removed: {len(results_dict) - len(high_quality_dict)}\")\n",
        "\n",
        "    return high_quality_dict\n",
        "\n",
        "# Analyze video quality and apply filtering\n",
        "analyze_video_quality(window_mean_frames, threshold=0.0)\n",
        "filtered_results = filter_high_quality_videos(window_mean_frames, threshold=0.0)"
      ],
      "metadata": {
        "id": "xPD9CCPq7yYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SigLIP Feature Extraction"
      ],
      "metadata": {
        "id": "3EyzKQUF7zaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureExtractor:\n",
        "    \"\"\"\n",
        "    SigLIP-based feature extractor for fish species classification.\n",
        "\n",
        "    Uses ViT-SO400M-14-SigLIP model as feature extractor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name='ViT-SO400M-14-SigLIP', device=None):\n",
        "        # CLIP model uses GPU, but Decord uses CPU for stability\n",
        "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"SigLIP model using device: {self.device}\")\n",
        "\n",
        "        # Force Decord to use CPU for video processing\n",
        "        decord.bridge.set_bridge('torch')\n",
        "        self.ctx = cpu(0)\n",
        "\n",
        "        # Create SigLIP model and preprocessing transform\n",
        "        try:\n",
        "            self.model, _, self.preprocess = open_clip.create_model_and_transforms(\n",
        "                model_name,\n",
        "                pretrained='webli'\n",
        "            )\n",
        "            self.model = self.model.to(self.device)\n",
        "            self.model.eval()\n",
        "            print(f\"Successfully loaded {model_name} model\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _extract_frames_from_video(self, video_path, frame_numbers):\n",
        "        \"\"\"\n",
        "        Extract specific frames from video using Decord with CPU processing.\n",
        "\n",
        "        Args:\n",
        "            video_path (str): Path to video file\n",
        "            frame_numbers (list): List of frame indices to extract\n",
        "\n",
        "        Returns:\n",
        "            list: PIL Images or None if extraction fails\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Load video with Decord on CPU\n",
        "            vr = VideoReader(str(video_path), ctx=self.ctx)\n",
        "\n",
        "            # Direct frame access\n",
        "            frames = vr.get_batch(frame_numbers)\n",
        "\n",
        "            # Convert to PIL Images\n",
        "            pil_frames = [\n",
        "                Image.fromarray(frame.numpy())\n",
        "                for frame in frames\n",
        "            ]\n",
        "\n",
        "            return pil_frames\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing video {video_path}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def extract_features(self, video_path, frame_numbers, batch_size=256):\n",
        "        \"\"\"\n",
        "        Extract SigLIP features for specific frames from a video.\n",
        "\n",
        "        Implements the feature extraction strategy described in the paper,\n",
        "        producing normalized 1152-dimensional feature vectors.\n",
        "\n",
        "        Args:\n",
        "            video_path (str): Path to video file\n",
        "            frame_numbers (list): Frame indices to process\n",
        "            batch_size (int): Batch size for processing (default 256)\n",
        "\n",
        "        Returns:\n",
        "            dict: Mapping from frame_number to normalized feature vector\n",
        "        \"\"\"\n",
        "        features_dict = {}\n",
        "        frames_batches = [frame_numbers[i:i + batch_size]\n",
        "                         for i in range(0, len(frame_numbers), batch_size)]\n",
        "\n",
        "        for batch in frames_batches:\n",
        "            frames = self._extract_frames_from_video(video_path, batch)\n",
        "\n",
        "            if frames is None:\n",
        "                continue\n",
        "\n",
        "            # Preprocess frames for SigLIP model\n",
        "            processed_frames = torch.stack([\n",
        "                self.preprocess(frame).to(self.device)\n",
        "                for frame in frames\n",
        "            ])\n",
        "\n",
        "            # Extract features with automatic mixed precision\n",
        "            with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "                batch_features = self.model.encode_image(processed_frames)\n",
        "                # L2 normalize features as described in paper methodology\n",
        "                batch_features = F.normalize(batch_features, dim=-1)\n",
        "\n",
        "            # Store features\n",
        "            for idx, frame_num in enumerate(batch):\n",
        "                features_dict[frame_num] = batch_features[idx].cpu().numpy()\n",
        "\n",
        "        return features_dict\n",
        "\n",
        "def process_videos_batch(videos_mapping, output_dir, model_name='ViT-SO400M-14-SigLIP', device=None):\n",
        "    \"\"\"\n",
        "    Process a batch of videos and save their SigLIP features.\n",
        "\n",
        "    Args:\n",
        "        videos_mapping (dict): Video paths and metadata\n",
        "        output_dir (str): Directory to save feature files\n",
        "        model_name (str): SigLIP model variant to use\n",
        "        device (str): Device for computation\n",
        "    \"\"\"\n",
        "    # Initialize the feature extractor\n",
        "    extractor = FeatureExtractor(model_name=model_name, device=device)\n",
        "\n",
        "    # Create output directory\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    # Track processing results\n",
        "    results = Counter()\n",
        "\n",
        "    # Process each video\n",
        "    pbar = tqdm(videos_mapping.items(), desc=\"Extracting SigLIP features\")\n",
        "    for video_path, data in pbar:\n",
        "        try:\n",
        "            # Extract features for selected frames\n",
        "            features = extractor.extract_features(\n",
        "                video_path,\n",
        "                data['selected_frames']\n",
        "            )\n",
        "\n",
        "            if not features:\n",
        "                results['failed'] += 1\n",
        "                continue\n",
        "\n",
        "            # Generate output filename with dataset information\n",
        "            dataset_name = Path(video_path).parent.name.split('_')[0]\n",
        "            video_number = Path(video_path).stem\n",
        "            output_filename = f\"{dataset_name}_{video_number}_features.npz\"\n",
        "            output_path = output_dir / output_filename\n",
        "\n",
        "            # Save features and metadata\n",
        "            np.savez(\n",
        "                output_path,\n",
        "                features=features,\n",
        "                frame_numbers=data['selected_frames'],\n",
        "                middle_frame=data['middle_frame'],\n",
        "                fish_species=data['fish_species']\n",
        "            )\n",
        "\n",
        "            results['success'] += 1\n",
        "\n",
        "            # Update progress bar\n",
        "            pbar.set_postfix({\n",
        "                'Success': results['success'],\n",
        "                'Failed': results['failed']\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {video_path}: {str(e)}\")\n",
        "            results['failed'] += 1\n",
        "            continue\n",
        "\n",
        "    # Print final statistics\n",
        "    print(\"\\nSigLIP Feature Extraction Complete!\")\n",
        "    print(f\"Successfully processed: {results['success']} videos\")\n",
        "    print(f\"Failed to process: {results['failed']} videos\")\n",
        "    if results['success'] + results['failed'] > 0:\n",
        "        completion_rate = results['success']/(results['success'] + results['failed'])*100\n",
        "        print(f\"Total completion rate: {completion_rate:.2f}%\")\n"
      ],
      "metadata": {
        "id": "evFrLYl977yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ResNet-50 Feature Extraction (Baseline Comparison)\n"
      ],
      "metadata": {
        "id": "2wZbXqbc781K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_single_video_frame_extraction(args):\n",
        "    \"\"\"\n",
        "    Process a single video and save its middle frame for ResNet-50 processing.\n",
        "\n",
        "    Args:\n",
        "        args (tuple): (video_path, info, output_dir)\n",
        "\n",
        "    Returns:\n",
        "        bool: Success status\n",
        "    \"\"\"\n",
        "    video_path, info, output_dir = args\n",
        "    try:\n",
        "        # Get middle frame number\n",
        "        middle_frame = info['middle_frame']\n",
        "\n",
        "        # Create VideoReader object and extract frame\n",
        "        vr = VideoReader(str(video_path), ctx=cpu(0))\n",
        "        frame = vr[middle_frame].asnumpy()\n",
        "\n",
        "        # Create output filename\n",
        "        video_name = Path(video_path).stem\n",
        "        fish_species = info['fish_species']\n",
        "        output_path = output_dir / f\"{video_name}_{fish_species}_frame_{middle_frame}.jpg\"\n",
        "\n",
        "        # Save frame\n",
        "        cv2.imwrite(str(output_path), cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError processing {video_path}: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def extract_middle_frames(video_dict, output_dir):\n",
        "    \"\"\"\n",
        "    Extract middle frames from videos for ResNet-50 feature extraction.\n",
        "\n",
        "    Args:\n",
        "        video_dict (dict): Dictionary containing video paths and frame information\n",
        "        output_dir (str): Directory to save frames\n",
        "    \"\"\"\n",
        "    # Create output directory\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Determine number of workers (optimized for Colab)\n",
        "    num_workers = min(4, multiprocessing.cpu_count())\n",
        "\n",
        "    total_videos = len(video_dict)\n",
        "    print(f\"Extracting middle frames from {total_videos} videos using {num_workers} workers\")\n",
        "\n",
        "    # Prepare arguments for parallel processing\n",
        "    args_list = [(path, info, output_dir)\n",
        "                 for path, info in list(video_dict.items())]\n",
        "\n",
        "    # Process videos in parallel\n",
        "    success_count = 0\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "        results = list(tqdm(\n",
        "            executor.map(process_single_video_frame_extraction, args_list),\n",
        "            total=total_videos,\n",
        "            desc=\"Extracting frames\"\n",
        "        ))\n",
        "        success_count = sum(results)\n",
        "\n",
        "    print(f\"\\nFrame extraction complete!\")\n",
        "    print(f\"Successfully processed: {success_count}/{total_videos} videos\")\n",
        "    print(f\"Frames saved to: {output_dir}\")\n",
        "\n",
        "class ResNetFeatureExtractor:\n",
        "    \"\"\"\n",
        "    ResNet-50 feature extractor for baseline comparison.\n",
        "\n",
        "    Implements ResNet-50 feature extraction as described in the paper\n",
        "    for comparison with SigLIP-based approach.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device=None):\n",
        "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"ResNet-50 using device: {self.device}\")\n",
        "\n",
        "        try:\n",
        "            # Load pretrained ResNet-50 and remove final classifier\n",
        "            self.model = models.resnet50(pretrained=True)\n",
        "            self.model = torch.nn.Sequential(*list(self.model.children())[:-1])\n",
        "            self.model = self.model.to(self.device)\n",
        "            self.model.eval()\n",
        "\n",
        "            # ImageNet normalization as used in paper\n",
        "            self.preprocess = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225]\n",
        "                )\n",
        "            ])\n",
        "            print(\"ResNet-50 model loaded successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading ResNet-50 model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def extract_feature(self, image_path):\n",
        "        \"\"\"\n",
        "        Extract ResNet-50 features from a single image.\n",
        "\n",
        "        Args:\n",
        "            image_path (str): Path to image file\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Normalized 2048-dimensional feature vector\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Load and process image\n",
        "            image = Image.open(image_path)\n",
        "            processed_image = self.preprocess(image).unsqueeze(0).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                feature = self.model(processed_image)\n",
        "                feature = feature.squeeze()\n",
        "                # L2 normalize features for fair comparison\n",
        "                feature = torch.nn.functional.normalize(feature, dim=-1)\n",
        "\n",
        "            return feature.cpu().numpy()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {image_path}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "def extract_species_from_filename(filename):\n",
        "    \"\"\"Extract species label from generated filename.\"\"\"\n",
        "    return filename.split('_')[1]  # Adjusted for our filename format\n",
        "\n",
        "def process_images_batch_resnet(image_folder, output_dir):\n",
        "    \"\"\"\n",
        "    Process extracted frames with ResNet-50 and save features.\n",
        "\n",
        "    Args:\n",
        "        image_folder (str): Directory containing extracted frames\n",
        "        output_dir (str): Directory to save ResNet-50 features\n",
        "    \"\"\"\n",
        "    extractor = ResNetFeatureExtractor()\n",
        "\n",
        "    # Create output directory\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    # Process all jpg images in the folder\n",
        "    image_paths = list(Path(image_folder).glob('*.jpg'))\n",
        "\n",
        "    print(f\"Processing {len(image_paths)} images with ResNet-50...\")\n",
        "\n",
        "    for image_path in tqdm(image_paths, desc=\"Processing images\"):\n",
        "        try:\n",
        "            # Extract features\n",
        "            feature = extractor.extract_feature(image_path)\n",
        "\n",
        "            if feature is not None:\n",
        "                # Extract species from filename\n",
        "                species = extract_species_from_filename(image_path.name)\n",
        "\n",
        "                # Create output filename\n",
        "                output_filename = f\"{image_path.stem}_resnet_features.npz\"\n",
        "                output_path = output_dir / output_filename\n",
        "\n",
        "                # Save features and species\n",
        "                np.savez(\n",
        "                    output_path,\n",
        "                    features=feature,\n",
        "                    fish_species=species\n",
        "                )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {image_path}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    print(\"\\nResNet-50 feature extraction complete!\")"
      ],
      "metadata": {
        "id": "lMQk_uWp8BQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Main Processing Pipeline\n"
      ],
      "metadata": {
        "id": "y2JwpSA48JiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main feature extraction pipeline.\n",
        "\n",
        "    Processes videos through both SigLIP and ResNet-50 feature extraction\n",
        "    for comparison as described in the paper methodology.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 70)\n",
        "    print(\"Fish Species Classification - Feature Extraction Pipeline\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Define output directories\n",
        "    siglip_output_dir = \"features/ViT-SO400M-14-SigLIP\"\n",
        "    resnet_output_dir = \"features/ResNet-50\"\n",
        "    frames_output_dir = \"middle_frames\"\n",
        "\n",
        "    print(f\"\\nProcessing {len(filtered_results)} videos...\")\n",
        "    print(f\"SigLIP features will be saved to: {siglip_output_dir}\")\n",
        "    print(f\"ResNet-50 features will be saved to: {resnet_output_dir}\")\n",
        "\n",
        "    # Extract SigLIP features for temporal aggregation\n",
        "    print(\"\\n1. Extracting SigLIP features for temporal aggregation...\")\n",
        "    process_videos_batch(\n",
        "        filtered_results,  # these results where not filetered (threshold = 0)\n",
        "        siglip_output_dir\n",
        "    )\n",
        "\n",
        "    # Extract middle frames for ResNet-50 baseline\n",
        "    print(\"\\n2. Extracting middle frames for ResNet-50 baseline...\")\n",
        "    extract_middle_frames(filtered_results, frames_output_dir)\n",
        "\n",
        "    # Extract ResNet-50 features for comparison\n",
        "    print(\"\\n3. Extracting ResNet-50 features for baseline comparison...\")\n",
        "    process_images_batch_resnet(frames_output_dir, resnet_output_dir)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"Feature extraction pipeline completed successfully!\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"\\nGenerated outputs:\")\n",
        "    print(f\"- SigLIP features: {siglip_output_dir}/\")\n",
        "    print(f\"- ResNet-50 features: {resnet_output_dir}/\")\n",
        "    print(f\"- Middle frame images: {frames_output_dir}/\")\n",
        "    print(\"\\nThese features can now be used for temporal aggregation and species classification.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "nZK-Gg92wldM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}