{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPXZvmoXe+y/Fif1S4mrNeW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joaosMart/fish-species-class-siglip/blob/main/Code/species-classification/Learning_Curve_ResNet_50_fine_tuned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet-50 Learning Curve Analysis for Fish Classification\n",
        "\n",
        "This notebook implements the learning curve experiments from:\n",
        "\n",
        "\"Temporal Aggregation of Vision-Language Features for High-Accuracy Fish Classification in Automated Monitoring\"\n",
        "\n",
        "The analysis trains ResNet-50 models with varying amounts of training data across multiple random seeds to evaluate model performance and data efficiency for classifying salmonid species.\n",
        "\n",
        "## Requirements\n",
        "\n",
        "Install required packages:\n",
        "\n",
        "```bash\n",
        "!pip install torch torchvision scikit-learn matplotlib seaborn pandas numpy tqdm Pillow decord\n",
        "```\n",
        "\n",
        "### Dataset Structure\n",
        "Place your dataset in the following structure:\n",
        "```\n",
        "dataset/\n",
        "‚îú‚îÄ‚îÄ Bleikja/          # Arctic Char images\n",
        "‚îú‚îÄ‚îÄ Lax/              # Atlantic Salmon images  \n",
        "‚îî‚îÄ‚îÄ Urri√∞i/           # Brown Trout images\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "_CWiGynJbKvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration and Imports"
      ],
      "metadata": {
        "id": "bp5rjQdsc86z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import time\n",
        "import copy\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "import gc\n",
        "import os\n",
        "import random\n",
        "from collections import namedtuple\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")"
      ],
      "metadata": {
        "id": "P4R47RCBc7E1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration"
      ],
      "metadata": {
        "id": "CzMr0eLqdC8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dataset configuration\n",
        "DATASET_PATH = './dataset'  # Update this path to your dataset location\n",
        "CLASS_NAMES = ['Bleikja', 'Lax', 'Urri√∞i']  # Arctic Char, Atlantic Salmon, Brown/Sea Trout\n",
        "\n",
        "# Training configuration\n",
        "BATCH_SIZE = 128  # Adjust based on GPU memory\n",
        "NUM_EPOCHS = 40\n",
        "PATIENCE = 3\n",
        "LEARNING_RATE = 4.37e-04  # Optimized learning rate from paper\n",
        "NUM_RUNS = 10  # Number of random seeds for statistical significance\n",
        "BASE_SEED = 42\n",
        "\n",
        "# Learning curve configuration\n",
        "TRAIN_FRACTIONS = [0.05, 0.2, 0.4, 0.6, 0.8, 1.0]  # Fractions of training data to use\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"Dataset path: {DATASET_PATH}\")\n",
        "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Number of runs: {NUM_RUNS}\")"
      ],
      "metadata": {
        "id": "nhA2klTldCZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions"
      ],
      "metadata": {
        "id": "eSJYLi4bdJXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    \"\"\"Set random seed for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def print_dataset_info(dataset_path):\n",
        "    \"\"\"Print information about the dataset structure.\"\"\"\n",
        "    dataset_path = Path(dataset_path)\n",
        "    if not dataset_path.exists():\n",
        "        print(f\"‚ö†Ô∏è  Dataset path {dataset_path} not found!\")\n",
        "        print(\"Please update DATASET_PATH or ensure dataset is in correct location.\")\n",
        "        return False\n",
        "\n",
        "    print(f\"\\nüìÅ Dataset Information:\")\n",
        "    print(f\"Location: {dataset_path}\")\n",
        "\n",
        "    total_images = 0\n",
        "    for class_name in CLASS_NAMES:\n",
        "        class_path = dataset_path / class_name\n",
        "        if class_path.exists():\n",
        "            image_count = len(list(class_path.glob('*.jpg')) + list(class_path.glob('*.png')))\n",
        "            print(f\"  {class_name}: {image_count} images\")\n",
        "            total_images += image_count\n",
        "        else:\n",
        "            print(f\"  ‚ùå {class_name}: Directory not found\")\n",
        "\n",
        "    print(f\"  Total: {total_images} images\")\n",
        "    return total_images > 0\n",
        "\n",
        "# Check dataset\n",
        "if not print_dataset_info(DATASET_PATH):\n",
        "    print(\"\\n‚ö†Ô∏è  Please ensure your dataset is properly structured before continuing.\")\n"
      ],
      "metadata": {
        "id": "X1CAUbX4dH8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Classes"
      ],
      "metadata": {
        "id": "-M6Nq0zYdMJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FishFrameDataset(Dataset):\n",
        "    \"\"\"Custom dataset for fish frame classification.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.transform = transform\n",
        "\n",
        "        print(\"üìÇ Scanning directories...\")\n",
        "        # Get all valid class directories\n",
        "        self.classes = sorted([\n",
        "            d.name for d in self.root_dir.iterdir()\n",
        "            if d.is_dir() and not d.name.startswith('.')\n",
        "        ])\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "\n",
        "        # Index dataset\n",
        "        self.samples = []\n",
        "        print(\"üîç Indexing dataset...\")\n",
        "        for class_name in tqdm(self.classes, desc=\"Loading classes\"):\n",
        "            class_dir = self.root_dir / class_name\n",
        "            image_files = [\n",
        "                f for f in class_dir.glob('*')\n",
        "                if f.suffix.lower() in ('.jpg', '.jpeg', '.png')\n",
        "                and not f.name.startswith('.')\n",
        "            ]\n",
        "            self.samples.extend([(str(img_path), class_name) for img_path in image_files])\n",
        "\n",
        "        print(f\"‚úÖ Found {len(self.samples)} images across {len(self.classes)} classes\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, class_name = self.samples[idx]\n",
        "\n",
        "        try:\n",
        "            with Image.open(img_path) as image:\n",
        "                image = image.convert('RGB')\n",
        "            label_idx = self.class_to_idx[class_name]\n",
        "\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "\n",
        "            return image, torch.tensor(label_idx)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error reading image {img_path}: {str(e)}\")\n",
        "            return self.__getitem__((idx + 1) % len(self))\n",
        "\n",
        "class TransformSubset(Dataset):\n",
        "    \"\"\"Dataset subset with transforms applied.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset, indices, transform):\n",
        "        self.dataset = dataset\n",
        "        self.indices = indices\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.dataset[self.indices[idx]]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n"
      ],
      "metadata": {
        "id": "ZpFk6a6IdMxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture"
      ],
      "metadata": {
        "id": "wLp9CIamdUzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    \"\"\"Bottleneck block for ResNet-50.\"\"\"\n",
        "\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                              stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(out_channels, self.expansion * out_channels,\n",
        "                              kernel_size=1, stride=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion * out_channels)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        if downsample:\n",
        "            conv = nn.Conv2d(in_channels, self.expansion * out_channels,\n",
        "                           kernel_size=1, stride=stride, bias=False)\n",
        "            bn = nn.BatchNorm2d(self.expansion * out_channels)\n",
        "            downsample = nn.Sequential(conv, bn)\n",
        "        else:\n",
        "            downsample = None\n",
        "\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(identity)\n",
        "\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"Custom ResNet implementation.\"\"\"\n",
        "\n",
        "    def __init__(self, config, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        block, n_blocks, channels = config\n",
        "        self.in_channels = channels[0]\n",
        "\n",
        "        assert len(n_blocks) == len(channels) == 4\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self.get_resnet_layer(block, n_blocks[0], channels[0])\n",
        "        self.layer2 = self.get_resnet_layer(block, n_blocks[1], channels[1], stride=2)\n",
        "        self.layer3 = self.get_resnet_layer(block, n_blocks[2], channels[2], stride=2)\n",
        "        self.layer4 = self.get_resnet_layer(block, n_blocks[3], channels[3], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(self.in_channels, output_dim)\n",
        "\n",
        "    def get_resnet_layer(self, block, n_blocks, channels, stride=1):\n",
        "        layers = []\n",
        "\n",
        "        if self.in_channels != block.expansion * channels:\n",
        "            downsample = True\n",
        "        else:\n",
        "            downsample = False\n",
        "\n",
        "        layers.append(block(self.in_channels, channels, stride, downsample))\n",
        "\n",
        "        for i in range(1, n_blocks):\n",
        "            layers.append(block(block.expansion * channels, channels))\n",
        "\n",
        "        self.in_channels = block.expansion * channels\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        h = x.view(x.shape[0], -1)\n",
        "        x = self.fc(h)\n",
        "\n",
        "        return x, h\n"
      ],
      "metadata": {
        "id": "9pRRMAFNdTCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Functions"
      ],
      "metadata": {
        "id": "qzWtSJredaqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stopping utility.\"\"\"\n",
        "\n",
        "    def __init__(self, patience=7, min_delta=0, verbose=False):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "\n",
        "    def __call__(self, val_loss, model, path='checkpoint.pt'):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            self.save_checkpoint(val_loss, model, path)\n",
        "        elif val_loss > self.best_loss + self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.save_checkpoint(val_loss, model, path)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model, path):\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...')\n",
        "        torch.save(model.state_dict(), path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "def train_model_with_early_stopping(model, train_loader, criterion, optimizer, scheduler,\n",
        "                                   device, num_epochs=20, patience=5, verbose=True):\n",
        "    \"\"\"Train model with early stopping.\"\"\"\n",
        "    best_model_weights = copy.deepcopy(model.state_dict())\n",
        "    best_loss = float('inf')\n",
        "    no_improve_epochs = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Train for one epoch\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", disable=not verbose)\n",
        "        for inputs, labels in progress_bar:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs, _ = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            progress_bar.set_postfix({'loss': loss.item(), 'acc': 100.*correct/total})\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        epoch_acc = correct / total\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}, Acc: {epoch_acc*100:.2f}%\")\n",
        "\n",
        "        # Update learning rate scheduler\n",
        "        scheduler.step(epoch_loss)\n",
        "\n",
        "        # Early stopping check\n",
        "        if epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            best_model_weights = copy.deepcopy(model.state_dict())\n",
        "            no_improve_epochs = 0\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "            if no_improve_epochs >= patience:\n",
        "                if verbose:\n",
        "                    print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_weights)\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, data_loader, device, metrics):\n",
        "    \"\"\"Evaluate model and return specified metrics.\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs, _ = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    results = {}\n",
        "    if 'accuracy' in metrics:\n",
        "        results['accuracy'] = accuracy_score(all_labels, all_preds)\n",
        "    if 'f1_weighted' in metrics:\n",
        "        results['f1_weighted'] = f1_score(all_labels, all_preds, average='weighted')\n",
        "    if 'f1_macro' in metrics:\n",
        "        results['f1_macro'] = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "q4Wjt4-OdY9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Curve Experiment"
      ],
      "metadata": {
        "id": "trw7P7raddhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_learning_curve_experiment(dataset_path=DATASET_PATH,\n",
        "                                 train_sizes=None,\n",
        "                                 num_runs=NUM_RUNS,\n",
        "                                 batch_size=BATCH_SIZE,\n",
        "                                 num_epochs=NUM_EPOCHS,\n",
        "                                 patience=PATIENCE,\n",
        "                                 metrics=['accuracy', 'f1_weighted', 'f1_macro'],\n",
        "                                 class_names=CLASS_NAMES,\n",
        "                                 base_seed=BASE_SEED):\n",
        "    \"\"\"Run learning curve experiment across multiple random seeds.\"\"\"\n",
        "\n",
        "    print(f\"üöÄ Starting learning curve experiment with {num_runs} runs...\")\n",
        "    print(f\"üìä Metrics: {', '.join(metrics)}\")\n",
        "    print(f\"üè∑Ô∏è  Classes: {', '.join(class_names)}\")\n",
        "\n",
        "    # Storage for results across all runs\n",
        "    all_results = []\n",
        "\n",
        "    # Define transforms\n",
        "    pretrained_size = 224\n",
        "    pretrained_means = [0.485, 0.456, 0.406]\n",
        "    pretrained_stds = [0.229, 0.224, 0.225]\n",
        "\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.Resize((pretrained_size, pretrained_size)),\n",
        "        transforms.RandomRotation(5),\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "        transforms.RandomCrop(pretrained_size, padding=10),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=pretrained_means, std=pretrained_stds)\n",
        "    ])\n",
        "\n",
        "    eval_transforms = transforms.Compose([\n",
        "        transforms.Resize((pretrained_size, pretrained_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=pretrained_means, std=pretrained_stds)\n",
        "    ])\n",
        "\n",
        "    # Create base dataset\n",
        "    print(f\"üìÇ Loading dataset from {dataset_path}...\")\n",
        "    base_dataset = FishFrameDataset(root_dir=dataset_path)\n",
        "\n",
        "    # Calculate split sizes\n",
        "    total_size = len(base_dataset)\n",
        "    train_val_size = int(0.8 * total_size)  # 80% for train+val\n",
        "\n",
        "    # Convert train_sizes to absolute numbers if needed\n",
        "    if train_sizes is None:\n",
        "        train_fractions = TRAIN_FRACTIONS\n",
        "        train_sizes_abs = [int(frac * train_val_size) for frac in train_fractions]\n",
        "    elif all(isinstance(size, float) for size in train_sizes) and all(0 < size <= 1 for size in train_sizes):\n",
        "        train_sizes_abs = [int(frac * train_val_size) for frac in train_sizes]\n",
        "    else:\n",
        "        train_sizes_abs = train_sizes\n",
        "\n",
        "    # Model configuration\n",
        "    ResNetConfig = namedtuple('ResNetConfig', ['block', 'n_blocks', 'channels'])\n",
        "    resnet50_config = ResNetConfig(\n",
        "        block=Bottleneck,\n",
        "        n_blocks=[3, 4, 6, 3],\n",
        "        channels=[64, 128, 256, 512]\n",
        "    )\n",
        "\n",
        "    OUTPUT_DIM = len(class_names)\n",
        "\n",
        "    # Class weights for balanced loss (adjust these based on your dataset)\n",
        "    class_counts = torch.tensor([279, 1086, 1588], dtype=torch.float32)  # Update for your dataset\n",
        "    class_weights = 1.0 / class_counts\n",
        "    class_weights = class_weights / class_weights.sum()\n",
        "\n",
        "    # Run experiments across multiple seeds\n",
        "    for run in range(num_runs):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"üîÑ Starting run {run+1}/{num_runs}\")\n",
        "\n",
        "        # Set seed for this run\n",
        "        current_seed = base_seed + run\n",
        "        set_seed(current_seed)\n",
        "        print(f\"üé≤ Using seed: {current_seed}\")\n",
        "\n",
        "        # Create train/test split for this run\n",
        "        indices = torch.randperm(total_size)\n",
        "        train_val_indices = indices[:train_val_size]\n",
        "        test_indices = indices[train_val_size:]\n",
        "\n",
        "        # Create test dataset\n",
        "        test_dataset = TransformSubset(base_dataset, test_indices, eval_transforms)\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=0,\n",
        "            pin_memory=torch.cuda.is_available()\n",
        "        )\n",
        "\n",
        "        # Device setup\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        class_weights_device = class_weights.to(device)\n",
        "\n",
        "        # For each training size in this run\n",
        "        for i, n_train_samples in enumerate(train_sizes_abs):\n",
        "            print(f\"\\nüìà Training with {n_train_samples} samples ({n_train_samples/train_val_size*100:.1f}% of training data)\")\n",
        "\n",
        "            # Create subset for training\n",
        "            shuffled_train_indices = train_val_indices[torch.randperm(len(train_val_indices))]\n",
        "            train_subset_indices = shuffled_train_indices[:n_train_samples]\n",
        "            train_subset = TransformSubset(base_dataset, train_subset_indices, train_transforms)\n",
        "\n",
        "            # Create DataLoader\n",
        "            train_loader = DataLoader(\n",
        "                train_subset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True,\n",
        "                num_workers=0,\n",
        "                pin_memory=torch.cuda.is_available()\n",
        "            )\n",
        "\n",
        "            # Initialize model with pre-trained weights\n",
        "            pretrained_model = models.resnet50(weights='DEFAULT')\n",
        "            IN_FEATURES = pretrained_model.fc.in_features\n",
        "            fc = nn.Linear(IN_FEATURES, OUTPUT_DIM)\n",
        "            pretrained_model.fc = fc\n",
        "\n",
        "            # Initialize custom ResNet model\n",
        "            model = ResNet(resnet50_config, OUTPUT_DIM)\n",
        "            model.load_state_dict(pretrained_model.state_dict())\n",
        "            model = model.to(device)\n",
        "\n",
        "            # Layer-wise learning rates (as in paper)\n",
        "            params = [\n",
        "                {'params': model.conv1.parameters(), 'lr': LEARNING_RATE / 10},\n",
        "                {'params': model.bn1.parameters(), 'lr': LEARNING_RATE / 10},\n",
        "                {'params': model.layer1.parameters(), 'lr': LEARNING_RATE / 8},\n",
        "                {'params': model.layer2.parameters(), 'lr': LEARNING_RATE / 6},\n",
        "                {'params': model.layer3.parameters(), 'lr': LEARNING_RATE / 4},\n",
        "                {'params': model.layer4.parameters(), 'lr': LEARNING_RATE / 2},\n",
        "                {'params': model.fc.parameters()}\n",
        "            ]\n",
        "\n",
        "            # Loss function with class weights\n",
        "            criterion = nn.CrossEntropyLoss(weight=class_weights_device)\n",
        "\n",
        "            # Optimizer and scheduler\n",
        "            optimizer = optim.Adam(params, lr=LEARNING_RATE)\n",
        "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "            # Train the model\n",
        "            start_time = time.time()\n",
        "            model = train_model_with_early_stopping(\n",
        "                model=model,\n",
        "                train_loader=train_loader,\n",
        "                criterion=criterion,\n",
        "                optimizer=optimizer,\n",
        "                scheduler=scheduler,\n",
        "                device=device,\n",
        "                num_epochs=num_epochs,\n",
        "                patience=patience,\n",
        "                verbose=True\n",
        "            )\n",
        "            end_time = time.time()\n",
        "            training_time = end_time - start_time\n",
        "\n",
        "            # Evaluate on test data\n",
        "            test_metrics = evaluate_model(model, test_loader, device, metrics)\n",
        "\n",
        "            # Store results\n",
        "            result = {\n",
        "                'run': run,\n",
        "                'seed': current_seed,\n",
        "                'train_size': n_train_samples,\n",
        "                'training_time': training_time\n",
        "            }\n",
        "            for metric in metrics:\n",
        "                result[f'test_{metric}'] = test_metrics[metric]\n",
        "\n",
        "            all_results.append(result)\n",
        "\n",
        "            # Print results for this training size\n",
        "            print(f\"‚è±Ô∏è  Training completed in {training_time:.1f} seconds\")\n",
        "            print(\"üìä Test metrics:\")\n",
        "            for metric in metrics:\n",
        "                print(f\"   {metric}: {test_metrics[metric]:.4f}\")\n",
        "\n",
        "            # Clean up GPU memory\n",
        "            del model, optimizer, scheduler\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            time.sleep(1)\n",
        "\n",
        "    # Convert results to DataFrame\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "\n",
        "    # Save results to CSV\n",
        "    results_df.to_csv('learning_curve_results.csv', index=False)\n",
        "    print(f\"\\nüíæ Results saved to 'learning_curve_results.csv'\")\n",
        "\n",
        "    return train_sizes_abs, results_df"
      ],
      "metadata": {
        "id": "4BRk-Rf3dhLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization Functions"
      ],
      "metadata": {
        "id": "vySkjdxydlAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def plot_learning_curve_with_confidence(results_df, train_sizes, metrics, class_names, figsize=(14, 12)):\n",
        "    \"\"\"Plot learning curve with confidence intervals from multiple runs.\"\"\"\n",
        "\n",
        "    # Calculate statistics for each training size\n",
        "    stats_df = pd.DataFrame()\n",
        "    for train_size in train_sizes:\n",
        "        size_data = results_df[results_df['train_size'] == train_size]\n",
        "        row = {'train_size': train_size}\n",
        "\n",
        "        for metric in metrics:\n",
        "            test_values = size_data[f'test_{metric}']\n",
        "            row[f'test_{metric}_mean'] = test_values.mean()\n",
        "            row[f'test_{metric}_std'] = test_values.std()\n",
        "            # Calculate 95% confidence interval\n",
        "            n = len(test_values)\n",
        "            row[f'test_{metric}_ci95'] = stats.t.ppf(0.975, n-1) * test_values.std() / np.sqrt(n)\n",
        "\n",
        "        stats_df = pd.concat([stats_df, pd.DataFrame([row])], ignore_index=True)\n",
        "\n",
        "    # Sort by train_size\n",
        "    stats_df = stats_df.sort_values('train_size')\n",
        "\n",
        "    # Create figure\n",
        "    fig, axes = plt.subplots(len(metrics), 1, figsize=figsize, sharex=True)\n",
        "    if len(metrics) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    # Color palette\n",
        "    colors = sns.color_palette(\"viridis\", len(metrics))\n",
        "\n",
        "    # Plot each metric\n",
        "    for i, metric in enumerate(metrics):\n",
        "        ax = axes[i]\n",
        "\n",
        "        # Get data\n",
        "        test_mean = stats_df[f'test_{metric}_mean']\n",
        "        test_ci95 = stats_df[f'test_{metric}_ci95']\n",
        "\n",
        "        # Plot curve\n",
        "        ax.plot(stats_df['train_size'], test_mean, 'o-', color=colors[i],\n",
        "                linewidth=2, markersize=8, label=f'Test {metric}')\n",
        "\n",
        "        # Plot confidence interval\n",
        "        ax.fill_between(stats_df['train_size'],\n",
        "                       test_mean - test_ci95,\n",
        "                       test_mean + test_ci95,\n",
        "                       alpha=0.3, color=colors[i],\n",
        "                       label='95% CI')\n",
        "\n",
        "        # Formatting\n",
        "        metric_name = metric.replace('_', ' ').title()\n",
        "        ax.set_title(f'{metric_name} vs Training Size', fontsize=14, fontweight='bold')\n",
        "        ax.set_ylabel(f'{metric_name}', fontsize=12)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.legend(loc='best')\n",
        "\n",
        "        # Set y-axis limits for better visualization\n",
        "        y_min = max(0.5, test_mean.min() - test_ci95.max() - 0.05)\n",
        "        y_max = min(1.0, test_mean.max() + test_ci95.max() + 0.05)\n",
        "        ax.set_ylim(y_min, y_max)\n",
        "\n",
        "    # Format x-axis\n",
        "    axes[-1].set_xlabel('Number of Training Examples', fontsize=12)\n",
        "\n",
        "    # Overall title\n",
        "    fig.suptitle(f'Learning Curves for Fish Classification\\n({\", \".join(class_names)})',\n",
        "                fontsize=16, fontweight='bold', y=0.98)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(top=0.93)\n",
        "    plt.savefig('learning_curve_resnet50.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(\"\\nüìà Learning Curve Summary:\")\n",
        "    print(\"=\"*50)\n",
        "    for metric in metrics:\n",
        "        print(f\"\\n{metric.replace('_', ' ').title()}:\")\n",
        "        smallest = stats_df.iloc[0]\n",
        "        largest = stats_df.iloc[-1]\n",
        "        improvement = largest[f'test_{metric}_mean'] - smallest[f'test_{metric}_mean']\n",
        "        improvement_pct = improvement / smallest[f'test_{metric}_mean'] * 100\n",
        "\n",
        "        print(f\"  Smallest dataset: {smallest[f'test_{metric}_mean']:.4f}\")\n",
        "        print(f\"  Largest dataset:  {largest[f'test_{metric}_mean']:.4f}\")\n",
        "        print(f\"  Improvement:      {improvement:.4f} ({improvement_pct:.1f}%)\")\n",
        "\n",
        "def print_final_summary(results_df, metrics, class_names):\n",
        "    \"\"\"Print final summary of results.\"\"\"\n",
        "    print(\"\\nüéØ Final Results Summary\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Overall performance\n",
        "    print(f\"\\nAverage performance across all runs:\")\n",
        "    for metric in metrics:\n",
        "        mean_val = results_df[f'test_{metric}'].mean()\n",
        "        std_val = results_df[f'test_{metric}'].std()\n",
        "        print(f\"  {metric.replace('_', ' ').title()}: {mean_val:.4f} ¬± {std_val:.4f}\")\n",
        "\n",
        "    # Best performance\n",
        "    print(f\"\\nBest performance (single run):\")\n",
        "    for metric in metrics:\n",
        "        best_val = results_df[f'test_{metric}'].max()\n",
        "        best_run = results_df.loc[results_df[f'test_{metric}'].idxmax()]\n",
        "        print(f\"  {metric.replace('_', ' ').title()}: {best_val:.4f} (Run {best_run['run']+1}, Seed {best_run['seed']})\")\n",
        "\n",
        "    # Data efficiency analysis\n",
        "    print(f\"\\nüí° Data Efficiency Insights:\")\n",
        "    largest_size = results_df['train_size'].max()\n",
        "    largest_results = results_df[results_df['train_size'] == largest_size]\n",
        "\n",
        "    for metric in metrics:\n",
        "        # Find minimum data needed to reach 95% of best performance\n",
        "        best_score = largest_results[f'test_{metric}'].mean()\n",
        "        target_score = 0.95 * best_score\n",
        "\n",
        "        efficient_size = None\n",
        "        for size in sorted(results_df['train_size']."
      ],
      "metadata": {
        "id": "WJ83I3_9cyO_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}